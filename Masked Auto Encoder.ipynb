{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/dataiku/DATA_DESIGNER/code-envs/python/offus/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Masked Variational Autoencoder\n",
    "# =========================\n",
    "\n",
    "class MaskedVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Variational Autoencoder for incomplete data.\n",
    "    Encoder input: [x âŠ™ m , m]\n",
    "    Latent output: mu (used as latent feature u_i)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: input_dim * 2 because of concatenation with mask\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, mask):\n",
    "        x_masked = x * mask\n",
    "        enc_input = torch.cat([x_masked, mask], dim=1)\n",
    "        h = self.encoder(enc_input)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mu, logvar = self.encode(x, mask)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Masked VAE Loss\n",
    "# =========================\n",
    "\n",
    "def masked_vae_loss(\n",
    "    recon_x,\n",
    "    x,\n",
    "    mask,\n",
    "    mu,\n",
    "    logvar,\n",
    "    recon_weight=0.7,\n",
    "    kl_weight=0.3,\n",
    "    eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Masked reconstruction + KL divergence loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Masked reconstruction loss (MSE over observed entries only)\n",
    "    se = (recon_x - x) ** 2\n",
    "    masked_se = se * mask\n",
    "    recon_loss = masked_se.sum(dim=1) / (mask.sum(dim=1) + eps)\n",
    "    recon_loss = recon_loss.mean()\n",
    "\n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * torch.mean(\n",
    "        torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    )\n",
    "\n",
    "    total_loss = recon_weight * recon_loss + kl_weight * kl_loss\n",
    "    return total_loss, recon_loss.item(), kl_loss.item()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Training Function\n",
    "# =========================\n",
    "\n",
    "def train_masked_vae(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    epochs=50,\n",
    "    lr=1e-3\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        total_rec = 0.0\n",
    "        total_kl = 0.0\n",
    "\n",
    "        for x, mask in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            mask = mask.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x, mask)\n",
    "            loss, rec, kl = masked_vae_loss(recon, x, mask, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_rec += rec\n",
    "            total_kl += kl\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Loss: {total_loss:.4f} | \"\n",
    "            f\"Recon: {total_rec:.4f} | \"\n",
    "            f\"KL: {total_kl:.4f}\"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Latent Feature Extraction\n",
    "# =========================\n",
    "\n",
    "def extract_latent_features(model, X, mask, device):\n",
    "    \"\"\"\n",
    "    Returns latent feature vectors u_i = mu_i\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = X.to(device).float()\n",
    "        mask = mask.to(device).float()\n",
    "        mu, _ = model.encode(X, mask)\n",
    "    return mu.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Load Iris Dataset\n",
    "# ============================================\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df[\"target\"] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Inject Missing Values (MCAR)\n",
    "# ============================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = df.shape[0]\n",
    "missing_ratio = 0.10\n",
    "n_missing_rows = int(missing_ratio * n_samples)\n",
    "\n",
    "missing_rows = np.random.choice(df.index, size=n_missing_rows, replace=False)\n",
    "\n",
    "for row in missing_rows:\n",
    "    n_cols_missing = np.random.randint(1, len(iris.feature_names))\n",
    "    cols_missing = np.random.choice(\n",
    "        iris.feature_names, size=n_cols_missing, replace=False\n",
    "    )\n",
    "    df.loc[row, cols_missing] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Create Feature Matrix and Mask\n",
    "# ============================================\n",
    "\n",
    "X = df[iris.feature_names].values.astype(np.float32)\n",
    "\n",
    "# Binary mask: 1 = observed, 0 = missing\n",
    "mask = (~np.isnan(X)).astype(np.float32)\n",
    "\n",
    "# Fill missing values with zero (mask-aware)\n",
    "X_filled = np.nan_to_num(X, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. Mask-aware Standardization\n",
    "# ============================================\n",
    "class TorchScaler:\n",
    "    def fit(self, X: torch.Tensor):\n",
    "        self.mean = X.mean(dim=0, keepdim=True)\n",
    "        self.std = X.std(dim=0, unbiased=False, keepdim=True)\n",
    "        self.std[self.std == 0] = 1.0\n",
    "        return self\n",
    "    def transform(self, X: torch.Tensor):\n",
    "        return (X - self.mean) / self.std\n",
    "    def inverse_transform(self, X: torch.Tensor):\n",
    "        return X * self.std + self.mean\n",
    "\n",
    "X_scaled = X_filled.copy()\n",
    "X_tensor = torch.from_numpy(np.array(X_scaled))\n",
    "scaler = TorchScaler().fit(X_tensor)\n",
    "X_scaled = scaler.transform(X_tensor)\n",
    "mask_tensor = torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_938110/1857059863.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 5. PyTorch Dataset and DataLoader\n",
    "# ============================================\n",
    "    \n",
    "class MaskedDataset:\n",
    "    def __init__(self, X, mask):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.mask[idx]\n",
    "\n",
    "\n",
    "dataset = MaskedDataset(X_scaled, mask)\n",
    "torch.manual_seed(42)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAssumes your DataLoader yields (x, mask)\\nx    : tensor of shape [batch_size, input_dim]\\nmask : tensor of shape [batch_size, input_dim], binary {0,1}\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Example Usage (Skeleton)\n",
    "# =========================\n",
    "\"\"\"\n",
    "Assumes your DataLoader yields (x, mask)\n",
    "x    : tensor of shape [batch_size, input_dim]\n",
    "mask : tensor of shape [batch_size, input_dim], binary {0,1}\n",
    "\"\"\"\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = MaskedVAE(input_dim=INPUT_DIM, latent_dim=LATENT_DIM)\n",
    "# model = train_masked_vae(model, dataloader, device)\n",
    "# U = extract_latent_features(model, X_full, mask_full, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 2.7090 | Recon: 3.2302 | KL: 1.4930\n",
      "Epoch 002 | Loss: 2.6824 | Recon: 3.2295 | KL: 1.4057\n",
      "Epoch 003 | Loss: 2.6347 | Recon: 3.1962 | KL: 1.3246\n",
      "Epoch 004 | Loss: 2.6220 | Recon: 3.2103 | KL: 1.2494\n",
      "Epoch 005 | Loss: 2.5854 | Recon: 3.1880 | KL: 1.1794\n",
      "Epoch 006 | Loss: 2.5393 | Recon: 3.1503 | KL: 1.1136\n",
      "Epoch 007 | Loss: 2.5283 | Recon: 3.1609 | KL: 1.0523\n",
      "Epoch 008 | Loss: 2.4745 | Recon: 3.1087 | KL: 0.9947\n",
      "Epoch 009 | Loss: 2.5326 | Recon: 3.2147 | KL: 0.9409\n",
      "Epoch 010 | Loss: 2.4705 | Recon: 3.1477 | KL: 0.8902\n",
      "Epoch 011 | Loss: 2.4535 | Recon: 3.1439 | KL: 0.8425\n",
      "Epoch 012 | Loss: 2.4233 | Recon: 3.1200 | KL: 0.7975\n",
      "Epoch 013 | Loss: 2.4579 | Recon: 3.1877 | KL: 0.7549\n",
      "Epoch 014 | Loss: 2.4089 | Recon: 3.1350 | KL: 0.7146\n",
      "Epoch 015 | Loss: 2.4105 | Recon: 3.1536 | KL: 0.6764\n",
      "Epoch 016 | Loss: 2.3563 | Recon: 3.0918 | KL: 0.6400\n",
      "Epoch 017 | Loss: 2.3759 | Recon: 3.1346 | KL: 0.6054\n",
      "Epoch 018 | Loss: 2.3242 | Recon: 3.0751 | KL: 0.5722\n",
      "Epoch 019 | Loss: 2.3690 | Recon: 3.1527 | KL: 0.5405\n",
      "Epoch 020 | Loss: 2.3476 | Recon: 3.1349 | KL: 0.5103\n",
      "Epoch 021 | Loss: 2.3524 | Recon: 3.1544 | KL: 0.4812\n",
      "Epoch 022 | Loss: 2.2780 | Recon: 3.0600 | KL: 0.4534\n",
      "Epoch 023 | Loss: 2.3186 | Recon: 3.1294 | KL: 0.4268\n",
      "Epoch 024 | Loss: 2.2842 | Recon: 3.0912 | KL: 0.4013\n",
      "Epoch 025 | Loss: 2.2745 | Recon: 3.0878 | KL: 0.3767\n",
      "Epoch 026 | Loss: 2.2873 | Recon: 3.1162 | KL: 0.3534\n",
      "Epoch 027 | Loss: 2.2781 | Recon: 3.1127 | KL: 0.3310\n",
      "Epoch 028 | Loss: 2.2525 | Recon: 3.0853 | KL: 0.3095\n",
      "Epoch 029 | Loss: 2.2634 | Recon: 3.1097 | KL: 0.2888\n",
      "Epoch 030 | Loss: 2.2476 | Recon: 3.0955 | KL: 0.2690\n",
      "Epoch 031 | Loss: 2.2340 | Recon: 3.0842 | KL: 0.2500\n",
      "Epoch 032 | Loss: 2.2216 | Recon: 3.0743 | KL: 0.2318\n",
      "Epoch 033 | Loss: 2.2386 | Recon: 3.1060 | KL: 0.2145\n",
      "Epoch 034 | Loss: 2.2174 | Recon: 3.0828 | KL: 0.1981\n",
      "Epoch 035 | Loss: 2.2164 | Recon: 3.0881 | KL: 0.1824\n",
      "Epoch 036 | Loss: 2.2395 | Recon: 3.1275 | KL: 0.1674\n",
      "Epoch 037 | Loss: 2.2162 | Recon: 3.1004 | KL: 0.1531\n",
      "Epoch 038 | Loss: 2.1726 | Recon: 3.0439 | KL: 0.1397\n",
      "Epoch 039 | Loss: 2.2154 | Recon: 3.1105 | KL: 0.1268\n",
      "Epoch 040 | Loss: 2.1952 | Recon: 3.0868 | KL: 0.1149\n",
      "Epoch 041 | Loss: 2.2169 | Recon: 3.1226 | KL: 0.1036\n",
      "Epoch 042 | Loss: 2.1712 | Recon: 3.0618 | KL: 0.0932\n",
      "Epoch 043 | Loss: 2.1893 | Recon: 3.0917 | KL: 0.0835\n",
      "Epoch 044 | Loss: 2.1712 | Recon: 3.0698 | KL: 0.0744\n",
      "Epoch 045 | Loss: 2.1566 | Recon: 3.0525 | KL: 0.0662\n",
      "Epoch 046 | Loss: 2.1643 | Recon: 3.0668 | KL: 0.0585\n",
      "Epoch 047 | Loss: 2.1886 | Recon: 3.1044 | KL: 0.0517\n",
      "Epoch 048 | Loss: 2.1634 | Recon: 3.0711 | KL: 0.0453\n",
      "Epoch 049 | Loss: 2.1829 | Recon: 3.1014 | KL: 0.0397\n",
      "Epoch 050 | Loss: 2.1991 | Recon: 3.1267 | KL: 0.0345\n",
      "Epoch 051 | Loss: 2.1294 | Recon: 3.0291 | KL: 0.0301\n",
      "Epoch 052 | Loss: 2.1722 | Recon: 3.0919 | KL: 0.0261\n",
      "Epoch 053 | Loss: 2.1677 | Recon: 3.0871 | KL: 0.0225\n",
      "Epoch 054 | Loss: 2.1263 | Recon: 3.0293 | KL: 0.0194\n",
      "Epoch 055 | Loss: 2.1109 | Recon: 3.0085 | KL: 0.0167\n",
      "Epoch 056 | Loss: 2.1763 | Recon: 3.1029 | KL: 0.0144\n",
      "Epoch 057 | Loss: 2.1469 | Recon: 3.0618 | KL: 0.0123\n",
      "Epoch 058 | Loss: 2.1108 | Recon: 3.0110 | KL: 0.0105\n",
      "Epoch 059 | Loss: 2.1318 | Recon: 3.0416 | KL: 0.0090\n",
      "Epoch 060 | Loss: 2.1353 | Recon: 3.0471 | KL: 0.0078\n",
      "Epoch 061 | Loss: 2.1651 | Recon: 3.0901 | KL: 0.0067\n",
      "Epoch 062 | Loss: 2.1620 | Recon: 3.0861 | KL: 0.0057\n",
      "Epoch 063 | Loss: 2.1569 | Recon: 3.0791 | KL: 0.0050\n",
      "Epoch 064 | Loss: 2.1281 | Recon: 3.0383 | KL: 0.0042\n",
      "Epoch 065 | Loss: 2.1309 | Recon: 3.0426 | KL: 0.0037\n",
      "Epoch 066 | Loss: 2.1239 | Recon: 3.0327 | KL: 0.0033\n",
      "Epoch 067 | Loss: 2.1265 | Recon: 3.0366 | KL: 0.0029\n",
      "Epoch 068 | Loss: 2.1350 | Recon: 3.0490 | KL: 0.0026\n",
      "Epoch 069 | Loss: 2.1488 | Recon: 3.0687 | KL: 0.0023\n",
      "Epoch 070 | Loss: 2.1695 | Recon: 3.0984 | KL: 0.0021\n",
      "Epoch 071 | Loss: 2.1352 | Recon: 3.0495 | KL: 0.0019\n",
      "Epoch 072 | Loss: 2.1311 | Recon: 3.0437 | KL: 0.0018\n",
      "Epoch 073 | Loss: 2.1309 | Recon: 3.0434 | KL: 0.0017\n",
      "Epoch 074 | Loss: 2.1554 | Recon: 3.0784 | KL: 0.0016\n",
      "Epoch 075 | Loss: 2.1502 | Recon: 3.0711 | KL: 0.0015\n",
      "Epoch 076 | Loss: 2.1586 | Recon: 3.0832 | KL: 0.0014\n",
      "Epoch 077 | Loss: 2.1449 | Recon: 3.0636 | KL: 0.0013\n",
      "Epoch 078 | Loss: 2.1296 | Recon: 3.0418 | KL: 0.0012\n",
      "Epoch 079 | Loss: 2.1417 | Recon: 3.0591 | KL: 0.0012\n",
      "Epoch 080 | Loss: 2.1510 | Recon: 3.0724 | KL: 0.0011\n",
      "Epoch 081 | Loss: 2.1756 | Recon: 3.1075 | KL: 0.0011\n",
      "Epoch 082 | Loss: 2.1533 | Recon: 3.0757 | KL: 0.0010\n",
      "Epoch 083 | Loss: 2.1416 | Recon: 3.0590 | KL: 0.0010\n",
      "Epoch 084 | Loss: 2.1345 | Recon: 3.0488 | KL: 0.0010\n",
      "Epoch 085 | Loss: 2.1699 | Recon: 3.0995 | KL: 0.0009\n",
      "Epoch 086 | Loss: 2.1524 | Recon: 3.0744 | KL: 0.0010\n",
      "Epoch 087 | Loss: 2.1003 | Recon: 3.0001 | KL: 0.0009\n",
      "Epoch 088 | Loss: 2.1279 | Recon: 3.0395 | KL: 0.0009\n",
      "Epoch 089 | Loss: 2.1402 | Recon: 3.0570 | KL: 0.0009\n",
      "Epoch 090 | Loss: 2.1384 | Recon: 3.0545 | KL: 0.0009\n",
      "Epoch 091 | Loss: 2.1374 | Recon: 3.0531 | KL: 0.0008\n",
      "Epoch 092 | Loss: 2.1561 | Recon: 3.0798 | KL: 0.0008\n",
      "Epoch 093 | Loss: 2.1576 | Recon: 3.0819 | KL: 0.0008\n",
      "Epoch 094 | Loss: 2.1239 | Recon: 3.0338 | KL: 0.0008\n",
      "Epoch 095 | Loss: 2.1326 | Recon: 3.0462 | KL: 0.0007\n",
      "Epoch 096 | Loss: 2.1695 | Recon: 3.0990 | KL: 0.0007\n",
      "Epoch 097 | Loss: 2.1149 | Recon: 3.0210 | KL: 0.0007\n",
      "Epoch 098 | Loss: 2.1332 | Recon: 3.0471 | KL: 0.0007\n",
      "Epoch 099 | Loss: 2.1430 | Recon: 3.0611 | KL: 0.0007\n",
      "Epoch 100 | Loss: 2.1222 | Recon: 3.0314 | KL: 0.0006\n",
      "Epoch 101 | Loss: 2.1358 | Recon: 3.0508 | KL: 0.0006\n",
      "Epoch 102 | Loss: 2.1473 | Recon: 3.0674 | KL: 0.0006\n",
      "Epoch 103 | Loss: 2.1466 | Recon: 3.0663 | KL: 0.0007\n",
      "Epoch 104 | Loss: 2.1653 | Recon: 3.0930 | KL: 0.0006\n",
      "Epoch 105 | Loss: 2.0959 | Recon: 2.9939 | KL: 0.0006\n",
      "Epoch 106 | Loss: 2.1739 | Recon: 3.1053 | KL: 0.0006\n",
      "Epoch 107 | Loss: 2.1369 | Recon: 3.0525 | KL: 0.0005\n",
      "Epoch 108 | Loss: 2.1263 | Recon: 3.0373 | KL: 0.0006\n",
      "Epoch 109 | Loss: 2.1376 | Recon: 3.0535 | KL: 0.0005\n",
      "Epoch 110 | Loss: 2.1170 | Recon: 3.0241 | KL: 0.0005\n",
      "Epoch 111 | Loss: 2.1736 | Recon: 3.1050 | KL: 0.0005\n",
      "Epoch 112 | Loss: 2.1186 | Recon: 3.0264 | KL: 0.0005\n",
      "Epoch 113 | Loss: 2.1281 | Recon: 3.0400 | KL: 0.0005\n",
      "Epoch 114 | Loss: 2.1727 | Recon: 3.1036 | KL: 0.0005\n",
      "Epoch 115 | Loss: 2.1487 | Recon: 3.0693 | KL: 0.0005\n",
      "Epoch 116 | Loss: 2.1472 | Recon: 3.0671 | KL: 0.0005\n",
      "Epoch 117 | Loss: 2.1201 | Recon: 3.0285 | KL: 0.0005\n",
      "Epoch 118 | Loss: 2.1512 | Recon: 3.0729 | KL: 0.0005\n",
      "Epoch 119 | Loss: 2.1537 | Recon: 3.0765 | KL: 0.0005\n",
      "Epoch 120 | Loss: 2.1282 | Recon: 3.0401 | KL: 0.0005\n",
      "Epoch 121 | Loss: 2.1567 | Recon: 3.0807 | KL: 0.0005\n",
      "Epoch 122 | Loss: 2.1386 | Recon: 3.0550 | KL: 0.0005\n",
      "Epoch 123 | Loss: 2.1509 | Recon: 3.0725 | KL: 0.0005\n",
      "Epoch 124 | Loss: 2.1857 | Recon: 3.1222 | KL: 0.0005\n",
      "Epoch 125 | Loss: 2.1131 | Recon: 3.0185 | KL: 0.0004\n",
      "Epoch 126 | Loss: 2.1465 | Recon: 3.0662 | KL: 0.0005\n",
      "Epoch 127 | Loss: 2.1403 | Recon: 3.0573 | KL: 0.0005\n",
      "Epoch 128 | Loss: 2.1609 | Recon: 3.0868 | KL: 0.0004\n",
      "Epoch 129 | Loss: 2.1562 | Recon: 3.0801 | KL: 0.0005\n",
      "Epoch 130 | Loss: 2.1492 | Recon: 3.0702 | KL: 0.0004\n",
      "Epoch 131 | Loss: 2.1222 | Recon: 3.0316 | KL: 0.0004\n",
      "Epoch 132 | Loss: 2.1538 | Recon: 3.0766 | KL: 0.0005\n",
      "Epoch 133 | Loss: 2.1702 | Recon: 3.1002 | KL: 0.0004\n",
      "Epoch 134 | Loss: 2.1404 | Recon: 3.0575 | KL: 0.0004\n",
      "Epoch 135 | Loss: 2.1679 | Recon: 3.0968 | KL: 0.0005\n",
      "Epoch 136 | Loss: 2.1444 | Recon: 3.0632 | KL: 0.0004\n",
      "Epoch 137 | Loss: 2.1020 | Recon: 3.0027 | KL: 0.0004\n",
      "Epoch 138 | Loss: 2.1046 | Recon: 3.0063 | KL: 0.0004\n",
      "Epoch 139 | Loss: 2.1363 | Recon: 3.0517 | KL: 0.0004\n",
      "Epoch 140 | Loss: 2.1400 | Recon: 3.0570 | KL: 0.0004\n",
      "Epoch 141 | Loss: 2.1403 | Recon: 3.0574 | KL: 0.0004\n",
      "Epoch 142 | Loss: 2.1528 | Recon: 3.0753 | KL: 0.0004\n",
      "Epoch 143 | Loss: 2.1284 | Recon: 3.0404 | KL: 0.0004\n",
      "Epoch 144 | Loss: 2.1695 | Recon: 3.0991 | KL: 0.0004\n",
      "Epoch 145 | Loss: 2.1552 | Recon: 3.0786 | KL: 0.0004\n",
      "Epoch 146 | Loss: 2.1377 | Recon: 3.0537 | KL: 0.0004\n",
      "Epoch 147 | Loss: 2.1441 | Recon: 3.0628 | KL: 0.0004\n",
      "Epoch 148 | Loss: 2.1500 | Recon: 3.0712 | KL: 0.0004\n",
      "Epoch 149 | Loss: 2.1684 | Recon: 3.0975 | KL: 0.0004\n",
      "Epoch 150 | Loss: 2.1486 | Recon: 3.0693 | KL: 0.0004\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vae = MaskedVAE(input_dim=4, hidden_dim=4, latent_dim=4)\n",
    "vae = train_masked_vae(vae, dataloader, device,epochs=150,lr=1e-3)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code ran for 1.18 seconds\n"
     ]
    }
   ],
   "source": [
    "elapsed = end_time - start_time\n",
    "print(f\"Code ran for {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = extract_latent_features(vae, X_scaled,mask_tensor,device)\n",
    "latent_np = latent.numpy()"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1769068019794,
  "creator": "2018509",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env offus)",
   "language": "python",
   "name": "py-dku-venv-offus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "modifiedBy": "2018509",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
