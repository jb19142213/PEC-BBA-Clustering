{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"createdOn":1769068019794,"creator":"2018509","customFields":{},"hide_input":false,"modifiedBy":"2018509","tags":[],"versionNumber":1,"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14662381,"sourceType":"datasetVersion","datasetId":9366947}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch import nn, optim\nimport numpy as np\nfrom tqdm import tqdm\nfrom torchvision.utils import save_image, make_grid\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:43:02.014314Z","iopub.execute_input":"2026-01-29T10:43:02.014914Z","iopub.status.idle":"2026-01-29T10:43:15.267181Z","shell.execute_reply.started":"2026-01-29T10:43:02.014869Z","shell.execute_reply":"2026-01-29T10:43:15.266608Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ============================================================\n# Stage 2: Distributional Evidential Clustering (Option 3)\n# ------------------------------------------------------------\n# Input  : U  -> latent features from Stage 1 (masked AE/VAE)\n# Output : fuzzy memberships, hard labels, cluster parameters\n#\n# Key design choices (intentional):\n# - Clusters are Gaussian distributions in latent space\n# - Memberships are evidential (fuzzy), not probabilistic\n# - Distance is negative log-likelihood (not Euclidean)\n# - Explicit cluster–cluster divergence regularization\n# - Diagonal covariance for stability and efficiency\n# - No EM, no mixture weights, no DST\n# ============================================================\n\n\n# ------------------------------------------------------------\n# Negative log-likelihood under a diagonal Gaussian\n# This replaces point-wise Euclidean distance\n# ------------------------------------------------------------\ndef neg_log_likelihood(u, mu, var, eps=1e-6):\n    \"\"\"\n    u   : latent feature vector of a sample (d,)\n    mu  : cluster mean (d,)\n    var : diagonal variance of cluster (d,)\n    \"\"\"\n    d = u.shape[0]\n    return 0.5 * (\n        np.sum(np.log(var + eps)) +\n        np.sum((u - mu) ** 2 / (var + eps)) +\n        d * np.log(2 * np.pi)\n    )\n\n\n# ------------------------------------------------------------\n# KL divergence between two diagonal Gaussian clusters\n# Used to explicitly penalize cluster similarity\n# ------------------------------------------------------------\ndef kl_divergence(mu1, var1, mu2, var2, eps=1e-6):\n    \"\"\"\n    Computes KL( N(mu1,var1) || N(mu2,var2) )\n    \"\"\"\n    return 0.5 * np.sum(\n        np.log((var2 + eps) / (var1 + eps)) +\n        (var1 + (mu1 - mu2) ** 2) / (var2 + eps) - 1.0\n    )\n\n\n# ------------------------------------------------------------\n# Initialization\n# - Means initialized from random latent points\n# - Variances initialized to ones\n# - Memberships initialized uniformly (maximum uncertainty)\n# ------------------------------------------------------------\ndef initialize_clusters(U, n_clusters, seed=42):\n    np.random.seed(seed)\n    n, d = U.shape\n\n    idx = np.random.choice(n, n_clusters, replace=False)\n    mu = U[idx].copy()\n    var = np.ones((n_clusters, d))           # diagonal covariance\n    M = np.ones((n, n_clusters)) / n_clusters\n\n    return mu, var, M\n\n\n# ------------------------------------------------------------\n# Compute divergence penalty for each cluster\n# Δ_j = sum_{k≠j} KL(C_j || C_k)\n# ------------------------------------------------------------\ndef compute_cluster_penalties(mu, var):\n    c = mu.shape[0]\n    Delta = np.zeros(c)\n\n    for j in range(c):\n        for k in range(c):\n            if j != k:\n                Delta[j] += kl_divergence(\n                    mu[j], var[j],\n                    mu[k], var[k]\n                )\n    return Delta\n\n\n# ------------------------------------------------------------\n# Membership update (Option 3)\n# Implements Eq. (4) in the paper\n# Uses direct exponentiation (no logsumexp)\n# ------------------------------------------------------------\ndef update_memberships(U, mu, var, beta, gamma, eps=1e-12):\n    \"\"\"\n    Memberships are updated using likelihood + divergence cost.\n    This is NOT a posterior probability (unlike GMM).\n    \"\"\"\n    n, _ = U.shape\n    c = mu.shape[0]\n\n    Delta = compute_cluster_penalties(mu, var)\n    M = np.zeros((n, c))\n\n    for i in range(n):\n        numerators = np.zeros(c)\n\n        for j in range(c):\n            # Combined cost:\n            # data-to-cluster fit + cluster-separation penalty\n            cost_ij = (\n                neg_log_likelihood(U[i], mu[j], var[j])\n                + gamma * Delta[j]\n            )\n\n            numerators[j] = np.exp(-cost_ij / (beta - 1.0))\n\n        # Normalize memberships to sum to 1\n        M[i, :] = numerators / (np.sum(numerators) + eps)\n\n    return M\n\n\n# ------------------------------------------------------------\n# Cluster parameter update\n# Weighted maximum likelihood using evidential memberships\n# Implements Eq. (5) and Eq. (6)\n# ------------------------------------------------------------\ndef update_clusters(U, M, beta, eps=1e-8):\n    n, d = U.shape\n    c = M.shape[1]\n\n    mu_new = np.zeros((c, d))\n    var_new = np.zeros((c, d))\n    M_beta = M ** beta\n\n    for j in range(c):\n        w = M_beta[:, j][:, None]\n        denom = np.sum(w) + eps\n\n        # Mean update\n        mu_new[j] = np.sum(w * U, axis=0) / denom\n\n        # Diagonal covariance update\n        var_new[j] = np.sum(\n            w * (U - mu_new[j]) ** 2, axis=0\n        ) / denom\n\n    return mu_new, var_new\n\n# ------------------------------------------------------------\n# Calculate objective function I(M, C)\n# I(M, C) = Σ_i Σ_j m_ij^β [ℓ_ij + γ·Δ_j]\n# ------------------------------------------------------------\ndef calculate_objective(U, M, mu, var, beta, gamma):\n    \"\"\"\n    Calculate the clustering objective I(M, C)\n    \"\"\"\n    n = U.shape[0]\n    c = mu.shape[0]\n    \n    # Compute divergence penalties for each cluster\n    Delta = compute_cluster_penalties(mu, var)\n    \n    total_obj = 0.0\n    \n    for i in range(n):\n        for j in range(c):\n            # Negative log-likelihood for sample i in cluster j\n            nll = neg_log_likelihood(U[i], mu[j], var[j])\n            \n            # Combined cost: nll + divergence penalty\n            cost_ij = nll + gamma * Delta[j]\n            \n            # Weight by membership raised to beta\n            total_obj += (M[i, j] ** beta) * cost_ij\n    \n    return total_obj\n\n\n# ------------------------------------------------------------\n# Main optimization loop (Algorithm 1 in paper) with objective tracking\n# ------------------------------------------------------------\ndef distributional_evidential_clustering(\n    U,\n    n_clusters,\n    beta=2.0,\n    gamma=0.05,\n    max_iter=30,\n    tol=1e-4,\n    seed=42,\n    verbose=True):\n    \"\"\"\n    Performs distributional evidential clustering on latent features.\n    Returns clustering results including objective function values.\n    \"\"\"\n    # Initialize\n    mu, var, M = initialize_clusters(U, n_clusters, seed)\n    \n    # Track objective values\n    objectives = []\n    \n    for it in range(max_iter):\n        mu_old = mu.copy()\n        var_old = var.copy()\n        \n        # Calculate objective before updates\n        obj_before = calculate_objective(U, M, mu, var, beta, gamma)\n        objectives.append(obj_before)\n        \n        if verbose and it % 10 == 0:\n            print(f\"Iteration {it} running\")\n        \n        # Step 1: update memberships\n        M = update_memberships(U, mu, var, beta, gamma)\n        \n        # Step 2: update cluster distributions\n        mu, var = update_clusters(U, M, beta)\n        \n        # Calculate objective after updates\n        obj_after = calculate_objective(U, M, mu, var, beta, gamma)\n        \n        # Convergence check (multiple criteria)\n        shift_mean = np.max(np.linalg.norm(mu - mu_old, axis=1))\n        shift_obj = abs(obj_after - obj_before)\n        \n        if shift_mean < tol and shift_obj < tol:\n            if verbose:\n                print(f\"Converged at iteration {it}\")\n            objectives.append(obj_after)\n            break\n            \n        if it == max_iter - 1 and verbose:\n            print(f\"Reached maximum iterations ({max_iter})\")\n    \n    # Final labels\n    labels = np.argmax(M, axis=1)\n    \n    # Calculate final objective\n    final_objective = calculate_objective(U, M, mu, var, beta, gamma)\n    if len(objectives) < max_iter:\n        objectives.append(final_objective)\n    \n    return {\n        \"memberships\": M,\n        \"labels\": labels,\n        \"mu\": mu,\n        \"var\": var,\n        \"objectives\": np.array(objectives),\n        \"final_objective\": final_objective\n    }\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nmetrics.py\nClustering evaluation metrics for algorithms that assign single labels (no meta-clusters)\n-1 indicates outliers\n\"\"\"\n\nimport numpy as np\nfrom itertools import combinations\nfrom scipy.special import comb\n\ndef Re(original_label, pred_label):\n    \"\"\"\n    Error Rate (Re)\n    \n    Calculates the ratio of misclassifications to total number of objects.\n    \n    Definition: Re = Ne / N\n    where Ne = number of misclassifications\n          N  = total number of objects\n    \n    Misclassification occurs when:\n    1. pred_label[i] != original_label[i] (wrong cluster assignment)\n    2. pred_label[i] == -1 (outlier) but original_label[i] is not an outlier*\n    \n    *Note: Assumes original_label has no outlier class (-1)\n    \n    Parameters:\n    -----------\n    original_label : array-like, shape (n_samples,)\n        Ground truth cluster assignments (non-negative integers)\n    pred_label : array-like, shape (n_samples,)\n        Predicted cluster assignments (-1 for outliers)\n        \n    Returns:\n    --------\n    re : float\n        Error rate (0 to 1, lower is better)\n    \"\"\"\n    n = len(original_label)\n    if n == 0:\n        return 0.0\n    \n    ne = 0  # Number of misclassifications\n    \n    for i in range(n):\n        true_cluster = original_label[i]\n        pred_cluster = pred_label[i]\n        \n        # Misclassification if:\n        # 1. Wrong cluster assignment\n        # 2. Predicted as outlier (-1) but true is not outlier\n        if pred_cluster == -1:\n            # If predicted as outlier, always misclassification \n            # (since original_label has no outlier class)\n            ne += 1\n        elif pred_cluster != true_cluster:\n            ne += 1\n    \n    return ne / n\n\ndef Ri(original_label, pred_label):\n    \"\"\"\n    Imprecision Rate (Ri)\n    \n    In your algorithm without meta-clusters, this should always return 0\n    since there are no meta-cluster assignments.\n    \n    Definition: Ri = Ni / N\n    where Ni = number of objects in meta-clusters\n          N  = total number of objects\n    \n    Parameters:\n    -----------\n    original_label : array-like, shape (n_samples,)\n        Ground truth cluster assignments\n    pred_label : array-like, shape (n_samples,)\n        Predicted cluster assignments\n        \n    Returns:\n    --------\n    ri : float\n        Always 0 (no meta-clusters)\n    \"\"\"\n    return 0.0\n\ndef EP(original_label, pred_label):\n    \"\"\"\n    Evidential Precision (EP) - Simplified for single label assignments\n    \n    Measures precision of non-outlier assignments.\n    \n    Definition: EP = TP* / (TP* + FP*)\n    where TP* = number of pairs of similar objects (same true cluster) \n                that are both assigned to the same cluster (and not outliers)\n          FP* = number of pairs of dissimilar objects (different true clusters)\n                that are both assigned to the same cluster (and not outliers)\n    \n    Only considers pairs where both objects are NOT outliers (pred_label != -1)\n    \n    Parameters:\n    -----------\n    original_label : array-like, shape (n_samples,)\n        Ground truth cluster assignments\n    pred_label : array-like, shape (n_samples,)\n        Predicted cluster assignments (-1 for outliers)\n        \n    Returns:\n    --------\n    ep : float\n        Evidential precision (0 to 1, higher is better)\n    \"\"\"\n    n = len(original_label)\n    \n    # Find indices of non-outlier predictions\n    non_outlier_indices = [i for i in range(n) if pred_label[i] != -1]\n    \n    if len(non_outlier_indices) < 2:\n        return 0.0  # Need at least 2 non-outliers to compute pairs\n    \n    tp_star = 0  # True positive pairs (similar objects in same cluster)\n    fp_star = 0  # False positive pairs (dissimilar objects in same cluster)\n    \n    # Consider all pairs of non-outlier objects\n    for i, j in combinations(non_outlier_indices, 2):\n        if i >= j:\n            continue\n            \n        true_i = original_label[i]\n        true_j = original_label[j]\n        pred_i = pred_label[i]\n        pred_j = pred_label[j]\n        \n        # Check if assigned to the same cluster\n        if pred_i == pred_j:\n            if true_i == true_j:\n                tp_star += 1\n            else:\n                fp_star += 1\n    \n    denominator = tp_star + fp_star\n    return tp_star / denominator if denominator > 0 else 0.0\n\ndef ERI(original_label, pred_label):\n    \"\"\"\n    Evidential Rank Index (ERI) - Simplified for single label assignments\n    \n    Comprehensive evaluation considering both beneficial and detrimental elements.\n    \n    Definition: ERI = 2(TP* + TN*) / [N(N-1)]\n    where TP* = number of pairs of similar objects (same true cluster) \n                that are both assigned to the same cluster (and not outliers)\n          TN* = number of pairs of dissimilar objects (different true clusters)\n                that are assigned to different clusters (and both not outliers)\n          N   = total number of objects\n    \n    Only considers pairs where both objects are NOT outliers (pred_label != -1)\n    \n    Parameters:\n    -----------\n    original_label : array-like, shape (n_samples,)\n        Ground truth cluster assignments\n    pred_label : array-like, shape (n_samples,)\n        Predicted cluster assignments (-1 for outliers)\n        \n    Returns:\n    --------\n    eri : float\n        Evidential rank index (0 to 1, higher is better)\n    \"\"\"\n    n = len(original_label)\n    \n    if n < 2:\n        return 0.0\n    \n    # Find indices of non-outlier predictions\n    non_outlier_indices = [i for i in range(n) if pred_label[i] != -1]\n    \n    tp_star = 0  # True positive pairs\n    tn_star = 0  # True negative pairs\n    \n    # Consider all pairs of non-outlier objects\n    for i, j in combinations(non_outlier_indices, 2):\n        if i >= j:\n            continue\n            \n        true_i = original_label[i]\n        true_j = original_label[j]\n        pred_i = pred_label[i]\n        pred_j = pred_label[j]\n        \n        same_true = (true_i == true_j)\n        same_pred = (pred_i == pred_j)\n        \n        if same_true and same_pred:\n            tp_star += 1\n        elif (not same_true) and (not same_pred):\n            tn_star += 1\n    \n    total_pairs = n * (n - 1)\n    return 2 * (tp_star + tn_star) / total_pairs if total_pairs > 0 else 0.0\n\ndef rand_index(original_label, pred_label):\n    \"\"\"\n    Rand Index (RI) - Standard version\n    \n    Measures the similarity between two data clusterings.\n    \n    Definition: RI = (TP + TN) / (TP + FP + FN + TN)\n    where TP = number of pairs of similar objects (same true cluster) \n               that are both assigned to the same predicted cluster\n          TN = number of pairs of dissimilar objects (different true clusters)\n               that are assigned to different predicted clusters\n          FP = number of pairs of dissimilar objects \n               that are assigned to the same predicted cluster\n          FN = number of pairs of similar objects \n               that are assigned to different predicted clusters\n    \n    Treats -1 (outliers) as a separate cluster.\n    \n    Parameters:\n    -----------\n    original_label : array-like, shape (n_samples,)\n        Ground truth cluster assignments\n    pred_label : array-like, shape (n_samples,)\n        Predicted cluster assignments\n        \n    Returns:\n    --------\n    ri : float\n        Rand Index (0 to 1, higher is better)\n    \"\"\"\n    n = len(original_label)\n    \n    if n < 2:\n        return 0.0\n    \n    tp = 0  # True positives\n    tn = 0  # True negatives\n    fp = 0  # False positives\n    fn = 0  # False negatives\n    \n    # Consider all pairs of objects\n    for i, j in combinations(range(n), 2):\n        if i >= j:\n            continue\n            \n        true_i = original_label[i]\n        true_j = original_label[j]\n        pred_i = pred_label[i]\n        pred_j = pred_label[j]\n        \n        same_true = (true_i == true_j)\n        same_pred = (pred_i == pred_j)\n        \n        if same_true and same_pred:\n            tp += 1\n        elif same_true and not same_pred:\n            fn += 1\n        elif not same_true and same_pred:\n            fp += 1\n        else:  # not same_true and not same_pred\n            tn += 1\n    \n    total = tp + tn + fp + fn\n    return (tp + tn) / total if total > 0 else 0.0\n\ndef adjusted_rand_index(original_label, pred_label):\n    \"\"\"\n    Adjusted Rand Index (ARI)\n    \n    Adjusted for chance version of the Rand Index.\n    \n    Definition: ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n    where RI = Rand Index\n          Expected_RI = expected value of RI under random assignment\n    \n    Parameters:\n    -----------\n    original_label : array-like, shape (n_samples,)\n        Ground truth cluster assignments\n    pred_label : array-like, shape (n_samples,)\n        Predicted cluster assignments\n        \n    Returns:\n    --------\n    ari : float\n        Adjusted Rand Index (-1 to 1, higher is better, 0 = random)\n    \"\"\"\n    n = len(original_label)\n    \n    if n < 2:\n        return 0.0\n    \n    # Create contingency table\n    true_clusters = np.unique(original_label)\n    pred_clusters = np.unique(pred_label)\n    \n    # Create contingency matrix\n    contingency = np.zeros((len(true_clusters), len(pred_clusters)), dtype=int)\n    \n    # Map clusters to indices\n    true_to_idx = {cluster: idx for idx, cluster in enumerate(true_clusters)}\n    pred_to_idx = {cluster: idx for idx, cluster in enumerate(pred_clusters)}\n    \n    # Fill contingency matrix\n    for i in range(n):\n        true_idx = true_to_idx[original_label[i]]\n        pred_idx = pred_to_idx[pred_label[i]]\n        contingency[true_idx, pred_idx] += 1\n    \n    # Calculate row and column sums\n    a = contingency.sum(axis=1)  # Row sums\n    b = contingency.sum(axis=0)  # Column sums\n    \n    # Calculate index (sum over cells of n_ij choose 2)\n    index = np.sum([comb(n_ij, 2) for n_ij in contingency.flatten() if n_ij >= 2])\n    \n    # Calculate expected index\n    sum_ai = np.sum([comb(ai, 2) for ai in a if ai >= 2])\n    sum_bj = np.sum([comb(bj, 2) for bj in b if bj >= 2])\n    expected_index = sum_ai * sum_bj / comb(n, 2)\n    \n    # Calculate max index\n    max_index = (sum_ai + sum_bj) / 2\n    \n    # Calculate ARI\n    if max_index == expected_index:\n        return 0.0\n    else:\n        ari = (index - expected_index) / (max_index - expected_index)\n        return ari\n\ndef compute_all_metrics(original_label, pred_label):\n    \"\"\"\n    Compute all 6 clustering metrics.\n    \n    Parameters:\n    -----------\n    original_label : array-like, shape (n_samples,)\n        Ground truth cluster assignments\n    pred_label : array-like, shape (n_samples,)\n        Predicted cluster assignments (-1 for outliers)\n        \n    Returns:\n    --------\n    metrics : dict\n        Dictionary containing all metrics\n    \"\"\"\n    metrics = {}\n    \n    # Evidential clustering metrics (simplified)\n    metrics['Re'] = Re(original_label, pred_label)\n    metrics['Ri'] = Ri(original_label, pred_label)  # Always 0\n    metrics['EP'] = EP(original_label, pred_label)\n    metrics['ERI'] = ERI(original_label, pred_label)\n    \n    # Standard clustering metrics\n    metrics['RI'] = rand_index(original_label, pred_label)\n    metrics['ARI'] = adjusted_rand_index(original_label, pred_label)\n    \n    return metrics","metadata":{"code_folding":[10,60,85,142,201,266,332],"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:43:19.757186Z","iopub.execute_input":"2026-01-29T10:43:19.757956Z","iopub.status.idle":"2026-01-29T10:43:20.662452Z","shell.execute_reply.started":"2026-01-29T10:43:19.757926Z","shell.execute_reply":"2026-01-29T10:43:20.661832Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:43:28.765859Z","iopub.execute_input":"2026-01-29T10:43:28.766712Z","iopub.status.idle":"2026-01-29T10:43:28.824571Z","shell.execute_reply.started":"2026-01-29T10:43:28.766674Z","shell.execute_reply":"2026-01-29T10:43:28.823957Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# =========================\n# Masked Variational Autoencoder\n# =========================\n\nclass MaskedVAE(nn.Module):\n    \"\"\"\n    Masked Variational Autoencoder for incomplete data.\n    Encoder input: [x ⊙ m , m]\n    Latent output: mu (used as latent feature u_i)\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dim=128, latent_dim=8):\n        super().__init__()\n\n        # Encoder: input_dim * 2 because of concatenation with mask\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim * 2, hidden_dim),\n            nn.LeakyReLU(0.1),\n            nn.Linear(hidden_dim, hidden_dim - 2),\n            nn.LeakyReLU(0.1),\n            nn.Linear(hidden_dim - 2, hidden_dim - 4),\n            nn.LeakyReLU(0.1)\n        )\n\n        \n        self.fc_mu = nn.Linear(hidden_dim - 4, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim - 4, latent_dim)\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim-4),\n            nn.LeakyReLU(0.1),\n            nn.Linear(hidden_dim-4, hidden_dim-2),\n            nn.LeakyReLU(0.1),\n            nn.Linear(hidden_dim-2, hidden_dim),\n            nn.LeakyReLU(0.1),\n            nn.Linear(hidden_dim, input_dim)\n        )\n\n    def encode(self, x, mask):\n        x_masked = x * mask\n        enc_input = torch.cat([x_masked, mask], dim=1)\n        h = self.encoder(enc_input)\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        return self.decoder(z)\n\n    def forward(self, x, mask):\n        mu, logvar = self.encode(x, mask)\n        z = self.reparameterize(mu, logvar)\n        recon = self.decode(z)\n        return recon, mu, logvar\n\n\n# =========================\n# Masked VAE Loss\n# =========================\n\ndef masked_vae_loss(\n    recon_x,\n    x,\n    mask,\n    mu,\n    logvar,\n    recon_weight=1,\n    kl_weight=0,\n    eps=1e-8\n):\n    \"\"\"\n    Masked reconstruction + KL divergence loss\n    \"\"\"\n\n    # Masked reconstruction loss (MSE over observed entries only)\n    se = (recon_x - x) ** 2\n    masked_se = se * mask\n    recon_loss = masked_se.sum(dim=1) / (mask.sum(dim=1) + eps)\n    recon_loss = recon_loss.mean()\n\n    # KL divergence\n    kl_loss = -0.5 * torch.mean(\n        torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n    )\n\n    total_loss = recon_weight * recon_loss + kl_weight * kl_loss\n    return total_loss, recon_loss.item(), kl_loss.item()\n\n\n# =========================\n# Training Function\n# =========================\n\ndef train_masked_vae(\n    model,\n    dataloader,\n    device,\n    epochs=50,\n    lr=1e-3\n):\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    model.train()\n    for epoch in range(1, epochs + 1):\n        total_loss = 0.0\n        total_rec = 0.0\n        total_kl = 0.0\n\n        for x, mask in dataloader:\n            x = x.to(device).float()\n            mask = mask.to(device).float()\n\n            optimizer.zero_grad()\n            recon, mu, logvar = model(x, mask)\n            loss, rec, kl = masked_vae_loss(recon, x, mask, mu, logvar)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            total_rec += rec\n            total_kl += kl\n\n        print(\n            f\"Epoch {epoch:03d} | \"\n            f\"Loss: {total_loss:.4f} | \"\n            f\"Recon: {total_rec:.4f} | \"\n        )\n\n    return model\n\n\n# =========================\n# Latent Feature Extraction\n# =========================\n\ndef extract_latent_features(model, X, mask, device):\n    \"\"\"\n    Returns latent feature vectors u_i = mu_i\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        X = X.to(device).float()\n        mask = mask.to(device).float()\n        mu, _ = model.encode(X, mask)\n    return mu.cpu()","metadata":{"code_folding":[4,65,74,98,104,141],"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:51:02.931290Z","iopub.execute_input":"2026-01-29T10:51:02.931691Z","iopub.status.idle":"2026-01-29T10:51:02.946836Z","shell.execute_reply.started":"2026-01-29T10:51:02.931662Z","shell.execute_reply":"2026-01-29T10:51:02.946212Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# ============================================\n# 1. Load  Dataset\n# ============================================\nfrom sklearn.datasets import fetch_openml\nimport pandas as pd\n\ndef load_uci_dataset(name, target_col=None):\n    \"\"\"\n    Load a UCI dataset from OpenML.\n\n    Returns\n    -------\n    X : np.ndarray\n    y : np.ndarray\n    df : pd.DataFrame (features + target)\n    \"\"\"\n    data = fetch_openml(name=name, as_frame=True)\n    df = data.frame\n\n    if target_col is None:\n        target_col = data.target.name\n\n    X = df.drop(columns=[target_col]).values\n    y = df[target_col].values\n\n    return X, y, df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:01:02.888595Z","iopub.execute_input":"2026-01-29T08:01:02.888943Z","iopub.status.idle":"2026-01-29T08:01:03.431407Z","shell.execute_reply.started":"2026-01-29T08:01:02.888916Z","shell.execute_reply":"2026-01-29T08:01:03.430540Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# How to load\n_, true_labels, df = load_uci_dataset(\"ilpd\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:27:06.497156Z","iopub.execute_input":"2026-01-29T08:27:06.497660Z","iopub.status.idle":"2026-01-29T08:27:10.153220Z","shell.execute_reply.started":"2026-01-29T08:27:06.497627Z","shell.execute_reply":"2026-01-29T08:27:10.152650Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:28:14.489706Z","iopub.execute_input":"2026-01-29T08:28:14.490261Z","iopub.status.idle":"2026-01-29T08:28:14.503372Z","shell.execute_reply.started":"2026-01-29T08:28:14.490235Z","shell.execute_reply":"2026-01-29T08:28:14.502630Z"}},"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"     V1  V2    V3   V4   V5  V6   V7   V8   V9   V10 Class\n0    65   0   0.7  0.1  187  16   18  6.8  3.3  0.90     1\n1    62   1  10.9  5.5  699  64  100  7.5  3.2  0.74     1\n2    62   1   7.3  4.1  490  60   68  7.0  3.3  0.89     1\n3    58   1   1.0  0.4  182  14   20  6.8  3.4  1.00     1\n4    72   1   3.9  2.0  195  27   59  7.3  2.4  0.40     1\n..   ..  ..   ...  ...  ...  ..  ...  ...  ...   ...   ...\n578  60   1   0.5  0.1  500  20   34  5.9  1.6  0.37     2\n579  40   1   0.6  0.1   98  35   31  6.0  3.2  1.10     1\n580  52   1   0.8  0.2  245  48   49  6.4  3.2  1.00     1\n581  31   1   1.3  0.5  184  29   32  6.8  3.4  1.00     1\n582  38   1   1.0  0.3  216  21   24  7.3  4.4  1.50     2\n\n[583 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>65</td>\n      <td>0</td>\n      <td>0.7</td>\n      <td>0.1</td>\n      <td>187</td>\n      <td>16</td>\n      <td>18</td>\n      <td>6.8</td>\n      <td>3.3</td>\n      <td>0.90</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62</td>\n      <td>1</td>\n      <td>10.9</td>\n      <td>5.5</td>\n      <td>699</td>\n      <td>64</td>\n      <td>100</td>\n      <td>7.5</td>\n      <td>3.2</td>\n      <td>0.74</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>62</td>\n      <td>1</td>\n      <td>7.3</td>\n      <td>4.1</td>\n      <td>490</td>\n      <td>60</td>\n      <td>68</td>\n      <td>7.0</td>\n      <td>3.3</td>\n      <td>0.89</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.4</td>\n      <td>182</td>\n      <td>14</td>\n      <td>20</td>\n      <td>6.8</td>\n      <td>3.4</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>1</td>\n      <td>3.9</td>\n      <td>2.0</td>\n      <td>195</td>\n      <td>27</td>\n      <td>59</td>\n      <td>7.3</td>\n      <td>2.4</td>\n      <td>0.40</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>578</th>\n      <td>60</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>0.1</td>\n      <td>500</td>\n      <td>20</td>\n      <td>34</td>\n      <td>5.9</td>\n      <td>1.6</td>\n      <td>0.37</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>579</th>\n      <td>40</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>0.1</td>\n      <td>98</td>\n      <td>35</td>\n      <td>31</td>\n      <td>6.0</td>\n      <td>3.2</td>\n      <td>1.10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>580</th>\n      <td>52</td>\n      <td>1</td>\n      <td>0.8</td>\n      <td>0.2</td>\n      <td>245</td>\n      <td>48</td>\n      <td>49</td>\n      <td>6.4</td>\n      <td>3.2</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>581</th>\n      <td>31</td>\n      <td>1</td>\n      <td>1.3</td>\n      <td>0.5</td>\n      <td>184</td>\n      <td>29</td>\n      <td>32</td>\n      <td>6.8</td>\n      <td>3.4</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>582</th>\n      <td>38</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.3</td>\n      <td>216</td>\n      <td>21</td>\n      <td>24</td>\n      <td>7.3</td>\n      <td>4.4</td>\n      <td>1.50</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>583 rows × 11 columns</p>\n</div>"},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"true_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:27:15.582660Z","iopub.execute_input":"2026-01-29T08:27:15.583352Z","iopub.status.idle":"2026-01-29T08:27:15.588317Z","shell.execute_reply.started":"2026-01-29T08:27:15.583324Z","shell.execute_reply":"2026-01-29T08:27:15.587740Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"['1', '1', '1', '1', '1', ..., '2', '1', '1', '1', '2']\nLength: 583\nCategories (2, object): ['1', '2']"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"df[\"V2\"]=[1 if x==\"Male\" else 0 for x in df[\"V2\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:28:10.454312Z","iopub.execute_input":"2026-01-29T08:28:10.454854Z","iopub.status.idle":"2026-01-29T08:28:10.459078Z","shell.execute_reply.started":"2026-01-29T08:28:10.454829Z","shell.execute_reply":"2026-01-29T08:28:10.458354Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# ============================================\n# 2. Inject Missing Values (MCAR)\n# ============================================\n\n#Remove the target column\nfeature_cols = df.drop(df.columns[-1], axis=1).columns\nX1 = df.drop(df.columns[-1], axis=1).to_numpy().astype(float)\n\ndef inject_missing_entries(X, missing_ratio, seed=42):\n    \"\"\"\n    Entry-wise MCAR missingness.\n    Guarantees ~missing_ratio fraction of ALL entries are missing.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    X_missing = X.copy()\n    n, d = X_missing.shape\n    total_entries = n * d\n    n_missing = int(missing_ratio * total_entries)\n\n    flat_indices = rng.choice(\n        total_entries, size=n_missing, replace=False\n    )\n\n    rows = flat_indices // d\n    cols = flat_indices % d\n\n    X_missing[rows, cols] = np.nan\n\n    return X_missing\n\nX_missing = inject_missing_entries(X1, missing_ratio=0)\n\ndf = df.copy()\ndf[feature_cols] = X_missing\ndf=df[feature_cols]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:43:48.138720Z","iopub.execute_input":"2026-01-29T10:43:48.139470Z","iopub.status.idle":"2026-01-29T10:43:48.154757Z","shell.execute_reply.started":"2026-01-29T10:43:48.139440Z","shell.execute_reply":"2026-01-29T10:43:48.153942Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"feature_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:28:27.458647Z","iopub.execute_input":"2026-01-29T08:28:27.459172Z","iopub.status.idle":"2026-01-29T08:28:27.463726Z","shell.execute_reply.started":"2026-01-29T08:28:27.459143Z","shell.execute_reply":"2026-01-29T08:28:27.462967Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10'], dtype='object')"},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"# ============================================\n# 3. Create Feature Matrix and Mask\n# ============================================\n\nX = df.values.astype(np.float32)\n\n# Binary mask: 1 = observed, 0 = missing\nmask = (~np.isnan(X)).astype(np.float32)\n\n# Fill missing values with zero (mask-aware)\nX_filled = np.nan_to_num(X, nan=0.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:43:57.178943Z","iopub.execute_input":"2026-01-29T10:43:57.179676Z","iopub.status.idle":"2026-01-29T10:43:57.185452Z","shell.execute_reply.started":"2026-01-29T10:43:57.179646Z","shell.execute_reply":"2026-01-29T10:43:57.184833Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# ============================================\n# 4. Mask-aware Standardization\n# ============================================\nimport numpy as np\nimport torch\n\nclass TorchScaler:\n    def fit(self, X: torch.Tensor):\n        self.mean = X.mean(dim=0, keepdim=True)\n        self.std = X.std(dim=0, unbiased=False, keepdim=True)\n        self.std[self.std == 0] = 1.0\n        return self\n    def transform(self, X: torch.Tensor):\n        return (X - self.mean) / self.std\n    def inverse_transform(self, X: torch.Tensor):\n        return X * self.std + self.mean\n\nX_scaled = X_filled.copy()\nX_tensor = torch.from_numpy(np.array(X_scaled)).float()  # ensure float type\nscaler = TorchScaler().fit(X_tensor)\nX_scaled = scaler.transform(X_tensor)  # use X_tensor, not X_scaled\n\nmask_tensor = torch.tensor(mask, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T10:44:22.065951Z","iopub.execute_input":"2026-01-29T10:44:22.066505Z","iopub.status.idle":"2026-01-29T10:44:22.103346Z","shell.execute_reply.started":"2026-01-29T10:44:22.066478Z","shell.execute_reply":"2026-01-29T10:44:22.102588Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"\n# ============================================\n# 5. PyTorch Dataset and DataLoader\n# ============================================\n    \nclass MaskedDataset:\n    def __init__(self, X, mask):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.mask = torch.tensor(mask, dtype=torch.float32)\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.mask[idx]\n\n\ndataset = MaskedDataset(X_scaled, mask_tensor)\ntorch.manual_seed(42)\ndataloader = DataLoader(dataset, batch_size=256, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Example Usage (Skeleton)\nvae = MaskedVAE(input_dim=10, hidden_dim=10, latent_dim=4)\nvae = train_masked_vae(vae, dataloader, device,epochs=500,lr=1e-3)\nlatent = extract_latent_features(vae, X_scaled,mask_tensor,device)\nlatent_np = latent.numpy()\nU=latent_np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = distributional_evidential_clustering(U,n_clusters=2,beta=2,gamma=0,max_iter=500,tol=1e-8,seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:29:31.206601Z","iopub.execute_input":"2026-01-29T08:29:31.207130Z","iopub.status.idle":"2026-01-29T08:29:36.387042Z","shell.execute_reply.started":"2026-01-29T08:29:31.207097Z","shell.execute_reply":"2026-01-29T08:29:36.386352Z"}},"outputs":[{"name":"stdout","text":"Iteration 0 running\nIteration 10 running\nIteration 20 running\nIteration 30 running\nIteration 40 running\nIteration 50 running\nIteration 60 running\nIteration 70 running\nIteration 80 running\nIteration 90 running\nConverged at iteration 95\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"ilpd_result=result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:32:22.277016Z","iopub.execute_input":"2026-01-29T08:32:22.277694Z","iopub.status.idle":"2026-01-29T08:32:22.281030Z","shell.execute_reply.started":"2026-01-29T08:32:22.277667Z","shell.execute_reply":"2026-01-29T08:32:22.280324Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"ilpd_df=pd.DataFrame({\"Iterations\":np.array(range(1, len(ilpd_result[\"objectives\"][1:]))), \"convergance_rate\": np.diff(ilpd_result[\"objectives\"][1:])})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:45:39.038802Z","iopub.execute_input":"2026-01-29T08:45:39.039453Z","iopub.status.idle":"2026-01-29T08:45:39.046263Z","shell.execute_reply.started":"2026-01-29T08:45:39.039427Z","shell.execute_reply":"2026-01-29T08:45:39.045462Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"def missing_latent_value(p):\n    # How to load\n    _, true_labels, df = load_uci_dataset(\"ilpd\")\n    print(f\"missing rate:{p}\")\n    X_missing = inject_missing_entries(X1, missing_ratio=p)\n\n    df = df.copy()\n    df[feature_cols] = X_missing\n    df=df[feature_cols]\n\n    X = df.values.astype(np.float32)\n\n    # Binary mask: 1 = observed, 0 = missing\n    mask = (~np.isnan(X)).astype(np.float32)\n\n    # Fill missing values with zero (mask-aware)\n    X_filled = np.nan_to_num(X, nan=0.0)\n    X_scaled = X_filled.copy()\n    X_tensor = torch.from_numpy(np.array(X_scaled))\n    scaler = TorchScaler().fit(X_tensor)\n    X_scaled = scaler.transform(X_tensor)\n    mask_tensor = torch.tensor(mask, dtype=torch.float32)\n    dataset = MaskedDataset(X_scaled, mask)\n    torch.manual_seed(42)\n    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n    vae = MaskedVAE(input_dim=10, hidden_dim=10, latent_dim=4)\n    vae = train_masked_vae(vae, dataloader, device,epochs=1000,lr=1e-3)\n    latent = extract_latent_features(vae, X_scaled,mask_tensor,device)\n    latent_np = latent.numpy()\n    \n    return df, latent_np","metadata":{"code_folding":[],"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T17:31:31.773250Z","iopub.execute_input":"2026-01-27T17:31:31.773568Z","iopub.status.idle":"2026-01-27T17:31:31.781383Z","shell.execute_reply.started":"2026-01-27T17:31:31.773542Z","shell.execute_reply":"2026-01-27T17:31:31.780254Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def storing_results(p):\n    missing_rate=p\n    data, U = missing_latent_value(missing_rate)\n    result = distributional_evidential_clustering(U,n_clusters=2,beta=2,gamma=0,max_iter=500,tol=1e-8,seed=42)\n    labels = result[\"labels\"]\n    data[\"pred\"]=labels\n    return compute_all_metrics(true_labels, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T17:31:31.810402Z","iopub.execute_input":"2026-01-27T17:31:31.810720Z","iopub.status.idle":"2026-01-27T17:31:31.825222Z","shell.execute_reply.started":"2026-01-27T17:31:31.810645Z","shell.execute_reply":"2026-01-27T17:31:31.824339Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"incomplete_rate=[0,0.02,0.04,0.06,0.08,0.1,0.15,0.2,0.25,0.3]\niris_results=pd.DataFrame()\nfor i in incomplete_rate:\n    data = {\"incomplete_rate\":i} | storing_results(i)\n    data=pd.DataFrame([data])\n    iris_results=pd.concat([iris_results, data], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T17:31:36.251243Z","iopub.execute_input":"2026-01-27T17:31:36.251627Z","iopub.status.idle":"2026-01-27T17:34:34.409320Z","shell.execute_reply.started":"2026-01-27T17:31:36.251593Z","shell.execute_reply":"2026-01-27T17:34:34.408501Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n/tmp/ipykernel_55/2674980365.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"missing rate:0\nEpoch 1000 | Loss: 0.5474 | Recon: 0.5474 | \nConverged at iteration 43\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n/tmp/ipykernel_55/2674980365.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"missing rate:0.02\nEpoch 1000 | Loss: 0.5112 | Recon: 0.5112 | \nConverged at iteration 56\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n/tmp/ipykernel_55/2674980365.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"missing rate:0.04\nEpoch 1000 | Loss: 0.6381 | Recon: 0.6381 | \nConverged at iteration 32\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n/tmp/ipykernel_55/2674980365.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"missing rate:0.06\nEpoch 1000 | Loss: 0.4371 | Recon: 0.4371 | \nConverged at iteration 106\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n/tmp/ipykernel_55/2674980365.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"missing rate:0.08\nEpoch 1000 | Loss: 0.5229 | Recon: 0.5229 | \nConverged at iteration 27\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n/tmp/ipykernel_55/2674980365.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"missing rate:0.1\nEpoch 1000 | Loss: 0.4112 | Recon: 0.4112 | \nConverged at iteration 26\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n/tmp/ipykernel_55/2674980365.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"missing rate:0.15\nEpoch 1000 | Loss: 0.4835 | Recon: 0.4835 | \nConverged at iteration 40\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n/tmp/ipykernel_55/2674980365.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"missing rate:0.2\nEpoch 1000 | Loss: 0.3440 | Recon: 0.3440 | \nConverged at iteration 85\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n/tmp/ipykernel_55/2674980365.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"missing rate:0.25\nEpoch 1000 | Loss: 0.3086 | Recon: 0.3086 | \nConverged at iteration 34\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:323: UserWarning: Multiple active versions of the dataset matching the name ilpd exist. Versions may be fundamentally different, returning version 1. Available versions:\n- version 1, status: active\n  url: https://www.openml.org/search?type=data&id=1480\n- version 2, status: active\n  url: https://www.openml.org/search?type=data&id=43104\n\n  warn(warning_msg)\n/tmp/ipykernel_55/2674980365.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"missing rate:0.3\nEpoch 1000 | Loss: 0.3087 | Recon: 0.3087 | \nConverged at iteration 58\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}