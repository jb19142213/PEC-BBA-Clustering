{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/dataiku/DATA_DESIGNER/code-envs/python/offus/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Masked Variational Autoencoder\n",
    "# =========================\n",
    "\n",
    "class MaskedVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Variational Autoencoder for incomplete data.\n",
    "    Encoder input: [x âŠ™ m , m]\n",
    "    Latent output: mu (used as latent feature u_i)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: input_dim * 2 because of concatenation with mask\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, mask):\n",
    "        x_masked = x * mask\n",
    "        enc_input = torch.cat([x_masked, mask], dim=1)\n",
    "        h = self.encoder(enc_input)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mu, logvar = self.encode(x, mask)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Masked VAE Loss\n",
    "# =========================\n",
    "\n",
    "def masked_vae_loss(\n",
    "    recon_x,\n",
    "    x,\n",
    "    mask,\n",
    "    mu,\n",
    "    logvar,\n",
    "    recon_weight=1.0,\n",
    "    kl_weight=0.0,\n",
    "    eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Masked reconstruction + KL divergence loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Masked reconstruction loss (MSE over observed entries only)\n",
    "    se = (recon_x - x) ** 2\n",
    "    masked_se = se * mask\n",
    "    recon_loss = masked_se.sum(dim=1) / (mask.sum(dim=1) + eps)\n",
    "    recon_loss = recon_loss.mean()\n",
    "\n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * torch.mean(\n",
    "        torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    )\n",
    "\n",
    "    total_loss = recon_weight * recon_loss + kl_weight * kl_loss\n",
    "    return total_loss, recon_loss.item(), kl_loss.item()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Training Function\n",
    "# =========================\n",
    "\n",
    "def train_masked_vae(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    epochs=50,\n",
    "    lr=1e-3\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        total_rec = 0.0\n",
    "        total_kl = 0.0\n",
    "\n",
    "        for x, mask in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            mask = mask.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x, mask)\n",
    "            loss, rec, kl = masked_vae_loss(recon, x, mask, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_rec += rec\n",
    "            total_kl += kl\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Loss: {total_loss:.4f} | \"\n",
    "            f\"Recon: {total_rec:.4f} | \"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Latent Feature Extraction\n",
    "# =========================\n",
    "\n",
    "def extract_latent_features(model, X, mask, device):\n",
    "    \"\"\"\n",
    "    Returns latent feature vectors u_i = mu_i\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = X.to(device).float()\n",
    "        mask = mask.to(device).float()\n",
    "        mu, _ = model.encode(X, mask)\n",
    "    return mu.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Load Iris Dataset\n",
    "# ============================================\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df[\"target\"] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Inject Missing Values (MCAR)\n",
    "# ============================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = df.shape[0]\n",
    "missing_ratio = 0.0\n",
    "n_missing_rows = int(missing_ratio * n_samples)\n",
    "\n",
    "missing_rows = np.random.choice(df.index, size=n_missing_rows, replace=False)\n",
    "\n",
    "for row in missing_rows:\n",
    "    n_cols_missing = np.random.randint(1, len(iris.feature_names))\n",
    "    cols_missing = np.random.choice(\n",
    "        iris.feature_names, size=n_cols_missing, replace=False\n",
    "    )\n",
    "    df.loc[row, cols_missing] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Create Feature Matrix and Mask\n",
    "# ============================================\n",
    "\n",
    "X = df[iris.feature_names].values.astype(np.float32)\n",
    "\n",
    "# Binary mask: 1 = observed, 0 = missing\n",
    "mask = (~np.isnan(X)).astype(np.float32)\n",
    "\n",
    "# Fill missing values with zero (mask-aware)\n",
    "X_filled = np.nan_to_num(X, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. Mask-aware Standardization\n",
    "# ============================================\n",
    "class TorchScaler:\n",
    "    def fit(self, X: torch.Tensor):\n",
    "        self.mean = X.mean(dim=0, keepdim=True)\n",
    "        self.std = X.std(dim=0, unbiased=False, keepdim=True)\n",
    "        self.std[self.std == 0] = 1.0\n",
    "        return self\n",
    "    def transform(self, X: torch.Tensor):\n",
    "        return (X - self.mean) / self.std\n",
    "    def inverse_transform(self, X: torch.Tensor):\n",
    "        return X * self.std + self.mean\n",
    "\n",
    "X_scaled = X_filled.copy()\n",
    "X_tensor = torch.from_numpy(np.array(X_scaled))\n",
    "scaler = TorchScaler().fit(X_tensor)\n",
    "X_scaled = scaler.transform(X_tensor)\n",
    "mask_tensor = torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3478032/1857059863.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 5. PyTorch Dataset and DataLoader\n",
    "# ============================================\n",
    "    \n",
    "class MaskedDataset:\n",
    "    def __init__(self, X, mask):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.mask[idx]\n",
    "\n",
    "\n",
    "dataset = MaskedDataset(X_scaled, mask)\n",
    "torch.manual_seed(42)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAssumes your DataLoader yields (x, mask)\\nx    : tensor of shape [batch_size, input_dim]\\nmask : tensor of shape [batch_size, input_dim], binary {0,1}\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Example Usage (Skeleton)\n",
    "# =========================\n",
    "\"\"\"\n",
    "Assumes your DataLoader yields (x, mask)\n",
    "x    : tensor of shape [batch_size, input_dim]\n",
    "mask : tensor of shape [batch_size, input_dim], binary {0,1}\n",
    "\"\"\"\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = MaskedVAE(input_dim=INPUT_DIM, latent_dim=LATENT_DIM)\n",
    "# model = train_masked_vae(model, dataloader, device)\n",
    "# U = extract_latent_features(model, X_full, mask_full, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 4.0612 | Recon: 4.0612 | \n",
      "Epoch 002 | Loss: 4.0324 | Recon: 4.0324 | \n",
      "Epoch 003 | Loss: 3.9371 | Recon: 3.9371 | \n",
      "Epoch 004 | Loss: 3.9353 | Recon: 3.9353 | \n",
      "Epoch 005 | Loss: 3.9517 | Recon: 3.9517 | \n",
      "Epoch 006 | Loss: 3.9083 | Recon: 3.9083 | \n",
      "Epoch 007 | Loss: 3.8933 | Recon: 3.8933 | \n",
      "Epoch 008 | Loss: 3.9175 | Recon: 3.9175 | \n",
      "Epoch 009 | Loss: 3.8738 | Recon: 3.8738 | \n",
      "Epoch 010 | Loss: 3.8316 | Recon: 3.8316 | \n",
      "Epoch 011 | Loss: 3.8079 | Recon: 3.8079 | \n",
      "Epoch 012 | Loss: 3.8673 | Recon: 3.8673 | \n",
      "Epoch 013 | Loss: 3.7869 | Recon: 3.7869 | \n",
      "Epoch 014 | Loss: 3.7441 | Recon: 3.7441 | \n",
      "Epoch 015 | Loss: 3.7828 | Recon: 3.7828 | \n",
      "Epoch 016 | Loss: 3.6956 | Recon: 3.6956 | \n",
      "Epoch 017 | Loss: 3.6474 | Recon: 3.6474 | \n",
      "Epoch 018 | Loss: 3.7462 | Recon: 3.7462 | \n",
      "Epoch 019 | Loss: 3.6699 | Recon: 3.6699 | \n",
      "Epoch 020 | Loss: 3.7009 | Recon: 3.7009 | \n",
      "Epoch 021 | Loss: 3.6154 | Recon: 3.6154 | \n",
      "Epoch 022 | Loss: 3.6207 | Recon: 3.6207 | \n",
      "Epoch 023 | Loss: 3.5329 | Recon: 3.5329 | \n",
      "Epoch 024 | Loss: 3.5193 | Recon: 3.5193 | \n",
      "Epoch 025 | Loss: 3.4662 | Recon: 3.4662 | \n",
      "Epoch 026 | Loss: 3.4410 | Recon: 3.4410 | \n",
      "Epoch 027 | Loss: 3.4471 | Recon: 3.4471 | \n",
      "Epoch 028 | Loss: 3.2633 | Recon: 3.2633 | \n",
      "Epoch 029 | Loss: 3.2780 | Recon: 3.2780 | \n",
      "Epoch 030 | Loss: 3.2936 | Recon: 3.2936 | \n",
      "Epoch 031 | Loss: 3.1567 | Recon: 3.1567 | \n",
      "Epoch 032 | Loss: 3.0146 | Recon: 3.0146 | \n",
      "Epoch 033 | Loss: 3.0518 | Recon: 3.0518 | \n",
      "Epoch 034 | Loss: 3.0522 | Recon: 3.0522 | \n",
      "Epoch 035 | Loss: 2.8071 | Recon: 2.8071 | \n",
      "Epoch 036 | Loss: 2.8239 | Recon: 2.8239 | \n",
      "Epoch 037 | Loss: 2.6988 | Recon: 2.6988 | \n",
      "Epoch 038 | Loss: 2.7900 | Recon: 2.7900 | \n",
      "Epoch 039 | Loss: 2.6869 | Recon: 2.6869 | \n",
      "Epoch 040 | Loss: 2.6207 | Recon: 2.6207 | \n",
      "Epoch 041 | Loss: 2.5645 | Recon: 2.5645 | \n",
      "Epoch 042 | Loss: 2.5549 | Recon: 2.5549 | \n",
      "Epoch 043 | Loss: 2.4420 | Recon: 2.4420 | \n",
      "Epoch 044 | Loss: 2.4739 | Recon: 2.4739 | \n",
      "Epoch 045 | Loss: 2.4099 | Recon: 2.4099 | \n",
      "Epoch 046 | Loss: 2.3823 | Recon: 2.3823 | \n",
      "Epoch 047 | Loss: 2.3814 | Recon: 2.3814 | \n",
      "Epoch 048 | Loss: 2.3973 | Recon: 2.3973 | \n",
      "Epoch 049 | Loss: 2.3277 | Recon: 2.3277 | \n",
      "Epoch 050 | Loss: 2.2425 | Recon: 2.2425 | \n",
      "Epoch 051 | Loss: 2.1690 | Recon: 2.1690 | \n",
      "Epoch 052 | Loss: 2.1657 | Recon: 2.1657 | \n",
      "Epoch 053 | Loss: 2.0810 | Recon: 2.0810 | \n",
      "Epoch 054 | Loss: 2.0693 | Recon: 2.0693 | \n",
      "Epoch 055 | Loss: 2.1021 | Recon: 2.1021 | \n",
      "Epoch 056 | Loss: 2.0400 | Recon: 2.0400 | \n",
      "Epoch 057 | Loss: 1.9206 | Recon: 1.9206 | \n",
      "Epoch 058 | Loss: 1.9554 | Recon: 1.9554 | \n",
      "Epoch 059 | Loss: 1.8923 | Recon: 1.8923 | \n",
      "Epoch 060 | Loss: 1.8493 | Recon: 1.8493 | \n",
      "Epoch 061 | Loss: 1.7851 | Recon: 1.7851 | \n",
      "Epoch 062 | Loss: 1.7932 | Recon: 1.7932 | \n",
      "Epoch 063 | Loss: 1.7474 | Recon: 1.7474 | \n",
      "Epoch 064 | Loss: 1.6882 | Recon: 1.6882 | \n",
      "Epoch 065 | Loss: 1.6891 | Recon: 1.6891 | \n",
      "Epoch 066 | Loss: 1.6869 | Recon: 1.6869 | \n",
      "Epoch 067 | Loss: 1.6372 | Recon: 1.6372 | \n",
      "Epoch 068 | Loss: 1.6857 | Recon: 1.6857 | \n",
      "Epoch 069 | Loss: 1.6210 | Recon: 1.6210 | \n",
      "Epoch 070 | Loss: 1.5856 | Recon: 1.5856 | \n",
      "Epoch 071 | Loss: 1.5721 | Recon: 1.5721 | \n",
      "Epoch 072 | Loss: 1.6165 | Recon: 1.6165 | \n",
      "Epoch 073 | Loss: 1.5287 | Recon: 1.5287 | \n",
      "Epoch 074 | Loss: 1.4971 | Recon: 1.4971 | \n",
      "Epoch 075 | Loss: 1.5573 | Recon: 1.5573 | \n",
      "Epoch 076 | Loss: 1.4976 | Recon: 1.4976 | \n",
      "Epoch 077 | Loss: 1.4991 | Recon: 1.4991 | \n",
      "Epoch 078 | Loss: 1.4793 | Recon: 1.4793 | \n",
      "Epoch 079 | Loss: 1.4864 | Recon: 1.4864 | \n",
      "Epoch 080 | Loss: 1.5011 | Recon: 1.5011 | \n",
      "Epoch 081 | Loss: 1.4370 | Recon: 1.4370 | \n",
      "Epoch 082 | Loss: 1.4551 | Recon: 1.4551 | \n",
      "Epoch 083 | Loss: 1.4198 | Recon: 1.4198 | \n",
      "Epoch 084 | Loss: 1.4049 | Recon: 1.4049 | \n",
      "Epoch 085 | Loss: 1.4177 | Recon: 1.4177 | \n",
      "Epoch 086 | Loss: 1.3901 | Recon: 1.3901 | \n",
      "Epoch 087 | Loss: 1.3929 | Recon: 1.3929 | \n",
      "Epoch 088 | Loss: 1.3643 | Recon: 1.3643 | \n",
      "Epoch 089 | Loss: 1.3500 | Recon: 1.3500 | \n",
      "Epoch 090 | Loss: 1.3102 | Recon: 1.3102 | \n",
      "Epoch 091 | Loss: 1.3340 | Recon: 1.3340 | \n",
      "Epoch 092 | Loss: 1.3006 | Recon: 1.3006 | \n",
      "Epoch 093 | Loss: 1.2513 | Recon: 1.2513 | \n",
      "Epoch 094 | Loss: 1.2433 | Recon: 1.2433 | \n",
      "Epoch 095 | Loss: 1.2472 | Recon: 1.2472 | \n",
      "Epoch 096 | Loss: 1.2099 | Recon: 1.2099 | \n",
      "Epoch 097 | Loss: 1.1677 | Recon: 1.1677 | \n",
      "Epoch 098 | Loss: 1.1087 | Recon: 1.1087 | \n",
      "Epoch 099 | Loss: 1.0987 | Recon: 1.0987 | \n",
      "Epoch 100 | Loss: 1.0954 | Recon: 1.0954 | \n",
      "Epoch 101 | Loss: 1.1399 | Recon: 1.1399 | \n",
      "Epoch 102 | Loss: 1.1421 | Recon: 1.1421 | \n",
      "Epoch 103 | Loss: 1.0341 | Recon: 1.0341 | \n",
      "Epoch 104 | Loss: 1.0303 | Recon: 1.0303 | \n",
      "Epoch 105 | Loss: 1.0809 | Recon: 1.0809 | \n",
      "Epoch 106 | Loss: 1.0463 | Recon: 1.0463 | \n",
      "Epoch 107 | Loss: 1.0221 | Recon: 1.0221 | \n",
      "Epoch 108 | Loss: 0.9648 | Recon: 0.9648 | \n",
      "Epoch 109 | Loss: 0.9713 | Recon: 0.9713 | \n",
      "Epoch 110 | Loss: 1.0254 | Recon: 1.0254 | \n",
      "Epoch 111 | Loss: 0.9438 | Recon: 0.9438 | \n",
      "Epoch 112 | Loss: 1.0002 | Recon: 1.0002 | \n",
      "Epoch 113 | Loss: 0.9275 | Recon: 0.9275 | \n",
      "Epoch 114 | Loss: 0.9931 | Recon: 0.9931 | \n",
      "Epoch 115 | Loss: 1.0032 | Recon: 1.0032 | \n",
      "Epoch 116 | Loss: 0.9190 | Recon: 0.9190 | \n",
      "Epoch 117 | Loss: 0.9099 | Recon: 0.9099 | \n",
      "Epoch 118 | Loss: 0.8826 | Recon: 0.8826 | \n",
      "Epoch 119 | Loss: 0.8460 | Recon: 0.8460 | \n",
      "Epoch 120 | Loss: 0.8121 | Recon: 0.8121 | \n",
      "Epoch 121 | Loss: 0.8041 | Recon: 0.8041 | \n",
      "Epoch 122 | Loss: 0.8026 | Recon: 0.8026 | \n",
      "Epoch 123 | Loss: 0.8068 | Recon: 0.8068 | \n",
      "Epoch 124 | Loss: 0.7885 | Recon: 0.7885 | \n",
      "Epoch 125 | Loss: 0.8058 | Recon: 0.8058 | \n",
      "Epoch 126 | Loss: 0.8428 | Recon: 0.8428 | \n",
      "Epoch 127 | Loss: 0.8020 | Recon: 0.8020 | \n",
      "Epoch 128 | Loss: 0.7634 | Recon: 0.7634 | \n",
      "Epoch 129 | Loss: 0.7658 | Recon: 0.7658 | \n",
      "Epoch 130 | Loss: 0.7606 | Recon: 0.7606 | \n",
      "Epoch 131 | Loss: 0.7309 | Recon: 0.7309 | \n",
      "Epoch 132 | Loss: 0.7389 | Recon: 0.7389 | \n",
      "Epoch 133 | Loss: 0.7313 | Recon: 0.7313 | \n",
      "Epoch 134 | Loss: 0.7168 | Recon: 0.7168 | \n",
      "Epoch 135 | Loss: 0.6919 | Recon: 0.6919 | \n",
      "Epoch 136 | Loss: 0.7268 | Recon: 0.7268 | \n",
      "Epoch 137 | Loss: 0.6869 | Recon: 0.6869 | \n",
      "Epoch 138 | Loss: 0.7516 | Recon: 0.7516 | \n",
      "Epoch 139 | Loss: 0.7209 | Recon: 0.7209 | \n",
      "Epoch 140 | Loss: 0.6949 | Recon: 0.6949 | \n",
      "Epoch 141 | Loss: 0.6780 | Recon: 0.6780 | \n",
      "Epoch 142 | Loss: 0.6926 | Recon: 0.6926 | \n",
      "Epoch 143 | Loss: 0.7021 | Recon: 0.7021 | \n",
      "Epoch 144 | Loss: 0.6376 | Recon: 0.6376 | \n",
      "Epoch 145 | Loss: 0.6749 | Recon: 0.6749 | \n",
      "Epoch 146 | Loss: 0.6249 | Recon: 0.6249 | \n",
      "Epoch 147 | Loss: 0.6401 | Recon: 0.6401 | \n",
      "Epoch 148 | Loss: 0.6252 | Recon: 0.6252 | \n",
      "Epoch 149 | Loss: 0.6286 | Recon: 0.6286 | \n",
      "Epoch 150 | Loss: 0.6035 | Recon: 0.6035 | \n",
      "Epoch 151 | Loss: 0.6494 | Recon: 0.6494 | \n",
      "Epoch 152 | Loss: 0.6260 | Recon: 0.6260 | \n",
      "Epoch 153 | Loss: 0.5933 | Recon: 0.5933 | \n",
      "Epoch 154 | Loss: 0.6144 | Recon: 0.6144 | \n",
      "Epoch 155 | Loss: 0.5756 | Recon: 0.5756 | \n",
      "Epoch 156 | Loss: 0.5885 | Recon: 0.5885 | \n",
      "Epoch 157 | Loss: 0.5891 | Recon: 0.5891 | \n",
      "Epoch 158 | Loss: 0.5710 | Recon: 0.5710 | \n",
      "Epoch 159 | Loss: 0.5728 | Recon: 0.5728 | \n",
      "Epoch 160 | Loss: 0.5711 | Recon: 0.5711 | \n",
      "Epoch 161 | Loss: 0.5807 | Recon: 0.5807 | \n",
      "Epoch 162 | Loss: 0.5612 | Recon: 0.5612 | \n",
      "Epoch 163 | Loss: 0.5642 | Recon: 0.5642 | \n",
      "Epoch 164 | Loss: 0.5552 | Recon: 0.5552 | \n",
      "Epoch 165 | Loss: 0.5733 | Recon: 0.5733 | \n",
      "Epoch 166 | Loss: 0.5861 | Recon: 0.5861 | \n",
      "Epoch 167 | Loss: 0.5611 | Recon: 0.5611 | \n",
      "Epoch 168 | Loss: 0.5555 | Recon: 0.5555 | \n",
      "Epoch 169 | Loss: 0.5498 | Recon: 0.5498 | \n",
      "Epoch 170 | Loss: 0.5391 | Recon: 0.5391 | \n",
      "Epoch 171 | Loss: 0.5313 | Recon: 0.5313 | \n",
      "Epoch 172 | Loss: 0.5428 | Recon: 0.5428 | \n",
      "Epoch 173 | Loss: 0.5285 | Recon: 0.5285 | \n",
      "Epoch 174 | Loss: 0.5185 | Recon: 0.5185 | \n",
      "Epoch 175 | Loss: 0.5204 | Recon: 0.5204 | \n",
      "Epoch 176 | Loss: 0.5102 | Recon: 0.5102 | \n",
      "Epoch 177 | Loss: 0.5245 | Recon: 0.5245 | \n",
      "Epoch 178 | Loss: 0.5195 | Recon: 0.5195 | \n",
      "Epoch 179 | Loss: 0.5178 | Recon: 0.5178 | \n",
      "Epoch 180 | Loss: 0.5119 | Recon: 0.5119 | \n",
      "Epoch 181 | Loss: 0.4962 | Recon: 0.4962 | \n",
      "Epoch 182 | Loss: 0.4943 | Recon: 0.4943 | \n",
      "Epoch 183 | Loss: 0.5292 | Recon: 0.5292 | \n",
      "Epoch 184 | Loss: 0.5169 | Recon: 0.5169 | \n",
      "Epoch 185 | Loss: 0.4830 | Recon: 0.4830 | \n",
      "Epoch 186 | Loss: 0.4834 | Recon: 0.4834 | \n",
      "Epoch 187 | Loss: 0.4896 | Recon: 0.4896 | \n",
      "Epoch 188 | Loss: 0.4837 | Recon: 0.4837 | \n",
      "Epoch 189 | Loss: 0.4997 | Recon: 0.4997 | \n",
      "Epoch 190 | Loss: 0.4813 | Recon: 0.4813 | \n",
      "Epoch 191 | Loss: 0.4939 | Recon: 0.4939 | \n",
      "Epoch 192 | Loss: 0.4731 | Recon: 0.4731 | \n",
      "Epoch 193 | Loss: 0.4666 | Recon: 0.4666 | \n",
      "Epoch 194 | Loss: 0.4723 | Recon: 0.4723 | \n",
      "Epoch 195 | Loss: 0.4518 | Recon: 0.4518 | \n",
      "Epoch 196 | Loss: 0.4709 | Recon: 0.4709 | \n",
      "Epoch 197 | Loss: 0.4548 | Recon: 0.4548 | \n",
      "Epoch 198 | Loss: 0.4696 | Recon: 0.4696 | \n",
      "Epoch 199 | Loss: 0.4635 | Recon: 0.4635 | \n",
      "Epoch 200 | Loss: 0.4669 | Recon: 0.4669 | \n",
      "Epoch 201 | Loss: 0.4698 | Recon: 0.4698 | \n",
      "Epoch 202 | Loss: 0.4605 | Recon: 0.4605 | \n",
      "Epoch 203 | Loss: 0.4584 | Recon: 0.4584 | \n",
      "Epoch 204 | Loss: 0.4664 | Recon: 0.4664 | \n",
      "Epoch 205 | Loss: 0.4687 | Recon: 0.4687 | \n",
      "Epoch 206 | Loss: 0.4725 | Recon: 0.4725 | \n",
      "Epoch 207 | Loss: 0.4430 | Recon: 0.4430 | \n",
      "Epoch 208 | Loss: 0.4561 | Recon: 0.4561 | \n",
      "Epoch 209 | Loss: 0.4514 | Recon: 0.4514 | \n",
      "Epoch 210 | Loss: 0.4403 | Recon: 0.4403 | \n",
      "Epoch 211 | Loss: 0.4451 | Recon: 0.4451 | \n",
      "Epoch 212 | Loss: 0.4373 | Recon: 0.4373 | \n",
      "Epoch 213 | Loss: 0.4430 | Recon: 0.4430 | \n",
      "Epoch 214 | Loss: 0.4401 | Recon: 0.4401 | \n",
      "Epoch 215 | Loss: 0.4427 | Recon: 0.4427 | \n",
      "Epoch 216 | Loss: 0.4331 | Recon: 0.4331 | \n",
      "Epoch 217 | Loss: 0.4285 | Recon: 0.4285 | \n",
      "Epoch 218 | Loss: 0.4323 | Recon: 0.4323 | \n",
      "Epoch 219 | Loss: 0.4278 | Recon: 0.4278 | \n",
      "Epoch 220 | Loss: 0.4208 | Recon: 0.4208 | \n",
      "Epoch 221 | Loss: 0.4295 | Recon: 0.4295 | \n",
      "Epoch 222 | Loss: 0.4174 | Recon: 0.4174 | \n",
      "Epoch 223 | Loss: 0.4125 | Recon: 0.4125 | \n",
      "Epoch 224 | Loss: 0.4428 | Recon: 0.4428 | \n",
      "Epoch 225 | Loss: 0.4354 | Recon: 0.4354 | \n",
      "Epoch 226 | Loss: 0.4116 | Recon: 0.4116 | \n",
      "Epoch 227 | Loss: 0.4180 | Recon: 0.4180 | \n",
      "Epoch 228 | Loss: 0.4141 | Recon: 0.4141 | \n",
      "Epoch 229 | Loss: 0.4369 | Recon: 0.4369 | \n",
      "Epoch 230 | Loss: 0.4162 | Recon: 0.4162 | \n",
      "Epoch 231 | Loss: 0.4322 | Recon: 0.4322 | \n",
      "Epoch 232 | Loss: 0.4169 | Recon: 0.4169 | \n",
      "Epoch 233 | Loss: 0.4064 | Recon: 0.4064 | \n",
      "Epoch 234 | Loss: 0.4086 | Recon: 0.4086 | \n",
      "Epoch 235 | Loss: 0.4109 | Recon: 0.4109 | \n",
      "Epoch 236 | Loss: 0.4247 | Recon: 0.4247 | \n",
      "Epoch 237 | Loss: 0.4064 | Recon: 0.4064 | \n",
      "Epoch 238 | Loss: 0.4022 | Recon: 0.4022 | \n",
      "Epoch 239 | Loss: 0.4309 | Recon: 0.4309 | \n",
      "Epoch 240 | Loss: 0.4166 | Recon: 0.4166 | \n",
      "Epoch 241 | Loss: 0.4102 | Recon: 0.4102 | \n",
      "Epoch 242 | Loss: 0.3988 | Recon: 0.3988 | \n",
      "Epoch 243 | Loss: 0.4149 | Recon: 0.4149 | \n",
      "Epoch 244 | Loss: 0.3925 | Recon: 0.3925 | \n",
      "Epoch 245 | Loss: 0.3905 | Recon: 0.3905 | \n",
      "Epoch 246 | Loss: 0.4077 | Recon: 0.4077 | \n",
      "Epoch 247 | Loss: 0.4123 | Recon: 0.4123 | \n",
      "Epoch 248 | Loss: 0.3943 | Recon: 0.3943 | \n",
      "Epoch 249 | Loss: 0.4059 | Recon: 0.4059 | \n",
      "Epoch 250 | Loss: 0.4027 | Recon: 0.4027 | \n",
      "Epoch 251 | Loss: 0.3936 | Recon: 0.3936 | \n",
      "Epoch 252 | Loss: 0.3871 | Recon: 0.3871 | \n",
      "Epoch 253 | Loss: 0.3886 | Recon: 0.3886 | \n",
      "Epoch 254 | Loss: 0.3884 | Recon: 0.3884 | \n",
      "Epoch 255 | Loss: 0.3848 | Recon: 0.3848 | \n",
      "Epoch 256 | Loss: 0.3839 | Recon: 0.3839 | \n",
      "Epoch 257 | Loss: 0.4003 | Recon: 0.4003 | \n",
      "Epoch 258 | Loss: 0.3998 | Recon: 0.3998 | \n",
      "Epoch 259 | Loss: 0.3820 | Recon: 0.3820 | \n",
      "Epoch 260 | Loss: 0.4045 | Recon: 0.4045 | \n",
      "Epoch 261 | Loss: 0.3916 | Recon: 0.3916 | \n",
      "Epoch 262 | Loss: 0.3958 | Recon: 0.3958 | \n",
      "Epoch 263 | Loss: 0.3829 | Recon: 0.3829 | \n",
      "Epoch 264 | Loss: 0.3902 | Recon: 0.3902 | \n",
      "Epoch 265 | Loss: 0.3763 | Recon: 0.3763 | \n",
      "Epoch 266 | Loss: 0.3825 | Recon: 0.3825 | \n",
      "Epoch 267 | Loss: 0.3931 | Recon: 0.3931 | \n",
      "Epoch 268 | Loss: 0.3755 | Recon: 0.3755 | \n",
      "Epoch 269 | Loss: 0.3863 | Recon: 0.3863 | \n",
      "Epoch 270 | Loss: 0.3860 | Recon: 0.3860 | \n",
      "Epoch 271 | Loss: 0.3821 | Recon: 0.3821 | \n",
      "Epoch 272 | Loss: 0.4111 | Recon: 0.4111 | \n",
      "Epoch 273 | Loss: 0.3731 | Recon: 0.3731 | \n",
      "Epoch 274 | Loss: 0.3983 | Recon: 0.3983 | \n",
      "Epoch 275 | Loss: 0.3930 | Recon: 0.3930 | \n",
      "Epoch 276 | Loss: 0.3764 | Recon: 0.3764 | \n",
      "Epoch 277 | Loss: 0.3759 | Recon: 0.3759 | \n",
      "Epoch 278 | Loss: 0.3751 | Recon: 0.3751 | \n",
      "Epoch 279 | Loss: 0.3693 | Recon: 0.3693 | \n",
      "Epoch 280 | Loss: 0.3841 | Recon: 0.3841 | \n",
      "Epoch 281 | Loss: 0.3713 | Recon: 0.3713 | \n",
      "Epoch 282 | Loss: 0.3714 | Recon: 0.3714 | \n",
      "Epoch 283 | Loss: 0.3976 | Recon: 0.3976 | \n",
      "Epoch 284 | Loss: 0.3776 | Recon: 0.3776 | \n",
      "Epoch 285 | Loss: 0.3827 | Recon: 0.3827 | \n",
      "Epoch 286 | Loss: 0.3657 | Recon: 0.3657 | \n",
      "Epoch 287 | Loss: 0.3739 | Recon: 0.3739 | \n",
      "Epoch 288 | Loss: 0.3831 | Recon: 0.3831 | \n",
      "Epoch 289 | Loss: 0.3807 | Recon: 0.3807 | \n",
      "Epoch 290 | Loss: 0.3663 | Recon: 0.3663 | \n",
      "Epoch 291 | Loss: 0.3718 | Recon: 0.3718 | \n",
      "Epoch 292 | Loss: 0.3798 | Recon: 0.3798 | \n",
      "Epoch 293 | Loss: 0.3625 | Recon: 0.3625 | \n",
      "Epoch 294 | Loss: 0.3803 | Recon: 0.3803 | \n",
      "Epoch 295 | Loss: 0.3742 | Recon: 0.3742 | \n",
      "Epoch 296 | Loss: 0.3807 | Recon: 0.3807 | \n",
      "Epoch 297 | Loss: 0.3633 | Recon: 0.3633 | \n",
      "Epoch 298 | Loss: 0.3894 | Recon: 0.3894 | \n",
      "Epoch 299 | Loss: 0.3662 | Recon: 0.3662 | \n",
      "Epoch 300 | Loss: 0.3675 | Recon: 0.3675 | \n",
      "Epoch 301 | Loss: 0.3659 | Recon: 0.3659 | \n",
      "Epoch 302 | Loss: 0.3708 | Recon: 0.3708 | \n",
      "Epoch 303 | Loss: 0.3653 | Recon: 0.3653 | \n",
      "Epoch 304 | Loss: 0.3676 | Recon: 0.3676 | \n",
      "Epoch 305 | Loss: 0.3802 | Recon: 0.3802 | \n",
      "Epoch 306 | Loss: 0.3615 | Recon: 0.3615 | \n",
      "Epoch 307 | Loss: 0.3665 | Recon: 0.3665 | \n",
      "Epoch 308 | Loss: 0.3610 | Recon: 0.3610 | \n",
      "Epoch 309 | Loss: 0.3714 | Recon: 0.3714 | \n",
      "Epoch 310 | Loss: 0.3673 | Recon: 0.3673 | \n",
      "Epoch 311 | Loss: 0.3615 | Recon: 0.3615 | \n",
      "Epoch 312 | Loss: 0.3690 | Recon: 0.3690 | \n",
      "Epoch 313 | Loss: 0.3660 | Recon: 0.3660 | \n",
      "Epoch 314 | Loss: 0.3605 | Recon: 0.3605 | \n",
      "Epoch 315 | Loss: 0.3664 | Recon: 0.3664 | \n",
      "Epoch 316 | Loss: 0.3606 | Recon: 0.3606 | \n",
      "Epoch 317 | Loss: 0.3572 | Recon: 0.3572 | \n",
      "Epoch 318 | Loss: 0.3671 | Recon: 0.3671 | \n",
      "Epoch 319 | Loss: 0.3559 | Recon: 0.3559 | \n",
      "Epoch 320 | Loss: 0.3747 | Recon: 0.3747 | \n",
      "Epoch 321 | Loss: 0.3619 | Recon: 0.3619 | \n",
      "Epoch 322 | Loss: 0.3569 | Recon: 0.3569 | \n",
      "Epoch 323 | Loss: 0.3672 | Recon: 0.3672 | \n",
      "Epoch 324 | Loss: 0.3582 | Recon: 0.3582 | \n",
      "Epoch 325 | Loss: 0.3566 | Recon: 0.3566 | \n",
      "Epoch 326 | Loss: 0.3499 | Recon: 0.3499 | \n",
      "Epoch 327 | Loss: 0.3609 | Recon: 0.3609 | \n",
      "Epoch 328 | Loss: 0.3528 | Recon: 0.3528 | \n",
      "Epoch 329 | Loss: 0.3569 | Recon: 0.3569 | \n",
      "Epoch 330 | Loss: 0.3708 | Recon: 0.3708 | \n",
      "Epoch 331 | Loss: 0.3537 | Recon: 0.3537 | \n",
      "Epoch 332 | Loss: 0.3603 | Recon: 0.3603 | \n",
      "Epoch 333 | Loss: 0.3497 | Recon: 0.3497 | \n",
      "Epoch 334 | Loss: 0.3590 | Recon: 0.3590 | \n",
      "Epoch 335 | Loss: 0.3514 | Recon: 0.3514 | \n",
      "Epoch 336 | Loss: 0.3495 | Recon: 0.3495 | \n",
      "Epoch 337 | Loss: 0.3520 | Recon: 0.3520 | \n",
      "Epoch 338 | Loss: 0.3588 | Recon: 0.3588 | \n",
      "Epoch 339 | Loss: 0.3625 | Recon: 0.3625 | \n",
      "Epoch 340 | Loss: 0.3528 | Recon: 0.3528 | \n",
      "Epoch 341 | Loss: 0.3676 | Recon: 0.3676 | \n",
      "Epoch 342 | Loss: 0.3586 | Recon: 0.3586 | \n",
      "Epoch 343 | Loss: 0.3441 | Recon: 0.3441 | \n",
      "Epoch 344 | Loss: 0.3480 | Recon: 0.3480 | \n",
      "Epoch 345 | Loss: 0.3542 | Recon: 0.3542 | \n",
      "Epoch 346 | Loss: 0.3466 | Recon: 0.3466 | \n",
      "Epoch 347 | Loss: 0.3546 | Recon: 0.3546 | \n",
      "Epoch 348 | Loss: 0.3483 | Recon: 0.3483 | \n",
      "Epoch 349 | Loss: 0.3369 | Recon: 0.3369 | \n",
      "Epoch 350 | Loss: 0.3516 | Recon: 0.3516 | \n",
      "Epoch 351 | Loss: 0.3503 | Recon: 0.3503 | \n",
      "Epoch 352 | Loss: 0.3471 | Recon: 0.3471 | \n",
      "Epoch 353 | Loss: 0.3460 | Recon: 0.3460 | \n",
      "Epoch 354 | Loss: 0.3462 | Recon: 0.3462 | \n",
      "Epoch 355 | Loss: 0.3526 | Recon: 0.3526 | \n",
      "Epoch 356 | Loss: 0.3499 | Recon: 0.3499 | \n",
      "Epoch 357 | Loss: 0.3358 | Recon: 0.3358 | \n",
      "Epoch 358 | Loss: 0.3506 | Recon: 0.3506 | \n",
      "Epoch 359 | Loss: 0.3438 | Recon: 0.3438 | \n",
      "Epoch 360 | Loss: 0.3414 | Recon: 0.3414 | \n",
      "Epoch 361 | Loss: 0.3461 | Recon: 0.3461 | \n",
      "Epoch 362 | Loss: 0.3540 | Recon: 0.3540 | \n",
      "Epoch 363 | Loss: 0.3470 | Recon: 0.3470 | \n",
      "Epoch 364 | Loss: 0.3420 | Recon: 0.3420 | \n",
      "Epoch 365 | Loss: 0.3579 | Recon: 0.3579 | \n",
      "Epoch 366 | Loss: 0.3375 | Recon: 0.3375 | \n",
      "Epoch 367 | Loss: 0.3417 | Recon: 0.3417 | \n",
      "Epoch 368 | Loss: 0.3461 | Recon: 0.3461 | \n",
      "Epoch 369 | Loss: 0.3459 | Recon: 0.3459 | \n",
      "Epoch 370 | Loss: 0.3368 | Recon: 0.3368 | \n",
      "Epoch 371 | Loss: 0.3321 | Recon: 0.3321 | \n",
      "Epoch 372 | Loss: 0.3446 | Recon: 0.3446 | \n",
      "Epoch 373 | Loss: 0.3391 | Recon: 0.3391 | \n",
      "Epoch 374 | Loss: 0.3474 | Recon: 0.3474 | \n",
      "Epoch 375 | Loss: 0.3365 | Recon: 0.3365 | \n",
      "Epoch 376 | Loss: 0.3365 | Recon: 0.3365 | \n",
      "Epoch 377 | Loss: 0.3417 | Recon: 0.3417 | \n",
      "Epoch 378 | Loss: 0.3324 | Recon: 0.3324 | \n",
      "Epoch 379 | Loss: 0.3386 | Recon: 0.3386 | \n",
      "Epoch 380 | Loss: 0.3454 | Recon: 0.3454 | \n",
      "Epoch 381 | Loss: 0.3522 | Recon: 0.3522 | \n",
      "Epoch 382 | Loss: 0.3313 | Recon: 0.3313 | \n",
      "Epoch 383 | Loss: 0.3392 | Recon: 0.3392 | \n",
      "Epoch 384 | Loss: 0.3308 | Recon: 0.3308 | \n",
      "Epoch 385 | Loss: 0.3418 | Recon: 0.3418 | \n",
      "Epoch 386 | Loss: 0.3467 | Recon: 0.3467 | \n",
      "Epoch 387 | Loss: 0.3376 | Recon: 0.3376 | \n",
      "Epoch 388 | Loss: 0.3387 | Recon: 0.3387 | \n",
      "Epoch 389 | Loss: 0.3425 | Recon: 0.3425 | \n",
      "Epoch 390 | Loss: 0.3435 | Recon: 0.3435 | \n",
      "Epoch 391 | Loss: 0.3320 | Recon: 0.3320 | \n",
      "Epoch 392 | Loss: 0.3455 | Recon: 0.3455 | \n",
      "Epoch 393 | Loss: 0.3312 | Recon: 0.3312 | \n",
      "Epoch 394 | Loss: 0.3404 | Recon: 0.3404 | \n",
      "Epoch 395 | Loss: 0.3297 | Recon: 0.3297 | \n",
      "Epoch 396 | Loss: 0.3367 | Recon: 0.3367 | \n",
      "Epoch 397 | Loss: 0.3361 | Recon: 0.3361 | \n",
      "Epoch 398 | Loss: 0.3310 | Recon: 0.3310 | \n",
      "Epoch 399 | Loss: 0.3375 | Recon: 0.3375 | \n",
      "Epoch 400 | Loss: 0.3434 | Recon: 0.3434 | \n",
      "Epoch 401 | Loss: 0.3282 | Recon: 0.3282 | \n",
      "Epoch 402 | Loss: 0.3446 | Recon: 0.3446 | \n",
      "Epoch 403 | Loss: 0.3325 | Recon: 0.3325 | \n",
      "Epoch 404 | Loss: 0.3341 | Recon: 0.3341 | \n",
      "Epoch 405 | Loss: 0.3322 | Recon: 0.3322 | \n",
      "Epoch 406 | Loss: 0.3349 | Recon: 0.3349 | \n",
      "Epoch 407 | Loss: 0.3304 | Recon: 0.3304 | \n",
      "Epoch 408 | Loss: 0.3387 | Recon: 0.3387 | \n",
      "Epoch 409 | Loss: 0.3405 | Recon: 0.3405 | \n",
      "Epoch 410 | Loss: 0.3257 | Recon: 0.3257 | \n",
      "Epoch 411 | Loss: 0.3288 | Recon: 0.3288 | \n",
      "Epoch 412 | Loss: 0.3365 | Recon: 0.3365 | \n",
      "Epoch 413 | Loss: 0.3360 | Recon: 0.3360 | \n",
      "Epoch 414 | Loss: 0.3320 | Recon: 0.3320 | \n",
      "Epoch 415 | Loss: 0.3329 | Recon: 0.3329 | \n",
      "Epoch 416 | Loss: 0.3286 | Recon: 0.3286 | \n",
      "Epoch 417 | Loss: 0.3305 | Recon: 0.3305 | \n",
      "Epoch 418 | Loss: 0.3296 | Recon: 0.3296 | \n",
      "Epoch 419 | Loss: 0.3303 | Recon: 0.3303 | \n",
      "Epoch 420 | Loss: 0.3315 | Recon: 0.3315 | \n",
      "Epoch 421 | Loss: 0.3236 | Recon: 0.3236 | \n",
      "Epoch 422 | Loss: 0.3233 | Recon: 0.3233 | \n",
      "Epoch 423 | Loss: 0.3295 | Recon: 0.3295 | \n",
      "Epoch 424 | Loss: 0.3321 | Recon: 0.3321 | \n",
      "Epoch 425 | Loss: 0.3283 | Recon: 0.3283 | \n",
      "Epoch 426 | Loss: 0.3320 | Recon: 0.3320 | \n",
      "Epoch 427 | Loss: 0.3293 | Recon: 0.3293 | \n",
      "Epoch 428 | Loss: 0.3284 | Recon: 0.3284 | \n",
      "Epoch 429 | Loss: 0.3220 | Recon: 0.3220 | \n",
      "Epoch 430 | Loss: 0.3249 | Recon: 0.3249 | \n",
      "Epoch 431 | Loss: 0.3285 | Recon: 0.3285 | \n",
      "Epoch 432 | Loss: 0.3221 | Recon: 0.3221 | \n",
      "Epoch 433 | Loss: 0.3352 | Recon: 0.3352 | \n",
      "Epoch 434 | Loss: 0.3311 | Recon: 0.3311 | \n",
      "Epoch 435 | Loss: 0.3197 | Recon: 0.3197 | \n",
      "Epoch 436 | Loss: 0.3285 | Recon: 0.3285 | \n",
      "Epoch 437 | Loss: 0.3313 | Recon: 0.3313 | \n",
      "Epoch 438 | Loss: 0.3242 | Recon: 0.3242 | \n",
      "Epoch 439 | Loss: 0.3240 | Recon: 0.3240 | \n",
      "Epoch 440 | Loss: 0.3273 | Recon: 0.3273 | \n",
      "Epoch 441 | Loss: 0.3269 | Recon: 0.3269 | \n",
      "Epoch 442 | Loss: 0.3270 | Recon: 0.3270 | \n",
      "Epoch 443 | Loss: 0.3262 | Recon: 0.3262 | \n",
      "Epoch 444 | Loss: 0.3422 | Recon: 0.3422 | \n",
      "Epoch 445 | Loss: 0.3242 | Recon: 0.3242 | \n",
      "Epoch 446 | Loss: 0.3335 | Recon: 0.3335 | \n",
      "Epoch 447 | Loss: 0.3192 | Recon: 0.3192 | \n",
      "Epoch 448 | Loss: 0.3248 | Recon: 0.3248 | \n",
      "Epoch 449 | Loss: 0.3350 | Recon: 0.3350 | \n",
      "Epoch 450 | Loss: 0.3260 | Recon: 0.3260 | \n",
      "Epoch 451 | Loss: 0.3244 | Recon: 0.3244 | \n",
      "Epoch 452 | Loss: 0.3302 | Recon: 0.3302 | \n",
      "Epoch 453 | Loss: 0.3274 | Recon: 0.3274 | \n",
      "Epoch 454 | Loss: 0.3211 | Recon: 0.3211 | \n",
      "Epoch 455 | Loss: 0.3230 | Recon: 0.3230 | \n",
      "Epoch 456 | Loss: 0.3180 | Recon: 0.3180 | \n",
      "Epoch 457 | Loss: 0.3196 | Recon: 0.3196 | \n",
      "Epoch 458 | Loss: 0.3291 | Recon: 0.3291 | \n",
      "Epoch 459 | Loss: 0.3156 | Recon: 0.3156 | \n",
      "Epoch 460 | Loss: 0.3232 | Recon: 0.3232 | \n",
      "Epoch 461 | Loss: 0.3212 | Recon: 0.3212 | \n",
      "Epoch 462 | Loss: 0.3157 | Recon: 0.3157 | \n",
      "Epoch 463 | Loss: 0.3206 | Recon: 0.3206 | \n",
      "Epoch 464 | Loss: 0.3225 | Recon: 0.3225 | \n",
      "Epoch 465 | Loss: 0.3218 | Recon: 0.3218 | \n",
      "Epoch 466 | Loss: 0.3342 | Recon: 0.3342 | \n",
      "Epoch 467 | Loss: 0.3232 | Recon: 0.3232 | \n",
      "Epoch 468 | Loss: 0.3208 | Recon: 0.3208 | \n",
      "Epoch 469 | Loss: 0.3137 | Recon: 0.3137 | \n",
      "Epoch 470 | Loss: 0.3272 | Recon: 0.3272 | \n",
      "Epoch 471 | Loss: 0.3225 | Recon: 0.3225 | \n",
      "Epoch 472 | Loss: 0.3164 | Recon: 0.3164 | \n",
      "Epoch 473 | Loss: 0.3178 | Recon: 0.3178 | \n",
      "Epoch 474 | Loss: 0.3143 | Recon: 0.3143 | \n",
      "Epoch 475 | Loss: 0.3211 | Recon: 0.3211 | \n",
      "Epoch 476 | Loss: 0.3277 | Recon: 0.3277 | \n",
      "Epoch 477 | Loss: 0.3189 | Recon: 0.3189 | \n",
      "Epoch 478 | Loss: 0.3278 | Recon: 0.3278 | \n",
      "Epoch 479 | Loss: 0.3214 | Recon: 0.3214 | \n",
      "Epoch 480 | Loss: 0.3145 | Recon: 0.3145 | \n",
      "Epoch 481 | Loss: 0.3243 | Recon: 0.3243 | \n",
      "Epoch 482 | Loss: 0.3201 | Recon: 0.3201 | \n",
      "Epoch 483 | Loss: 0.3214 | Recon: 0.3214 | \n",
      "Epoch 484 | Loss: 0.3186 | Recon: 0.3186 | \n",
      "Epoch 485 | Loss: 0.3171 | Recon: 0.3171 | \n",
      "Epoch 486 | Loss: 0.3208 | Recon: 0.3208 | \n",
      "Epoch 487 | Loss: 0.3280 | Recon: 0.3280 | \n",
      "Epoch 488 | Loss: 0.3132 | Recon: 0.3132 | \n",
      "Epoch 489 | Loss: 0.3203 | Recon: 0.3203 | \n",
      "Epoch 490 | Loss: 0.3302 | Recon: 0.3302 | \n",
      "Epoch 491 | Loss: 0.3156 | Recon: 0.3156 | \n",
      "Epoch 492 | Loss: 0.3198 | Recon: 0.3198 | \n",
      "Epoch 493 | Loss: 0.3257 | Recon: 0.3257 | \n",
      "Epoch 494 | Loss: 0.3078 | Recon: 0.3078 | \n",
      "Epoch 495 | Loss: 0.3239 | Recon: 0.3239 | \n",
      "Epoch 496 | Loss: 0.3195 | Recon: 0.3195 | \n",
      "Epoch 497 | Loss: 0.3112 | Recon: 0.3112 | \n",
      "Epoch 498 | Loss: 0.3115 | Recon: 0.3115 | \n",
      "Epoch 499 | Loss: 0.3157 | Recon: 0.3157 | \n",
      "Epoch 500 | Loss: 0.3133 | Recon: 0.3133 | \n",
      "Epoch 501 | Loss: 0.3174 | Recon: 0.3174 | \n",
      "Epoch 502 | Loss: 0.3193 | Recon: 0.3193 | \n",
      "Epoch 503 | Loss: 0.3160 | Recon: 0.3160 | \n",
      "Epoch 504 | Loss: 0.3189 | Recon: 0.3189 | \n",
      "Epoch 505 | Loss: 0.3111 | Recon: 0.3111 | \n",
      "Epoch 506 | Loss: 0.3138 | Recon: 0.3138 | \n",
      "Epoch 507 | Loss: 0.3143 | Recon: 0.3143 | \n",
      "Epoch 508 | Loss: 0.3123 | Recon: 0.3123 | \n",
      "Epoch 509 | Loss: 0.3160 | Recon: 0.3160 | \n",
      "Epoch 510 | Loss: 0.3061 | Recon: 0.3061 | \n",
      "Epoch 511 | Loss: 0.3212 | Recon: 0.3212 | \n",
      "Epoch 512 | Loss: 0.3136 | Recon: 0.3136 | \n",
      "Epoch 513 | Loss: 0.3083 | Recon: 0.3083 | \n",
      "Epoch 514 | Loss: 0.3097 | Recon: 0.3097 | \n",
      "Epoch 515 | Loss: 0.3083 | Recon: 0.3083 | \n",
      "Epoch 516 | Loss: 0.3217 | Recon: 0.3217 | \n",
      "Epoch 517 | Loss: 0.3096 | Recon: 0.3096 | \n",
      "Epoch 518 | Loss: 0.3153 | Recon: 0.3153 | \n",
      "Epoch 519 | Loss: 0.3110 | Recon: 0.3110 | \n",
      "Epoch 520 | Loss: 0.3231 | Recon: 0.3231 | \n",
      "Epoch 521 | Loss: 0.3137 | Recon: 0.3137 | \n",
      "Epoch 522 | Loss: 0.3085 | Recon: 0.3085 | \n",
      "Epoch 523 | Loss: 0.3046 | Recon: 0.3046 | \n",
      "Epoch 524 | Loss: 0.3126 | Recon: 0.3126 | \n",
      "Epoch 525 | Loss: 0.3147 | Recon: 0.3147 | \n",
      "Epoch 526 | Loss: 0.3115 | Recon: 0.3115 | \n",
      "Epoch 527 | Loss: 0.3236 | Recon: 0.3236 | \n",
      "Epoch 528 | Loss: 0.3166 | Recon: 0.3166 | \n",
      "Epoch 529 | Loss: 0.3108 | Recon: 0.3108 | \n",
      "Epoch 530 | Loss: 0.3138 | Recon: 0.3138 | \n",
      "Epoch 531 | Loss: 0.3188 | Recon: 0.3188 | \n",
      "Epoch 532 | Loss: 0.3343 | Recon: 0.3343 | \n",
      "Epoch 533 | Loss: 0.3142 | Recon: 0.3142 | \n",
      "Epoch 534 | Loss: 0.3116 | Recon: 0.3116 | \n",
      "Epoch 535 | Loss: 0.3109 | Recon: 0.3109 | \n",
      "Epoch 536 | Loss: 0.3075 | Recon: 0.3075 | \n",
      "Epoch 537 | Loss: 0.3067 | Recon: 0.3067 | \n",
      "Epoch 538 | Loss: 0.3049 | Recon: 0.3049 | \n",
      "Epoch 539 | Loss: 0.3108 | Recon: 0.3108 | \n",
      "Epoch 540 | Loss: 0.3117 | Recon: 0.3117 | \n",
      "Epoch 541 | Loss: 0.3172 | Recon: 0.3172 | \n",
      "Epoch 542 | Loss: 0.3115 | Recon: 0.3115 | \n",
      "Epoch 543 | Loss: 0.3087 | Recon: 0.3087 | \n",
      "Epoch 544 | Loss: 0.3113 | Recon: 0.3113 | \n",
      "Epoch 545 | Loss: 0.3179 | Recon: 0.3179 | \n",
      "Epoch 546 | Loss: 0.3052 | Recon: 0.3052 | \n",
      "Epoch 547 | Loss: 0.3065 | Recon: 0.3065 | \n",
      "Epoch 548 | Loss: 0.3094 | Recon: 0.3094 | \n",
      "Epoch 549 | Loss: 0.3078 | Recon: 0.3078 | \n",
      "Epoch 550 | Loss: 0.3160 | Recon: 0.3160 | \n",
      "Epoch 551 | Loss: 0.3102 | Recon: 0.3102 | \n",
      "Epoch 552 | Loss: 0.3142 | Recon: 0.3142 | \n",
      "Epoch 553 | Loss: 0.3093 | Recon: 0.3093 | \n",
      "Epoch 554 | Loss: 0.3086 | Recon: 0.3086 | \n",
      "Epoch 555 | Loss: 0.3154 | Recon: 0.3154 | \n",
      "Epoch 556 | Loss: 0.3066 | Recon: 0.3066 | \n",
      "Epoch 557 | Loss: 0.3141 | Recon: 0.3141 | \n",
      "Epoch 558 | Loss: 0.3061 | Recon: 0.3061 | \n",
      "Epoch 559 | Loss: 0.3222 | Recon: 0.3222 | \n",
      "Epoch 560 | Loss: 0.3011 | Recon: 0.3011 | \n",
      "Epoch 561 | Loss: 0.3062 | Recon: 0.3062 | \n",
      "Epoch 562 | Loss: 0.3129 | Recon: 0.3129 | \n",
      "Epoch 563 | Loss: 0.3013 | Recon: 0.3013 | \n",
      "Epoch 564 | Loss: 0.3045 | Recon: 0.3045 | \n",
      "Epoch 565 | Loss: 0.3100 | Recon: 0.3100 | \n",
      "Epoch 566 | Loss: 0.3100 | Recon: 0.3100 | \n",
      "Epoch 567 | Loss: 0.3170 | Recon: 0.3170 | \n",
      "Epoch 568 | Loss: 0.3034 | Recon: 0.3034 | \n",
      "Epoch 569 | Loss: 0.3107 | Recon: 0.3107 | \n",
      "Epoch 570 | Loss: 0.3062 | Recon: 0.3062 | \n",
      "Epoch 571 | Loss: 0.3017 | Recon: 0.3017 | \n",
      "Epoch 572 | Loss: 0.3037 | Recon: 0.3037 | \n",
      "Epoch 573 | Loss: 0.3172 | Recon: 0.3172 | \n",
      "Epoch 574 | Loss: 0.3084 | Recon: 0.3084 | \n",
      "Epoch 575 | Loss: 0.3159 | Recon: 0.3159 | \n",
      "Epoch 576 | Loss: 0.3143 | Recon: 0.3143 | \n",
      "Epoch 577 | Loss: 0.3110 | Recon: 0.3110 | \n",
      "Epoch 578 | Loss: 0.3188 | Recon: 0.3188 | \n",
      "Epoch 579 | Loss: 0.3007 | Recon: 0.3007 | \n",
      "Epoch 580 | Loss: 0.3097 | Recon: 0.3097 | \n",
      "Epoch 581 | Loss: 0.3078 | Recon: 0.3078 | \n",
      "Epoch 582 | Loss: 0.3040 | Recon: 0.3040 | \n",
      "Epoch 583 | Loss: 0.3146 | Recon: 0.3146 | \n",
      "Epoch 584 | Loss: 0.3150 | Recon: 0.3150 | \n",
      "Epoch 585 | Loss: 0.3037 | Recon: 0.3037 | \n",
      "Epoch 586 | Loss: 0.3133 | Recon: 0.3133 | \n",
      "Epoch 587 | Loss: 0.3100 | Recon: 0.3100 | \n",
      "Epoch 588 | Loss: 0.3167 | Recon: 0.3167 | \n",
      "Epoch 589 | Loss: 0.3085 | Recon: 0.3085 | \n",
      "Epoch 590 | Loss: 0.3151 | Recon: 0.3151 | \n",
      "Epoch 591 | Loss: 0.3009 | Recon: 0.3009 | \n",
      "Epoch 592 | Loss: 0.2981 | Recon: 0.2981 | \n",
      "Epoch 593 | Loss: 0.3063 | Recon: 0.3063 | \n",
      "Epoch 594 | Loss: 0.3008 | Recon: 0.3008 | \n",
      "Epoch 595 | Loss: 0.3034 | Recon: 0.3034 | \n",
      "Epoch 596 | Loss: 0.3008 | Recon: 0.3008 | \n",
      "Epoch 597 | Loss: 0.3095 | Recon: 0.3095 | \n",
      "Epoch 598 | Loss: 0.3024 | Recon: 0.3024 | \n",
      "Epoch 599 | Loss: 0.3042 | Recon: 0.3042 | \n",
      "Epoch 600 | Loss: 0.3173 | Recon: 0.3173 | \n",
      "Epoch 601 | Loss: 0.3195 | Recon: 0.3195 | \n",
      "Epoch 602 | Loss: 0.3082 | Recon: 0.3082 | \n",
      "Epoch 603 | Loss: 0.3105 | Recon: 0.3105 | \n",
      "Epoch 604 | Loss: 0.3033 | Recon: 0.3033 | \n",
      "Epoch 605 | Loss: 0.3120 | Recon: 0.3120 | \n",
      "Epoch 606 | Loss: 0.3050 | Recon: 0.3050 | \n",
      "Epoch 607 | Loss: 0.3203 | Recon: 0.3203 | \n",
      "Epoch 608 | Loss: 0.3036 | Recon: 0.3036 | \n",
      "Epoch 609 | Loss: 0.3052 | Recon: 0.3052 | \n",
      "Epoch 610 | Loss: 0.3019 | Recon: 0.3019 | \n",
      "Epoch 611 | Loss: 0.3124 | Recon: 0.3124 | \n",
      "Epoch 612 | Loss: 0.3048 | Recon: 0.3048 | \n",
      "Epoch 613 | Loss: 0.3024 | Recon: 0.3024 | \n",
      "Epoch 614 | Loss: 0.3056 | Recon: 0.3056 | \n",
      "Epoch 615 | Loss: 0.2967 | Recon: 0.2967 | \n",
      "Epoch 616 | Loss: 0.3073 | Recon: 0.3073 | \n",
      "Epoch 617 | Loss: 0.3072 | Recon: 0.3072 | \n",
      "Epoch 618 | Loss: 0.3059 | Recon: 0.3059 | \n",
      "Epoch 619 | Loss: 0.3448 | Recon: 0.3448 | \n",
      "Epoch 620 | Loss: 0.2998 | Recon: 0.2998 | \n",
      "Epoch 621 | Loss: 0.3141 | Recon: 0.3141 | \n",
      "Epoch 622 | Loss: 0.3042 | Recon: 0.3042 | \n",
      "Epoch 623 | Loss: 0.3022 | Recon: 0.3022 | \n",
      "Epoch 624 | Loss: 0.2972 | Recon: 0.2972 | \n",
      "Epoch 625 | Loss: 0.3004 | Recon: 0.3004 | \n",
      "Epoch 626 | Loss: 0.3005 | Recon: 0.3005 | \n",
      "Epoch 627 | Loss: 0.2986 | Recon: 0.2986 | \n",
      "Epoch 628 | Loss: 0.3004 | Recon: 0.3004 | \n",
      "Epoch 629 | Loss: 0.3060 | Recon: 0.3060 | \n",
      "Epoch 630 | Loss: 0.2999 | Recon: 0.2999 | \n",
      "Epoch 631 | Loss: 0.2988 | Recon: 0.2988 | \n",
      "Epoch 632 | Loss: 0.3047 | Recon: 0.3047 | \n",
      "Epoch 633 | Loss: 0.3004 | Recon: 0.3004 | \n",
      "Epoch 634 | Loss: 0.3003 | Recon: 0.3003 | \n",
      "Epoch 635 | Loss: 0.3060 | Recon: 0.3060 | \n",
      "Epoch 636 | Loss: 0.3099 | Recon: 0.3099 | \n",
      "Epoch 637 | Loss: 0.2981 | Recon: 0.2981 | \n",
      "Epoch 638 | Loss: 0.2961 | Recon: 0.2961 | \n",
      "Epoch 639 | Loss: 0.3044 | Recon: 0.3044 | \n",
      "Epoch 640 | Loss: 0.3041 | Recon: 0.3041 | \n",
      "Epoch 641 | Loss: 0.3040 | Recon: 0.3040 | \n",
      "Epoch 642 | Loss: 0.3068 | Recon: 0.3068 | \n",
      "Epoch 643 | Loss: 0.3167 | Recon: 0.3167 | \n",
      "Epoch 644 | Loss: 0.3106 | Recon: 0.3106 | \n",
      "Epoch 645 | Loss: 0.3058 | Recon: 0.3058 | \n",
      "Epoch 646 | Loss: 0.3064 | Recon: 0.3064 | \n",
      "Epoch 647 | Loss: 0.3046 | Recon: 0.3046 | \n",
      "Epoch 648 | Loss: 0.3004 | Recon: 0.3004 | \n",
      "Epoch 649 | Loss: 0.3090 | Recon: 0.3090 | \n",
      "Epoch 650 | Loss: 0.3014 | Recon: 0.3014 | \n",
      "Epoch 651 | Loss: 0.3018 | Recon: 0.3018 | \n",
      "Epoch 652 | Loss: 0.3022 | Recon: 0.3022 | \n",
      "Epoch 653 | Loss: 0.3000 | Recon: 0.3000 | \n",
      "Epoch 654 | Loss: 0.2972 | Recon: 0.2972 | \n",
      "Epoch 655 | Loss: 0.2998 | Recon: 0.2998 | \n",
      "Epoch 656 | Loss: 0.3036 | Recon: 0.3036 | \n",
      "Epoch 657 | Loss: 0.2961 | Recon: 0.2961 | \n",
      "Epoch 658 | Loss: 0.2992 | Recon: 0.2992 | \n",
      "Epoch 659 | Loss: 0.3110 | Recon: 0.3110 | \n",
      "Epoch 660 | Loss: 0.3185 | Recon: 0.3185 | \n",
      "Epoch 661 | Loss: 0.3090 | Recon: 0.3090 | \n",
      "Epoch 662 | Loss: 0.3008 | Recon: 0.3008 | \n",
      "Epoch 663 | Loss: 0.3113 | Recon: 0.3113 | \n",
      "Epoch 664 | Loss: 0.3028 | Recon: 0.3028 | \n",
      "Epoch 665 | Loss: 0.2967 | Recon: 0.2967 | \n",
      "Epoch 666 | Loss: 0.2953 | Recon: 0.2953 | \n",
      "Epoch 667 | Loss: 0.3027 | Recon: 0.3027 | \n",
      "Epoch 668 | Loss: 0.3007 | Recon: 0.3007 | \n",
      "Epoch 669 | Loss: 0.2993 | Recon: 0.2993 | \n",
      "Epoch 670 | Loss: 0.2987 | Recon: 0.2987 | \n",
      "Epoch 671 | Loss: 0.3010 | Recon: 0.3010 | \n",
      "Epoch 672 | Loss: 0.3031 | Recon: 0.3031 | \n",
      "Epoch 673 | Loss: 0.3005 | Recon: 0.3005 | \n",
      "Epoch 674 | Loss: 0.3064 | Recon: 0.3064 | \n",
      "Epoch 675 | Loss: 0.2997 | Recon: 0.2997 | \n",
      "Epoch 676 | Loss: 0.3023 | Recon: 0.3023 | \n",
      "Epoch 677 | Loss: 0.3015 | Recon: 0.3015 | \n",
      "Epoch 678 | Loss: 0.3037 | Recon: 0.3037 | \n",
      "Epoch 679 | Loss: 0.3017 | Recon: 0.3017 | \n",
      "Epoch 680 | Loss: 0.3056 | Recon: 0.3056 | \n",
      "Epoch 681 | Loss: 0.3025 | Recon: 0.3025 | \n",
      "Epoch 682 | Loss: 0.3033 | Recon: 0.3033 | \n",
      "Epoch 683 | Loss: 0.3063 | Recon: 0.3063 | \n",
      "Epoch 684 | Loss: 0.3037 | Recon: 0.3037 | \n",
      "Epoch 685 | Loss: 0.3010 | Recon: 0.3010 | \n",
      "Epoch 686 | Loss: 0.3034 | Recon: 0.3034 | \n",
      "Epoch 687 | Loss: 0.3062 | Recon: 0.3062 | \n",
      "Epoch 688 | Loss: 0.3076 | Recon: 0.3076 | \n",
      "Epoch 689 | Loss: 0.3062 | Recon: 0.3062 | \n",
      "Epoch 690 | Loss: 0.3072 | Recon: 0.3072 | \n",
      "Epoch 691 | Loss: 0.3001 | Recon: 0.3001 | \n",
      "Epoch 692 | Loss: 0.3018 | Recon: 0.3018 | \n",
      "Epoch 693 | Loss: 0.3041 | Recon: 0.3041 | \n",
      "Epoch 694 | Loss: 0.2982 | Recon: 0.2982 | \n",
      "Epoch 695 | Loss: 0.3008 | Recon: 0.3008 | \n",
      "Epoch 696 | Loss: 0.3032 | Recon: 0.3032 | \n",
      "Epoch 697 | Loss: 0.2951 | Recon: 0.2951 | \n",
      "Epoch 698 | Loss: 0.3021 | Recon: 0.3021 | \n",
      "Epoch 699 | Loss: 0.3076 | Recon: 0.3076 | \n",
      "Epoch 700 | Loss: 0.3066 | Recon: 0.3066 | \n",
      "Epoch 701 | Loss: 0.3056 | Recon: 0.3056 | \n",
      "Epoch 702 | Loss: 0.3043 | Recon: 0.3043 | \n",
      "Epoch 703 | Loss: 0.3090 | Recon: 0.3090 | \n",
      "Epoch 704 | Loss: 0.3078 | Recon: 0.3078 | \n",
      "Epoch 705 | Loss: 0.2930 | Recon: 0.2930 | \n",
      "Epoch 706 | Loss: 0.3012 | Recon: 0.3012 | \n",
      "Epoch 707 | Loss: 0.3013 | Recon: 0.3013 | \n",
      "Epoch 708 | Loss: 0.3010 | Recon: 0.3010 | \n",
      "Epoch 709 | Loss: 0.3069 | Recon: 0.3069 | \n",
      "Epoch 710 | Loss: 0.3082 | Recon: 0.3082 | \n",
      "Epoch 711 | Loss: 0.2967 | Recon: 0.2967 | \n",
      "Epoch 712 | Loss: 0.3022 | Recon: 0.3022 | \n",
      "Epoch 713 | Loss: 0.3097 | Recon: 0.3097 | \n",
      "Epoch 714 | Loss: 0.2975 | Recon: 0.2975 | \n",
      "Epoch 715 | Loss: 0.2994 | Recon: 0.2994 | \n",
      "Epoch 716 | Loss: 0.3086 | Recon: 0.3086 | \n",
      "Epoch 717 | Loss: 0.3006 | Recon: 0.3006 | \n",
      "Epoch 718 | Loss: 0.3169 | Recon: 0.3169 | \n",
      "Epoch 719 | Loss: 0.2984 | Recon: 0.2984 | \n",
      "Epoch 720 | Loss: 0.3002 | Recon: 0.3002 | \n",
      "Epoch 721 | Loss: 0.2997 | Recon: 0.2997 | \n",
      "Epoch 722 | Loss: 0.2950 | Recon: 0.2950 | \n",
      "Epoch 723 | Loss: 0.2986 | Recon: 0.2986 | \n",
      "Epoch 724 | Loss: 0.3013 | Recon: 0.3013 | \n",
      "Epoch 725 | Loss: 0.3054 | Recon: 0.3054 | \n",
      "Epoch 726 | Loss: 0.2971 | Recon: 0.2971 | \n",
      "Epoch 727 | Loss: 0.3043 | Recon: 0.3043 | \n",
      "Epoch 728 | Loss: 0.3070 | Recon: 0.3070 | \n",
      "Epoch 729 | Loss: 0.2944 | Recon: 0.2944 | \n",
      "Epoch 730 | Loss: 0.2989 | Recon: 0.2989 | \n",
      "Epoch 731 | Loss: 0.3020 | Recon: 0.3020 | \n",
      "Epoch 732 | Loss: 0.3053 | Recon: 0.3053 | \n",
      "Epoch 733 | Loss: 0.2960 | Recon: 0.2960 | \n",
      "Epoch 734 | Loss: 0.2956 | Recon: 0.2956 | \n",
      "Epoch 735 | Loss: 0.3012 | Recon: 0.3012 | \n",
      "Epoch 736 | Loss: 0.2953 | Recon: 0.2953 | \n",
      "Epoch 737 | Loss: 0.3003 | Recon: 0.3003 | \n",
      "Epoch 738 | Loss: 0.2971 | Recon: 0.2971 | \n",
      "Epoch 739 | Loss: 0.3043 | Recon: 0.3043 | \n",
      "Epoch 740 | Loss: 0.2998 | Recon: 0.2998 | \n",
      "Epoch 741 | Loss: 0.3026 | Recon: 0.3026 | \n",
      "Epoch 742 | Loss: 0.3084 | Recon: 0.3084 | \n",
      "Epoch 743 | Loss: 0.3218 | Recon: 0.3218 | \n",
      "Epoch 744 | Loss: 0.2971 | Recon: 0.2971 | \n",
      "Epoch 745 | Loss: 0.3033 | Recon: 0.3033 | \n",
      "Epoch 746 | Loss: 0.3133 | Recon: 0.3133 | \n",
      "Epoch 747 | Loss: 0.3075 | Recon: 0.3075 | \n",
      "Epoch 748 | Loss: 0.2953 | Recon: 0.2953 | \n",
      "Epoch 749 | Loss: 0.3150 | Recon: 0.3150 | \n",
      "Epoch 750 | Loss: 0.2997 | Recon: 0.2997 | \n",
      "Epoch 751 | Loss: 0.3005 | Recon: 0.3005 | \n",
      "Epoch 752 | Loss: 0.3044 | Recon: 0.3044 | \n",
      "Epoch 753 | Loss: 0.3067 | Recon: 0.3067 | \n",
      "Epoch 754 | Loss: 0.3030 | Recon: 0.3030 | \n",
      "Epoch 755 | Loss: 0.3003 | Recon: 0.3003 | \n",
      "Epoch 756 | Loss: 0.3032 | Recon: 0.3032 | \n",
      "Epoch 757 | Loss: 0.2961 | Recon: 0.2961 | \n",
      "Epoch 758 | Loss: 0.3000 | Recon: 0.3000 | \n",
      "Epoch 759 | Loss: 0.3099 | Recon: 0.3099 | \n",
      "Epoch 760 | Loss: 0.3101 | Recon: 0.3101 | \n",
      "Epoch 761 | Loss: 0.3014 | Recon: 0.3014 | \n",
      "Epoch 762 | Loss: 0.3042 | Recon: 0.3042 | \n",
      "Epoch 763 | Loss: 0.2933 | Recon: 0.2933 | \n",
      "Epoch 764 | Loss: 0.3032 | Recon: 0.3032 | \n",
      "Epoch 765 | Loss: 0.3001 | Recon: 0.3001 | \n",
      "Epoch 766 | Loss: 0.3085 | Recon: 0.3085 | \n",
      "Epoch 767 | Loss: 0.3059 | Recon: 0.3059 | \n",
      "Epoch 768 | Loss: 0.2963 | Recon: 0.2963 | \n",
      "Epoch 769 | Loss: 0.3007 | Recon: 0.3007 | \n",
      "Epoch 770 | Loss: 0.3020 | Recon: 0.3020 | \n",
      "Epoch 771 | Loss: 0.3070 | Recon: 0.3070 | \n",
      "Epoch 772 | Loss: 0.2957 | Recon: 0.2957 | \n",
      "Epoch 773 | Loss: 0.2999 | Recon: 0.2999 | \n",
      "Epoch 774 | Loss: 0.2996 | Recon: 0.2996 | \n",
      "Epoch 775 | Loss: 0.3015 | Recon: 0.3015 | \n",
      "Epoch 776 | Loss: 0.3054 | Recon: 0.3054 | \n",
      "Epoch 777 | Loss: 0.2967 | Recon: 0.2967 | \n",
      "Epoch 778 | Loss: 0.2986 | Recon: 0.2986 | \n",
      "Epoch 779 | Loss: 0.2963 | Recon: 0.2963 | \n",
      "Epoch 780 | Loss: 0.2988 | Recon: 0.2988 | \n",
      "Epoch 781 | Loss: 0.3013 | Recon: 0.3013 | \n",
      "Epoch 782 | Loss: 0.3019 | Recon: 0.3019 | \n",
      "Epoch 783 | Loss: 0.2989 | Recon: 0.2989 | \n",
      "Epoch 784 | Loss: 0.2977 | Recon: 0.2977 | \n",
      "Epoch 785 | Loss: 0.2986 | Recon: 0.2986 | \n",
      "Epoch 786 | Loss: 0.3002 | Recon: 0.3002 | \n",
      "Epoch 787 | Loss: 0.3117 | Recon: 0.3117 | \n",
      "Epoch 788 | Loss: 0.2950 | Recon: 0.2950 | \n",
      "Epoch 789 | Loss: 0.3056 | Recon: 0.3056 | \n",
      "Epoch 790 | Loss: 0.2928 | Recon: 0.2928 | \n",
      "Epoch 791 | Loss: 0.2978 | Recon: 0.2978 | \n",
      "Epoch 792 | Loss: 0.3016 | Recon: 0.3016 | \n",
      "Epoch 793 | Loss: 0.2960 | Recon: 0.2960 | \n",
      "Epoch 794 | Loss: 0.2983 | Recon: 0.2983 | \n",
      "Epoch 795 | Loss: 0.2989 | Recon: 0.2989 | \n",
      "Epoch 796 | Loss: 0.3075 | Recon: 0.3075 | \n",
      "Epoch 797 | Loss: 0.3055 | Recon: 0.3055 | \n",
      "Epoch 798 | Loss: 0.2980 | Recon: 0.2980 | \n",
      "Epoch 799 | Loss: 0.2966 | Recon: 0.2966 | \n",
      "Epoch 800 | Loss: 0.3025 | Recon: 0.3025 | \n",
      "Epoch 801 | Loss: 0.3015 | Recon: 0.3015 | \n",
      "Epoch 802 | Loss: 0.2974 | Recon: 0.2974 | \n",
      "Epoch 803 | Loss: 0.3056 | Recon: 0.3056 | \n",
      "Epoch 804 | Loss: 0.3029 | Recon: 0.3029 | \n",
      "Epoch 805 | Loss: 0.3006 | Recon: 0.3006 | \n",
      "Epoch 806 | Loss: 0.3062 | Recon: 0.3062 | \n",
      "Epoch 807 | Loss: 0.2987 | Recon: 0.2987 | \n",
      "Epoch 808 | Loss: 0.3000 | Recon: 0.3000 | \n",
      "Epoch 809 | Loss: 0.3009 | Recon: 0.3009 | \n",
      "Epoch 810 | Loss: 0.2957 | Recon: 0.2957 | \n",
      "Epoch 811 | Loss: 0.3090 | Recon: 0.3090 | \n",
      "Epoch 812 | Loss: 0.3001 | Recon: 0.3001 | \n",
      "Epoch 813 | Loss: 0.2963 | Recon: 0.2963 | \n",
      "Epoch 814 | Loss: 0.3023 | Recon: 0.3023 | \n",
      "Epoch 815 | Loss: 0.2958 | Recon: 0.2958 | \n",
      "Epoch 816 | Loss: 0.3063 | Recon: 0.3063 | \n",
      "Epoch 817 | Loss: 0.2946 | Recon: 0.2946 | \n",
      "Epoch 818 | Loss: 0.3033 | Recon: 0.3033 | \n",
      "Epoch 819 | Loss: 0.3082 | Recon: 0.3082 | \n",
      "Epoch 820 | Loss: 0.3028 | Recon: 0.3028 | \n",
      "Epoch 821 | Loss: 0.3032 | Recon: 0.3032 | \n",
      "Epoch 822 | Loss: 0.2919 | Recon: 0.2919 | \n",
      "Epoch 823 | Loss: 0.3029 | Recon: 0.3029 | \n",
      "Epoch 824 | Loss: 0.2976 | Recon: 0.2976 | \n",
      "Epoch 825 | Loss: 0.3001 | Recon: 0.3001 | \n",
      "Epoch 826 | Loss: 0.3014 | Recon: 0.3014 | \n",
      "Epoch 827 | Loss: 0.2997 | Recon: 0.2997 | \n",
      "Epoch 828 | Loss: 0.2974 | Recon: 0.2974 | \n",
      "Epoch 829 | Loss: 0.2980 | Recon: 0.2980 | \n",
      "Epoch 830 | Loss: 0.2924 | Recon: 0.2924 | \n",
      "Epoch 831 | Loss: 0.2994 | Recon: 0.2994 | \n",
      "Epoch 832 | Loss: 0.2947 | Recon: 0.2947 | \n",
      "Epoch 833 | Loss: 0.2976 | Recon: 0.2976 | \n",
      "Epoch 834 | Loss: 0.2967 | Recon: 0.2967 | \n",
      "Epoch 835 | Loss: 0.2935 | Recon: 0.2935 | \n",
      "Epoch 836 | Loss: 0.2961 | Recon: 0.2961 | \n",
      "Epoch 837 | Loss: 0.3002 | Recon: 0.3002 | \n",
      "Epoch 838 | Loss: 0.2995 | Recon: 0.2995 | \n",
      "Epoch 839 | Loss: 0.2953 | Recon: 0.2953 | \n",
      "Epoch 840 | Loss: 0.2923 | Recon: 0.2923 | \n",
      "Epoch 841 | Loss: 0.2980 | Recon: 0.2980 | \n",
      "Epoch 842 | Loss: 0.2938 | Recon: 0.2938 | \n",
      "Epoch 843 | Loss: 0.2979 | Recon: 0.2979 | \n",
      "Epoch 844 | Loss: 0.3019 | Recon: 0.3019 | \n",
      "Epoch 845 | Loss: 0.3025 | Recon: 0.3025 | \n",
      "Epoch 846 | Loss: 0.2950 | Recon: 0.2950 | \n",
      "Epoch 847 | Loss: 0.2975 | Recon: 0.2975 | \n",
      "Epoch 848 | Loss: 0.2993 | Recon: 0.2993 | \n",
      "Epoch 849 | Loss: 0.2978 | Recon: 0.2978 | \n",
      "Epoch 850 | Loss: 0.3043 | Recon: 0.3043 | \n",
      "Epoch 851 | Loss: 0.3016 | Recon: 0.3016 | \n",
      "Epoch 852 | Loss: 0.3000 | Recon: 0.3000 | \n",
      "Epoch 853 | Loss: 0.3013 | Recon: 0.3013 | \n",
      "Epoch 854 | Loss: 0.2996 | Recon: 0.2996 | \n",
      "Epoch 855 | Loss: 0.2996 | Recon: 0.2996 | \n",
      "Epoch 856 | Loss: 0.3027 | Recon: 0.3027 | \n",
      "Epoch 857 | Loss: 0.3030 | Recon: 0.3030 | \n",
      "Epoch 858 | Loss: 0.2939 | Recon: 0.2939 | \n",
      "Epoch 859 | Loss: 0.3014 | Recon: 0.3014 | \n",
      "Epoch 860 | Loss: 0.2989 | Recon: 0.2989 | \n",
      "Epoch 861 | Loss: 0.3002 | Recon: 0.3002 | \n",
      "Epoch 862 | Loss: 0.2957 | Recon: 0.2957 | \n",
      "Epoch 863 | Loss: 0.2953 | Recon: 0.2953 | \n",
      "Epoch 864 | Loss: 0.2983 | Recon: 0.2983 | \n",
      "Epoch 865 | Loss: 0.2964 | Recon: 0.2964 | \n",
      "Epoch 866 | Loss: 0.2988 | Recon: 0.2988 | \n",
      "Epoch 867 | Loss: 0.3000 | Recon: 0.3000 | \n",
      "Epoch 868 | Loss: 0.2966 | Recon: 0.2966 | \n",
      "Epoch 869 | Loss: 0.2982 | Recon: 0.2982 | \n",
      "Epoch 870 | Loss: 0.3004 | Recon: 0.3004 | \n",
      "Epoch 871 | Loss: 0.2997 | Recon: 0.2997 | \n",
      "Epoch 872 | Loss: 0.3019 | Recon: 0.3019 | \n",
      "Epoch 873 | Loss: 0.2955 | Recon: 0.2955 | \n",
      "Epoch 874 | Loss: 0.2958 | Recon: 0.2958 | \n",
      "Epoch 875 | Loss: 0.2927 | Recon: 0.2927 | \n",
      "Epoch 876 | Loss: 0.2961 | Recon: 0.2961 | \n",
      "Epoch 877 | Loss: 0.3048 | Recon: 0.3048 | \n",
      "Epoch 878 | Loss: 0.2941 | Recon: 0.2941 | \n",
      "Epoch 879 | Loss: 0.3122 | Recon: 0.3122 | \n",
      "Epoch 880 | Loss: 0.3016 | Recon: 0.3016 | \n",
      "Epoch 881 | Loss: 0.2987 | Recon: 0.2987 | \n",
      "Epoch 882 | Loss: 0.2946 | Recon: 0.2946 | \n",
      "Epoch 883 | Loss: 0.2998 | Recon: 0.2998 | \n",
      "Epoch 884 | Loss: 0.2967 | Recon: 0.2967 | \n",
      "Epoch 885 | Loss: 0.2985 | Recon: 0.2985 | \n",
      "Epoch 886 | Loss: 0.3049 | Recon: 0.3049 | \n",
      "Epoch 887 | Loss: 0.2991 | Recon: 0.2991 | \n",
      "Epoch 888 | Loss: 0.3085 | Recon: 0.3085 | \n",
      "Epoch 889 | Loss: 0.2957 | Recon: 0.2957 | \n",
      "Epoch 890 | Loss: 0.2996 | Recon: 0.2996 | \n",
      "Epoch 891 | Loss: 0.3017 | Recon: 0.3017 | \n",
      "Epoch 892 | Loss: 0.2975 | Recon: 0.2975 | \n",
      "Epoch 893 | Loss: 0.2926 | Recon: 0.2926 | \n",
      "Epoch 894 | Loss: 0.3023 | Recon: 0.3023 | \n",
      "Epoch 895 | Loss: 0.3016 | Recon: 0.3016 | \n",
      "Epoch 896 | Loss: 0.2964 | Recon: 0.2964 | \n",
      "Epoch 897 | Loss: 0.2989 | Recon: 0.2989 | \n",
      "Epoch 898 | Loss: 0.2951 | Recon: 0.2951 | \n",
      "Epoch 899 | Loss: 0.2985 | Recon: 0.2985 | \n",
      "Epoch 900 | Loss: 0.2966 | Recon: 0.2966 | \n",
      "Epoch 901 | Loss: 0.2988 | Recon: 0.2988 | \n",
      "Epoch 902 | Loss: 0.2906 | Recon: 0.2906 | \n",
      "Epoch 903 | Loss: 0.2979 | Recon: 0.2979 | \n",
      "Epoch 904 | Loss: 0.2910 | Recon: 0.2910 | \n",
      "Epoch 905 | Loss: 0.3089 | Recon: 0.3089 | \n",
      "Epoch 906 | Loss: 0.2947 | Recon: 0.2947 | \n",
      "Epoch 907 | Loss: 0.3021 | Recon: 0.3021 | \n",
      "Epoch 908 | Loss: 0.2964 | Recon: 0.2964 | \n",
      "Epoch 909 | Loss: 0.2983 | Recon: 0.2983 | \n",
      "Epoch 910 | Loss: 0.2963 | Recon: 0.2963 | \n",
      "Epoch 911 | Loss: 0.2957 | Recon: 0.2957 | \n",
      "Epoch 912 | Loss: 0.2998 | Recon: 0.2998 | \n",
      "Epoch 913 | Loss: 0.2930 | Recon: 0.2930 | \n",
      "Epoch 914 | Loss: 0.3015 | Recon: 0.3015 | \n",
      "Epoch 915 | Loss: 0.2986 | Recon: 0.2986 | \n",
      "Epoch 916 | Loss: 0.3065 | Recon: 0.3065 | \n",
      "Epoch 917 | Loss: 0.3045 | Recon: 0.3045 | \n",
      "Epoch 918 | Loss: 0.2997 | Recon: 0.2997 | \n",
      "Epoch 919 | Loss: 0.2956 | Recon: 0.2956 | \n",
      "Epoch 920 | Loss: 0.2997 | Recon: 0.2997 | \n",
      "Epoch 921 | Loss: 0.2939 | Recon: 0.2939 | \n",
      "Epoch 922 | Loss: 0.3094 | Recon: 0.3094 | \n",
      "Epoch 923 | Loss: 0.2978 | Recon: 0.2978 | \n",
      "Epoch 924 | Loss: 0.3033 | Recon: 0.3033 | \n",
      "Epoch 925 | Loss: 0.2989 | Recon: 0.2989 | \n",
      "Epoch 926 | Loss: 0.3067 | Recon: 0.3067 | \n",
      "Epoch 927 | Loss: 0.2970 | Recon: 0.2970 | \n",
      "Epoch 928 | Loss: 0.3002 | Recon: 0.3002 | \n",
      "Epoch 929 | Loss: 0.2945 | Recon: 0.2945 | \n",
      "Epoch 930 | Loss: 0.2939 | Recon: 0.2939 | \n",
      "Epoch 931 | Loss: 0.2975 | Recon: 0.2975 | \n",
      "Epoch 932 | Loss: 0.2984 | Recon: 0.2984 | \n",
      "Epoch 933 | Loss: 0.3027 | Recon: 0.3027 | \n",
      "Epoch 934 | Loss: 0.3033 | Recon: 0.3033 | \n",
      "Epoch 935 | Loss: 0.2957 | Recon: 0.2957 | \n",
      "Epoch 936 | Loss: 0.2944 | Recon: 0.2944 | \n",
      "Epoch 937 | Loss: 0.3048 | Recon: 0.3048 | \n",
      "Epoch 938 | Loss: 0.3040 | Recon: 0.3040 | \n",
      "Epoch 939 | Loss: 0.2958 | Recon: 0.2958 | \n",
      "Epoch 940 | Loss: 0.3032 | Recon: 0.3032 | \n",
      "Epoch 941 | Loss: 0.2958 | Recon: 0.2958 | \n",
      "Epoch 942 | Loss: 0.2979 | Recon: 0.2979 | \n",
      "Epoch 943 | Loss: 0.3016 | Recon: 0.3016 | \n",
      "Epoch 944 | Loss: 0.3013 | Recon: 0.3013 | \n",
      "Epoch 945 | Loss: 0.2955 | Recon: 0.2955 | \n",
      "Epoch 946 | Loss: 0.2903 | Recon: 0.2903 | \n",
      "Epoch 947 | Loss: 0.2984 | Recon: 0.2984 | \n",
      "Epoch 948 | Loss: 0.2948 | Recon: 0.2948 | \n",
      "Epoch 949 | Loss: 0.2967 | Recon: 0.2967 | \n",
      "Epoch 950 | Loss: 0.2943 | Recon: 0.2943 | \n",
      "Epoch 951 | Loss: 0.2985 | Recon: 0.2985 | \n",
      "Epoch 952 | Loss: 0.3079 | Recon: 0.3079 | \n",
      "Epoch 953 | Loss: 0.2963 | Recon: 0.2963 | \n",
      "Epoch 954 | Loss: 0.2934 | Recon: 0.2934 | \n",
      "Epoch 955 | Loss: 0.2973 | Recon: 0.2973 | \n",
      "Epoch 956 | Loss: 0.2938 | Recon: 0.2938 | \n",
      "Epoch 957 | Loss: 0.2988 | Recon: 0.2988 | \n",
      "Epoch 958 | Loss: 0.2974 | Recon: 0.2974 | \n",
      "Epoch 959 | Loss: 0.3011 | Recon: 0.3011 | \n",
      "Epoch 960 | Loss: 0.2983 | Recon: 0.2983 | \n",
      "Epoch 961 | Loss: 0.2995 | Recon: 0.2995 | \n",
      "Epoch 962 | Loss: 0.2961 | Recon: 0.2961 | \n",
      "Epoch 963 | Loss: 0.2922 | Recon: 0.2922 | \n",
      "Epoch 964 | Loss: 0.2960 | Recon: 0.2960 | \n",
      "Epoch 965 | Loss: 0.2979 | Recon: 0.2979 | \n",
      "Epoch 966 | Loss: 0.2925 | Recon: 0.2925 | \n",
      "Epoch 967 | Loss: 0.3026 | Recon: 0.3026 | \n",
      "Epoch 968 | Loss: 0.3030 | Recon: 0.3030 | \n",
      "Epoch 969 | Loss: 0.2985 | Recon: 0.2985 | \n",
      "Epoch 970 | Loss: 0.3049 | Recon: 0.3049 | \n",
      "Epoch 971 | Loss: 0.2976 | Recon: 0.2976 | \n",
      "Epoch 972 | Loss: 0.2878 | Recon: 0.2878 | \n",
      "Epoch 973 | Loss: 0.2950 | Recon: 0.2950 | \n",
      "Epoch 974 | Loss: 0.2988 | Recon: 0.2988 | \n",
      "Epoch 975 | Loss: 0.2936 | Recon: 0.2936 | \n",
      "Epoch 976 | Loss: 0.2949 | Recon: 0.2949 | \n",
      "Epoch 977 | Loss: 0.2924 | Recon: 0.2924 | \n",
      "Epoch 978 | Loss: 0.2979 | Recon: 0.2979 | \n",
      "Epoch 979 | Loss: 0.2989 | Recon: 0.2989 | \n",
      "Epoch 980 | Loss: 0.2959 | Recon: 0.2959 | \n",
      "Epoch 981 | Loss: 0.3006 | Recon: 0.3006 | \n",
      "Epoch 982 | Loss: 0.3043 | Recon: 0.3043 | \n",
      "Epoch 983 | Loss: 0.2948 | Recon: 0.2948 | \n",
      "Epoch 984 | Loss: 0.2997 | Recon: 0.2997 | \n",
      "Epoch 985 | Loss: 0.3076 | Recon: 0.3076 | \n",
      "Epoch 986 | Loss: 0.2951 | Recon: 0.2951 | \n",
      "Epoch 987 | Loss: 0.2987 | Recon: 0.2987 | \n",
      "Epoch 988 | Loss: 0.2987 | Recon: 0.2987 | \n",
      "Epoch 989 | Loss: 0.3006 | Recon: 0.3006 | \n",
      "Epoch 990 | Loss: 0.3060 | Recon: 0.3060 | \n",
      "Epoch 991 | Loss: 0.3023 | Recon: 0.3023 | \n",
      "Epoch 992 | Loss: 0.2913 | Recon: 0.2913 | \n",
      "Epoch 993 | Loss: 0.2969 | Recon: 0.2969 | \n",
      "Epoch 994 | Loss: 0.2980 | Recon: 0.2980 | \n",
      "Epoch 995 | Loss: 0.2992 | Recon: 0.2992 | \n",
      "Epoch 996 | Loss: 0.2939 | Recon: 0.2939 | \n",
      "Epoch 997 | Loss: 0.3046 | Recon: 0.3046 | \n",
      "Epoch 998 | Loss: 0.3022 | Recon: 0.3022 | \n",
      "Epoch 999 | Loss: 0.2917 | Recon: 0.2917 | \n",
      "Epoch 1000 | Loss: 0.3030 | Recon: 0.3030 | \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vae = MaskedVAE(input_dim=4, hidden_dim=3, latent_dim=2)\n",
    "vae = train_masked_vae(vae, dataloader, device,epochs=1000,lr=1e-3)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code ran for 8.24 seconds\n"
     ]
    }
   ],
   "source": [
    "elapsed = end_time - start_time\n",
    "print(f\"Code ran for {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = extract_latent_features(vae, X_scaled,mask_tensor,device)\n",
    "latent_np = latent.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.9683517 ,  -0.4030925 ],\n",
       "       [  0.97714555,  -0.41217536],\n",
       "       [  0.97581625,  -0.41081858],\n",
       "       [  0.96903527,  -0.40383023],\n",
       "       [  0.96565276,  -0.40031576],\n",
       "       [  0.88982594,  -0.32203907],\n",
       "       [  0.9612055 ,  -0.39575344],\n",
       "       [  0.9656391 ,  -0.4003009 ],\n",
       "       [  0.9758154 ,  -0.41083705],\n",
       "       [  0.10375303,   0.36630014],\n",
       "       [  0.9622431 ,  -0.39677155],\n",
       "       [  0.9602276 ,  -0.39473245],\n",
       "       [  0.41262746,   0.11898038],\n",
       "       [  0.99113774,  -0.42665094],\n",
       "       [  0.9710523 ,  -0.4058323 ],\n",
       "       [  0.84171903,  -0.2723668 ],\n",
       "       [  0.9546932 ,  -0.3889818 ],\n",
       "       [  0.96253574,  -0.39709187],\n",
       "       [ -9.255842  ,   9.870984  ],\n",
       "       [  0.05904806,   0.4143396 ],\n",
       "       [  0.9601884 ,  -0.39465097],\n",
       "       [  0.9485966 ,  -0.38271266],\n",
       "       [  0.9792404 ,  -0.41435903],\n",
       "       [  0.9427534 ,  -0.37668237],\n",
       "       [  0.94802225,  -0.38214236],\n",
       "       [  0.9696803 ,  -0.40446794],\n",
       "       [  0.94993883,  -0.3841029 ],\n",
       "       [  0.96495485,  -0.39958167],\n",
       "       [  0.97105074,  -0.40586933],\n",
       "       [  0.9636109 ,  -0.3982285 ],\n",
       "       [  0.9663099 ,  -0.4010053 ],\n",
       "       [  0.16287577,   0.32436728],\n",
       "       [  0.95860636,  -0.39303675],\n",
       "       [  0.9539409 ,  -0.388201  ],\n",
       "       [  0.9710498 ,  -0.40588778],\n",
       "       [  0.98189926,  -0.4170728 ],\n",
       "       [  0.80605197,  -0.23620999],\n",
       "       [  0.9707972 ,  -0.4056305 ],\n",
       "       [  0.9778564 ,  -0.4129428 ],\n",
       "       [  0.9663107 ,  -0.40098676],\n",
       "       [  0.96593267,  -0.40060267],\n",
       "       [  0.98690385,  -0.4222645 ],\n",
       "       [  0.9738017 ,  -0.40876102],\n",
       "       [  0.9316276 ,  -0.36520916],\n",
       "       [  0.8962528 ,  -0.3286978 ],\n",
       "       [  0.9706582 ,  -0.40548888],\n",
       "       [  0.9541327 ,  -0.3884264 ],\n",
       "       [  0.97107625,  -0.40593594],\n",
       "       [  0.96157146,  -0.39608568],\n",
       "       [  0.971735  ,  -0.4065885 ],\n",
       "       [-13.363126  ,  14.050557  ],\n",
       "       [-12.071136  ,  12.738423  ],\n",
       "       [-14.100001  ,  14.798444  ],\n",
       "       [ -6.668114  ,   7.2525873 ],\n",
       "       [-12.023498  ,  12.690089  ],\n",
       "       [ -9.217659  ,   9.840928  ],\n",
       "       [-13.077893  ,  13.760308  ],\n",
       "       [ -2.1332607 ,   2.6485538 ],\n",
       "       [-11.406951  ,  12.064307  ],\n",
       "       [ -6.798582  ,   7.3848042 ],\n",
       "       [ -2.3824778 ,   2.9015532 ],\n",
       "       [ -9.934299  ,  10.568732  ],\n",
       "       [ -6.089411  ,   6.6656    ],\n",
       "       [-11.200804  ,  11.854508  ],\n",
       "       [ -9.646347  ,  10.27011   ],\n",
       "       [-11.751786  ,  12.414545  ],\n",
       "       [-10.239033  ,  10.877712  ],\n",
       "       [ -6.6704245 ,   7.2552576 ],\n",
       "       [ -5.570397  ,   6.129984  ],\n",
       "       [ -5.889658  ,   6.4624767 ],\n",
       "       [-13.405463  ,  14.092407  ],\n",
       "       [ -8.514492  ,   9.127624  ],\n",
       "       [-12.118681  ,  12.7864065 ],\n",
       "       [-13.296928  ,  13.973355  ],\n",
       "       [-10.1258335 ,  10.763637  ],\n",
       "       [-11.421568  ,  12.0792055 ],\n",
       "       [-12.48625   ,  13.156904  ],\n",
       "       [-14.818123  ,  15.527216  ],\n",
       "       [ -6.9109297 ,   7.4937487 ],\n",
       "       [ -4.559074  ,   5.1119275 ],\n",
       "       [ -5.2625823 ,   5.8258047 ],\n",
       "       [ -4.492345  ,   5.0439134 ],\n",
       "       [-10.224111  ,  10.859968  ],\n",
       "       [-12.869826  ,  13.548615  ],\n",
       "       [ -9.848478  ,  10.481034  ],\n",
       "       [-12.033294  ,  12.69963   ],\n",
       "       [-13.115736  ,  13.799101  ],\n",
       "       [ -9.417754  ,  10.044624  ],\n",
       "       [ -8.104845  ,   8.711263  ],\n",
       "       [ -6.9380007 ,   7.5265894 ],\n",
       "       [ -7.7869816 ,   8.388357  ],\n",
       "       [-11.038891  ,  11.690175  ],\n",
       "       [ -7.185394  ,   7.778047  ],\n",
       "       [ -2.1935945 ,   2.7098906 ],\n",
       "       [ -7.996872  ,   8.601592  ],\n",
       "       [ -8.123594  ,   8.730372  ],\n",
       "       [ -8.462036  ,   9.073934  ],\n",
       "       [ -9.735277  ,  10.366959  ],\n",
       "       [ -2.2415757 ,   2.7587967 ],\n",
       "       [ -8.030238  ,   8.635601  ],\n",
       "       [-21.197454  ,  22.002666  ],\n",
       "       [-13.899423  ,  14.593621  ],\n",
       "       [-20.164457  ,  20.9548    ],\n",
       "       [-16.156586  ,  16.885414  ],\n",
       "       [-19.169325  ,  19.943998  ],\n",
       "       [-23.218832  ,  24.055817  ],\n",
       "       [ -9.144145  ,   9.765459  ],\n",
       "       [-20.187346  ,  20.978123  ],\n",
       "       [-16.991636  ,  17.73343   ],\n",
       "       [-23.656635  ,  24.500048  ],\n",
       "       [-17.568054  ,  18.312353  ],\n",
       "       [-15.664797  ,  16.386318  ],\n",
       "       [-18.391209  ,  19.154457  ],\n",
       "       [-13.610786  ,  14.3005085 ],\n",
       "       [-16.40128   ,  17.133425  ],\n",
       "       [-18.23304   ,  18.993563  ],\n",
       "       [-16.38523   ,  17.117762  ],\n",
       "       [-25.263887  ,  26.132051  ],\n",
       "       [-19.827742  ,  20.602657  ],\n",
       "       [-11.424876  ,  12.08172   ],\n",
       "       [-20.396845  ,  21.19058   ],\n",
       "       [-13.523482  ,  14.21184   ],\n",
       "       [-22.967697  ,  23.800924  ],\n",
       "       [-13.808714  ,  14.502089  ],\n",
       "       [-19.194468  ,  19.969784  ],\n",
       "       [-19.506327  ,  20.28679   ],\n",
       "       [-13.451524  ,  14.139418  ],\n",
       "       [-13.822987  ,  14.516414  ],\n",
       "       [-17.637068  ,  18.388433  ],\n",
       "       [-17.695969  ,  18.449007  ],\n",
       "       [-20.127352  ,  20.917356  ],\n",
       "       [-23.817114  ,  24.663614  ],\n",
       "       [-18.11045   ,  18.868994  ],\n",
       "       [-13.11722   ,  13.800074  ],\n",
       "       [-13.467673  ,  14.155494  ],\n",
       "       [-22.876598  ,  23.708614  ],\n",
       "       [-19.671597  ,  20.453781  ],\n",
       "       [-16.324892  ,  17.056421  ],\n",
       "       [-13.330855  ,  14.016744  ],\n",
       "       [-18.424574  ,  19.188465  ],\n",
       "       [-20.047878  ,  20.836132  ],\n",
       "       [-16.103022  ,  16.826397  ],\n",
       "       [-13.899423  ,  14.593621  ],\n",
       "       [-20.79528   ,  21.594906  ],\n",
       "       [-21.088     ,  21.892025  ],\n",
       "       [-18.252129  ,  19.01324   ],\n",
       "       [-14.309065  ,  15.009978  ],\n",
       "       [-16.441427  ,  17.174887  ],\n",
       "       [-18.409225  ,  19.172218  ],\n",
       "       [-14.026144  ,  14.722401  ]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Stage 2: Distributional Evidential Clustering (Option 3)\n",
    "# ------------------------------------------------------------\n",
    "# Input  : U  -> latent features from Stage 1 (masked AE/VAE)\n",
    "# Output : fuzzy memberships, hard labels, cluster parameters\n",
    "#\n",
    "# Key design choices (intentional):\n",
    "# - Clusters are Gaussian distributions in latent space\n",
    "# - Memberships are evidential (fuzzy), not probabilistic\n",
    "# - Distance is negative log-likelihood (not Euclidean)\n",
    "# - Explicit clusterâ€“cluster divergence regularization\n",
    "# - Diagonal covariance for stability and efficiency\n",
    "# - No EM, no mixture weights, no DST\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Negative log-likelihood under a diagonal Gaussian\n",
    "# This replaces point-wise Euclidean distance\n",
    "# ------------------------------------------------------------\n",
    "def neg_log_likelihood(u, mu, var, eps=1e-6):\n",
    "    \"\"\"\n",
    "    u   : latent feature vector of a sample (d,)\n",
    "    mu  : cluster mean (d,)\n",
    "    var : diagonal variance of cluster (d,)\n",
    "    \"\"\"\n",
    "    d = u.shape[0]\n",
    "    return 0.5 * (\n",
    "        np.sum(np.log(var + eps)) +\n",
    "        np.sum((u - mu) ** 2 / (var + eps)) +\n",
    "        d * np.log(2 * np.pi)\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# KL divergence between two diagonal Gaussian clusters\n",
    "# Used to explicitly penalize cluster similarity\n",
    "# ------------------------------------------------------------\n",
    "def kl_divergence(mu1, var1, mu2, var2, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes KL( N(mu1,var1) || N(mu2,var2) )\n",
    "    \"\"\"\n",
    "    return 0.5 * np.sum(\n",
    "        np.log((var2 + eps) / (var1 + eps)) +\n",
    "        (var1 + (mu1 - mu2) ** 2) / (var2 + eps) - 1.0\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Initialization\n",
    "# - Means initialized from random latent points\n",
    "# - Variances initialized to ones\n",
    "# - Memberships initialized uniformly (maximum uncertainty)\n",
    "# ------------------------------------------------------------\n",
    "def initialize_clusters(U, n_clusters, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n, d = U.shape\n",
    "\n",
    "    idx = np.random.choice(n, n_clusters, replace=False)\n",
    "    mu = U[idx].copy()\n",
    "    var = np.ones((n_clusters, d))           # diagonal covariance\n",
    "    M = np.ones((n, n_clusters)) / n_clusters\n",
    "\n",
    "    return mu, var, M\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute divergence penalty for each cluster\n",
    "# Î”_j = sum_{kâ‰ j} KL(C_j || C_k)\n",
    "# ------------------------------------------------------------\n",
    "def compute_cluster_penalties(mu, var):\n",
    "    c = mu.shape[0]\n",
    "    Delta = np.zeros(c)\n",
    "\n",
    "    for j in range(c):\n",
    "        for k in range(c):\n",
    "            if j != k:\n",
    "                Delta[j] += kl_divergence(\n",
    "                    mu[j], var[j],\n",
    "                    mu[k], var[k]\n",
    "                )\n",
    "    return Delta\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Membership update (Option 3)\n",
    "# Implements Eq. (4) in the paper\n",
    "# Uses direct exponentiation (no logsumexp)\n",
    "# ------------------------------------------------------------\n",
    "def update_memberships(U, mu, var, beta, gamma, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Memberships are updated using likelihood + divergence cost.\n",
    "    This is NOT a posterior probability (unlike GMM).\n",
    "    \"\"\"\n",
    "    n, _ = U.shape\n",
    "    c = mu.shape[0]\n",
    "\n",
    "    Delta = compute_cluster_penalties(mu, var)\n",
    "    M = np.zeros((n, c))\n",
    "\n",
    "    for i in range(n):\n",
    "        numerators = np.zeros(c)\n",
    "\n",
    "        for j in range(c):\n",
    "            # Combined cost:\n",
    "            # data-to-cluster fit + cluster-separation penalty\n",
    "            cost_ij = (\n",
    "                neg_log_likelihood(U[i], mu[j], var[j])\n",
    "                + gamma * Delta[j]\n",
    "            )\n",
    "\n",
    "            numerators[j] = np.exp(-cost_ij / (beta - 1.0))\n",
    "\n",
    "        # Normalize memberships to sum to 1\n",
    "        M[i, :] = numerators / (np.sum(numerators) + eps)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cluster parameter update\n",
    "# Weighted maximum likelihood using evidential memberships\n",
    "# Implements Eq. (5) and Eq. (6)\n",
    "# ------------------------------------------------------------\n",
    "def update_clusters(U, M, beta, eps=1e-8):\n",
    "    n, d = U.shape\n",
    "    c = M.shape[1]\n",
    "\n",
    "    mu_new = np.zeros((c, d))\n",
    "    var_new = np.zeros((c, d))\n",
    "    M_beta = M ** beta\n",
    "\n",
    "    for j in range(c):\n",
    "        w = M_beta[:, j][:, None]\n",
    "        denom = np.sum(w) + eps\n",
    "\n",
    "        # Mean update\n",
    "        mu_new[j] = np.sum(w * U, axis=0) / denom\n",
    "\n",
    "        # Diagonal covariance update\n",
    "        var_new[j] = np.sum(\n",
    "            w * (U - mu_new[j]) ** 2, axis=0\n",
    "        ) / denom\n",
    "\n",
    "    return mu_new, var_new\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main optimization loop (Algorithm 1 in paper)\n",
    "# ------------------------------------------------------------\n",
    "def distributional_evidential_clustering(\n",
    "    U,\n",
    "    n_clusters,\n",
    "    beta=2.0,\n",
    "    gamma=0.05,\n",
    "    max_iter=3,\n",
    "    tol=1e-4,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs distributional evidential clustering on latent features.\n",
    "    \"\"\"\n",
    "    mu, var, M = initialize_clusters(U, n_clusters, seed)\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        mu_old = mu.copy()\n",
    "\n",
    "        # Step 1: update memberships\n",
    "        M = update_memberships(U, mu, var, beta, gamma)\n",
    "\n",
    "        # Step 2: update cluster distributions\n",
    "        mu, var = update_clusters(U, M, beta)\n",
    "\n",
    "        # Convergence check on cluster means\n",
    "        shift = np.max(np.linalg.norm(mu - mu_old, axis=1))\n",
    "        if shift < tol:\n",
    "            print(f\"Converged at iteration {it}\")\n",
    "            break\n",
    "\n",
    "    labels = np.argmax(M, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"memberships\": M,\n",
    "        \"labels\": labels,\n",
    "        \"mu\": mu,\n",
    "        \"var\": var\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 45\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = distributional_evidential_clustering(\n",
    "    latent_np,\n",
    "    n_clusters=3,\n",
    "    beta=2,\n",
    "    gamma=0.00000001,\n",
    "    max_iter=50,\n",
    "    tol=1e-8,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memberships': array([[8.40629411e-08, 9.99999916e-01, 8.47968548e-36],\n",
       "        [2.11448287e-07, 9.99999789e-01, 2.02307102e-35],\n",
       "        [1.70955735e-07, 9.99999829e-01, 1.64870321e-35],\n",
       "        [8.67038691e-08, 9.99999913e-01, 8.70940088e-36],\n",
       "        [7.99019499e-08, 9.99999920e-01, 8.19155369e-36],\n",
       "        [9.99999990e-01, 2.28785578e-12, 1.61599228e-28],\n",
       "        [9.31160714e-08, 9.99999907e-01, 9.80404276e-36],\n",
       "        [7.99035596e-08, 9.99999920e-01, 8.19240954e-36],\n",
       "        [1.71176344e-07, 9.99999829e-01, 1.65074554e-35],\n",
       "        [9.99999995e-01, 0.00000000e+00, 1.17395208e-26],\n",
       "        [8.76187027e-08, 9.99999912e-01, 9.16931043e-36],\n",
       "        [1.00251127e-07, 9.99999900e-01, 1.06178501e-35],\n",
       "        [9.99999993e-01, 0.00000000e+00, 2.36542665e-27],\n",
       "        [1.00383297e-05, 9.99989962e-01, 8.82827550e-34],\n",
       "        [9.84632027e-08, 9.99999902e-01, 9.77371159e-36],\n",
       "        [9.99999990e-01, 1.20907798e-43, 2.15535028e-28],\n",
       "        [1.99582155e-07, 9.99999800e-01, 2.18553987e-35],\n",
       "        [8.62998748e-08, 9.99999914e-01, 9.01494395e-36],\n",
       "        [9.99999919e-01, 0.00000000e+00, 8.05530577e-08],\n",
       "        [9.99999995e-01, 0.00000000e+00, 1.53017285e-26],\n",
       "        [1.00743274e-07, 9.99999899e-01, 1.06737705e-35],\n",
       "        [7.21178966e-07, 9.99999279e-01, 8.19132916e-35],\n",
       "        [3.13103531e-07, 9.99999687e-01, 2.95799086e-35],\n",
       "        [4.17160308e-06, 9.99995828e-01, 4.90741440e-34],\n",
       "        [8.35373025e-07, 9.99999165e-01, 9.52053288e-35],\n",
       "        [8.97227161e-08, 9.99999910e-01, 8.97850400e-36],\n",
       "        [5.17658470e-07, 9.99999482e-01, 5.83240048e-35],\n",
       "        [8.02746500e-08, 9.99999920e-01, 8.26468774e-36],\n",
       "        [9.85966802e-08, 9.99999901e-01, 9.78594452e-36],\n",
       "        [8.26333273e-08, 9.99999917e-01, 8.57566567e-36],\n",
       "        [8.00943415e-08, 9.99999920e-01, 8.17864066e-36],\n",
       "        [9.99999994e-01, 0.00000000e+00, 8.78582755e-27],\n",
       "        [1.16996912e-07, 9.99999883e-01, 1.25135571e-35],\n",
       "        [2.27097855e-07, 9.99999773e-01, 2.49814834e-35],\n",
       "        [9.86628599e-08, 9.99999901e-01, 9.79201189e-36],\n",
       "        [5.63149738e-07, 9.99999437e-01, 5.23633371e-35],\n",
       "        [9.99999991e-01, 3.25874228e-76, 2.66223060e-28],\n",
       "        [9.67602381e-08, 9.99999903e-01, 9.61767880e-36],\n",
       "        [2.40344762e-07, 9.99999760e-01, 2.28950901e-35],\n",
       "        [8.00884836e-08, 9.99999920e-01, 8.17846745e-36],\n",
       "        [7.99188388e-08, 9.99999920e-01, 8.17956113e-36],\n",
       "        [2.28740249e-06, 9.99997713e-01, 2.06366768e-34],\n",
       "        [1.30194829e-07, 9.99999870e-01, 1.27082344e-35],\n",
       "        [4.83274963e-04, 9.99516725e-01, 6.07767656e-32],\n",
       "        [9.99999987e-01, 2.52978723e-09, 1.55482597e-28],\n",
       "        [9.57673332e-08, 9.99999904e-01, 9.52689267e-36],\n",
       "        [2.19078090e-07, 9.99999781e-01, 2.40696041e-35],\n",
       "        [9.89510281e-08, 9.99999901e-01, 9.81845011e-36],\n",
       "        [9.10533469e-08, 9.99999909e-01, 9.56707607e-36],\n",
       "        [1.04522077e-07, 9.99999895e-01, 1.03310774e-35],\n",
       "        [9.81648713e-01, 0.00000000e+00, 1.83512863e-02],\n",
       "        [9.99402099e-01, 0.00000000e+00, 5.97900939e-04],\n",
       "        [9.00052203e-01, 0.00000000e+00, 9.99477963e-02],\n",
       "        [1.00000000e+00, 0.00000000e+00, 4.23543300e-12],\n",
       "        [9.99477338e-01, 0.00000000e+00, 5.22661907e-04],\n",
       "        [9.99999928e-01, 0.00000000e+00, 7.15280823e-08],\n",
       "        [9.91034862e-01, 0.00000000e+00, 8.96513752e-03],\n",
       "        [9.99999999e-01, 0.00000000e+00, 2.63218471e-21],\n",
       "        [9.99912819e-01, 0.00000000e+00, 8.71808627e-05],\n",
       "        [1.00000000e+00, 0.00000000e+00, 7.23769487e-12],\n",
       "        [9.99999999e-01, 0.00000000e+00, 9.60133672e-21],\n",
       "        [9.99999168e-01, 0.00000000e+00, 8.31905504e-07],\n",
       "        [1.00000000e+00, 0.00000000e+00, 3.73880710e-13],\n",
       "        [9.99953114e-01, 0.00000000e+00, 4.68860895e-05],\n",
       "        [9.99999688e-01, 0.00000000e+00, 3.11807005e-07],\n",
       "        [9.99759812e-01, 0.00000000e+00, 2.40187385e-04],\n",
       "        [9.99997727e-01, 0.00000000e+00, 2.27245240e-06],\n",
       "        [1.00000000e+00, 0.00000000e+00, 4.27882176e-12],\n",
       "        [1.00000000e+00, 0.00000000e+00, 3.87969036e-14],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.58575210e-13],\n",
       "        [9.79667401e-01, 0.00000000e+00, 2.03325983e-02],\n",
       "        [9.99999994e-01, 0.00000000e+00, 5.71632997e-09],\n",
       "        [9.99316833e-01, 0.00000000e+00, 6.83166715e-04],\n",
       "        [9.84609214e-01, 0.00000000e+00, 1.53907854e-02],\n",
       "        [9.99998429e-01, 0.00000000e+00, 1.57039163e-06],\n",
       "        [9.99908932e-01, 0.00000000e+00, 9.10679281e-05],\n",
       "        [9.98122117e-01, 0.00000000e+00, 1.87788261e-03],\n",
       "        [6.42859195e-01, 0.00000000e+00, 3.57140804e-01],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.13313907e-11],\n",
       "        [1.00000000e+00, 0.00000000e+00, 4.11476907e-16],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.01050944e-14],\n",
       "        [1.00000000e+00, 0.00000000e+00, 3.01637682e-16],\n",
       "        [9.99997844e-01, 0.00000000e+00, 2.15538440e-06],\n",
       "        [9.94765829e-01, 0.00000000e+00, 5.23417062e-03],\n",
       "        [9.99999376e-01, 0.00000000e+00, 6.23619229e-07],\n",
       "        [9.99462955e-01, 0.00000000e+00, 5.37044554e-04],\n",
       "        [9.90122882e-01, 0.00000000e+00, 9.87711767e-03],\n",
       "        [9.99999856e-01, 0.00000000e+00, 1.43811011e-07],\n",
       "        [9.99999999e-01, 0.00000000e+00, 1.23952339e-09],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.27857758e-11],\n",
       "        [9.99999999e-01, 0.00000000e+00, 3.68227523e-10],\n",
       "        [9.99971383e-01, 0.00000000e+00, 2.86167396e-05],\n",
       "        [1.00000000e+00, 0.00000000e+00, 3.46813464e-11],\n",
       "        [9.99999999e-01, 0.00000000e+00, 3.60638434e-21],\n",
       "        [9.99999999e-01, 0.00000000e+00, 8.22995161e-10],\n",
       "        [9.99999999e-01, 0.00000000e+00, 1.33066096e-09],\n",
       "        [9.99999995e-01, 0.00000000e+00, 4.70758383e-09],\n",
       "        [9.99999573e-01, 0.00000000e+00, 4.26387522e-07],\n",
       "        [9.99999999e-01, 0.00000000e+00, 4.63123580e-21],\n",
       "        [9.99999999e-01, 0.00000000e+00, 9.34514638e-10],\n",
       "        [2.62995795e-04, 0.00000000e+00, 9.99737004e-01],\n",
       "        [9.35308888e-01, 0.00000000e+00, 6.46911118e-02],\n",
       "        [5.63363763e-04, 0.00000000e+00, 9.99436636e-01],\n",
       "        [1.11033129e-01, 0.00000000e+00, 8.88966870e-01],\n",
       "        [1.49769527e-03, 0.00000000e+00, 9.98502305e-01],\n",
       "        [1.24795411e-04, 0.00000000e+00, 9.99875204e-01],\n",
       "        [9.99999945e-01, 0.00000000e+00, 5.51447476e-08],\n",
       "        [5.52367398e-04, 0.00000000e+00, 9.99447633e-01],\n",
       "        [2.85820475e-02, 0.00000000e+00, 9.71417952e-01],\n",
       "        [1.20955822e-04, 0.00000000e+00, 9.99879043e-01],\n",
       "        [1.18815784e-02, 0.00000000e+00, 9.88118422e-01],\n",
       "        [2.40455010e-01, 0.00000000e+00, 7.59544990e-01],\n",
       "        [3.79339660e-03, 0.00000000e+00, 9.96206603e-01],\n",
       "        [9.66681576e-01, 0.00000000e+00, 3.33184232e-02],\n",
       "        [7.44042353e-02, 0.00000000e+00, 9.25595765e-01],\n",
       "        [4.66422365e-03, 0.00000000e+00, 9.95335776e-01],\n",
       "        [7.63510220e-02, 0.00000000e+00, 9.23648978e-01],\n",
       "        [1.60457415e-04, 0.00000000e+00, 9.99839531e-01],\n",
       "        [7.67229035e-04, 0.00000000e+00, 9.99232771e-01],\n",
       "        [9.99908144e-01, 0.00000000e+00, 9.18553554e-05],\n",
       "        [4.64094364e-04, 0.00000000e+00, 9.99535906e-01],\n",
       "        [9.72943892e-01, 0.00000000e+00, 2.70561075e-02],\n",
       "        [1.29737326e-04, 0.00000000e+00, 9.99870262e-01],\n",
       "        [9.47214109e-01, 0.00000000e+00, 5.27858904e-02],\n",
       "        [1.45667647e-03, 0.00000000e+00, 9.98543323e-01],\n",
       "        [1.04694016e-03, 0.00000000e+00, 9.98953060e-01],\n",
       "        [9.77244156e-01, 0.00000000e+00, 2.27558437e-02],\n",
       "        [9.45486437e-01, 0.00000000e+00, 5.45135630e-02],\n",
       "        [1.06916353e-02, 0.00000000e+00, 9.89308365e-01],\n",
       "        [9.80987770e-03, 0.00000000e+00, 9.90190122e-01],\n",
       "        [5.81722513e-04, 0.00000000e+00, 9.99418277e-01],\n",
       "        [1.20974102e-04, 0.00000000e+00, 9.99879025e-01],\n",
       "        [5.49597265e-03, 0.00000000e+00, 9.94504027e-01],\n",
       "        [9.90092182e-01, 0.00000000e+00, 9.90781763e-03],\n",
       "        [9.76343748e-01, 0.00000000e+00, 2.36562515e-02],\n",
       "        [1.32072579e-04, 0.00000000e+00, 9.99867927e-01],\n",
       "        [8.87641828e-04, 0.00000000e+00, 9.99112358e-01],\n",
       "        [8.43026091e-02, 0.00000000e+00, 9.15697391e-01],\n",
       "        [9.83074715e-01, 0.00000000e+00, 1.69252842e-02],\n",
       "        [3.63414600e-03, 0.00000000e+00, 9.96365854e-01],\n",
       "        [6.24056125e-04, 0.00000000e+00, 9.99375944e-01],\n",
       "        [1.21581934e-01, 0.00000000e+00, 8.78418066e-01],\n",
       "        [9.35308888e-01, 0.00000000e+00, 6.46911118e-02],\n",
       "        [3.43106607e-04, 0.00000000e+00, 9.99656893e-01],\n",
       "        [2.81613294e-04, 0.00000000e+00, 9.99718387e-01],\n",
       "        [4.54715021e-03, 0.00000000e+00, 9.95452850e-01],\n",
       "        [8.47733636e-01, 0.00000000e+00, 1.52266363e-01],\n",
       "        [6.96213898e-02, 0.00000000e+00, 9.30378610e-01],\n",
       "        [3.70784837e-03, 0.00000000e+00, 9.96292152e-01],\n",
       "        [9.14645299e-01, 0.00000000e+00, 8.53547002e-02]]),\n",
       " 'labels': array([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "        2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n",
       "        2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0]),\n",
       " 'mu': array([[ -8.66020087,   9.2806883 ],\n",
       "        [  0.96567879,  -0.40034494],\n",
       "        [-19.46192556,  20.2408064 ]]),\n",
       " 'var': array([[2.09681669e+01, 2.14931627e+01],\n",
       "        [1.32585447e-04, 1.41331396e-04],\n",
       "        [5.92593175e+00, 6.10976285e+00]])}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = result[\"labels\"]\n",
    "memberships = result[\"memberships\"]\n",
    "df[\"pred\"]=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index: 57.62031253929242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "ari = adjusted_rand_score(df[\"target\"], df[\"pred\"])\n",
    "print(\"Adjusted Rand Index:\", 100*ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu shape: (3, 2)\n",
      "var shape: (3, 2)\n",
      "M shape: (150, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "U = latent_np\n",
    "n_clusters = 3\n",
    "\n",
    "mu, var, M = initialize_clusters(U, n_clusters)\n",
    "\n",
    "print(\"mu shape:\", mu.shape)   # (4, 3)\n",
    "print(\"var shape:\", var.shape) # (4, 3)\n",
    "print(\"M shape:\", M.shape)     # (10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0261173,  1.4990529],\n",
       "       [-3.9042404,  1.3606722],\n",
       "       [ 1.9128268,  1.6425785]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1769068019794,
  "creator": "2018509",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env offus)",
   "language": "python",
   "name": "py-dku-venv-offus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "modifiedBy": "2018509",
  "tags": [],
  "versionNumber": 1
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
