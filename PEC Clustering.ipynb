{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(100)\n",
    "\n",
    "# Number of rows to have missing values\n",
    "n_rows = df.shape[0]\n",
    "n_missing = int(0* n_rows)\n",
    "\n",
    "# Randomly select row indices\n",
    "missing_rows = np.random.choice(df.index, size=n_missing, replace=False)\n",
    "\n",
    "# For each selected row, randomly set 1 or more columns (excluding 'target') to NaN\n",
    "for row in missing_rows:\n",
    "    n_cols_missing = np.random.randint(1, len(iris.feature_names) + 1)\n",
    "    cols_missing = np.random.choice(iris.feature_names, size=n_cols_missing, replace=False)\n",
    "    df.loc[row, cols_missing] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df\n",
    "X = np.asarray(X, dtype=float)\n",
    "n, s = X.shape\n",
    "mask = ~np.isnan(X)  # True if observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=_init_centers(X,3,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2 = _compute_partial_distances(X, V, mask)  # (n, c)\n",
    "Delta2 = d2.sum() / (3 * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.5592"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.500e-01, 2.156e+01, 2.870e+01],\n",
       "       [6.500e-01, 7.490e+00, 7.700e+00],\n",
       "       [1.530e+00, 2.242e+01, 3.098e+01],\n",
       "       [1.750e+00, 2.106e+01, 2.970e+01],\n",
       "       [6.300e-01, 2.190e+01, 2.908e+01],\n",
       "       [1.100e-01, 1.950e+01, 2.494e+01],\n",
       "       [1.460e+00, 2.187e+01, 3.015e+01],\n",
       "       [7.000e-01, 2.079e+01, 2.815e+01],\n",
       "       [2.600e+00, 2.205e+01, 3.155e+01],\n",
       "       [1.210e+00, 2.086e+01, 2.892e+01],\n",
       "       [1.500e-01, 2.102e+01, 2.710e+01],\n",
       "       [1.000e-02, 7.240e+00, 7.610e+00],\n",
       "       [1.580e+00, 2.163e+01, 3.019e+01],\n",
       "       [3.000e+00, 2.503e+01, 3.501e+01],\n",
       "       [3.100e-01, 2.394e+01, 2.916e+01],\n",
       "       [4.000e-01, 1.986e+01, 2.285e+01],\n",
       "       [2.700e-01, 2.230e+01, 2.830e+01],\n",
       "       [5.400e-01, 2.121e+01, 2.833e+01],\n",
       "       [0.000e+00, 1.947e+01, 2.449e+01],\n",
       "       [4.000e-01, 2.119e+01, 2.769e+01],\n",
       "       [9.000e-02, 4.090e+00, 5.690e+00],\n",
       "       [4.200e-01, 2.061e+01, 2.725e+01],\n",
       "       [1.750e+00, 2.566e+01, 3.420e+01],\n",
       "       [6.500e-01, 1.814e+01, 2.512e+01],\n",
       "       [1.020e+00, 1.847e+01, 2.567e+01],\n",
       "       [1.150e+00, 1.954e+01, 2.740e+01],\n",
       "       [1.600e-01, 4.810e+00, 4.010e+00],\n",
       "       [3.900e-01, 2.074e+01, 2.754e+01],\n",
       "       [1.600e-01, 4.810e+00, 4.010e+00],\n",
       "       [3.800e-01, 1.929e+01, 2.443e+01],\n",
       "       [1.320e+00, 1.997e+01, 2.807e+01],\n",
       "       [2.000e-01, 1.706e+01, 2.165e+01],\n",
       "       [4.000e-02, 1.625e+01, 2.164e+01],\n",
       "       [3.000e-01, 2.313e+01, 2.835e+01],\n",
       "       [1.180e+00, 2.049e+01, 2.853e+01],\n",
       "       [1.110e+00, 2.266e+01, 3.076e+01],\n",
       "       [3.000e-01, 2.197e+01, 2.845e+01],\n",
       "       [8.100e-01, 2.242e+01, 2.982e+01],\n",
       "       [2.500e+00, 2.287e+01, 3.235e+01],\n",
       "       [5.700e-01, 2.066e+01, 2.782e+01],\n",
       "       [2.500e-01, 2.158e+01, 2.664e+01],\n",
       "       [3.850e+00, 2.206e+01, 3.244e+01],\n",
       "       [2.220e+00, 2.311e+01, 3.227e+01],\n",
       "       [6.800e-01, 1.901e+01, 2.599e+01],\n",
       "       [4.100e-01, 1.822e+01, 2.414e+01],\n",
       "       [0.000e+00, 4.000e+00, 4.000e+00],\n",
       "       [3.800e-01, 2.085e+01, 2.723e+01],\n",
       "       [1.670e+00, 2.190e+01, 3.052e+01],\n",
       "       [2.200e-01, 2.109e+01, 2.737e+01],\n",
       "       [8.400e-01, 2.133e+01, 2.899e+01],\n",
       "       [1.326e+01, 3.630e+00, 2.590e+00],\n",
       "       [1.113e+01, 2.480e+00, 2.900e+00],\n",
       "       [1.461e+01, 3.060e+00, 2.080e+00],\n",
       "       [9.580e+00, 2.570e+00, 6.970e+00],\n",
       "       [1.249e+01, 2.140e+00, 2.860e+00],\n",
       "       [1.084e+01, 1.830e+00, 4.330e+00],\n",
       "       [1.230e+01, 2.250e+00, 2.410e+00],\n",
       "       [6.650e+00, 5.540e+00, 1.202e+01],\n",
       "       [1.203e+01, 2.620e+00, 3.020e+00],\n",
       "       [8.510e+00, 2.860e+00, 7.340e+00],\n",
       "       [8.460e+00, 4.990e+00, 1.163e+01],\n",
       "       [9.370e+00, 2.180e+00, 4.340e+00],\n",
       "       [6.380e+00, 2.090e+00, 4.380e+00],\n",
       "       [1.218e+01, 1.770e+00, 3.010e+00],\n",
       "       [6.430e+00, 3.620e+00, 7.420e+00],\n",
       "       [1.099e+01, 3.080e+00, 3.220e+00],\n",
       "       [1.093e+01, 1.760e+00, 4.100e+00],\n",
       "       [8.470e+00, 2.860e+00, 5.940e+00],\n",
       "       [1.309e+01, 1.840e+00, 4.260e+00],\n",
       "       [7.540e+00, 2.220e+00, 6.090e+00],\n",
       "       [1.326e+01, 1.610e+00, 2.550e+00],\n",
       "       [8.450e+00, 2.740e+00, 5.140e+00],\n",
       "       [1.473e+01, 1.620e+00, 2.800e+00],\n",
       "       [1.097e+01, 1.890e+00, 3.170e+00],\n",
       "       [1.006e+01, 2.630e+00, 3.850e+00],\n",
       "       [1.810e+00, 1.810e+00, 1.010e+00],\n",
       "       [1.403e+01, 2.700e+00, 2.560e+00],\n",
       "       [1.353e+01, 2.250e+00, 1.580e+00],\n",
       "       [1.118e+01, 1.750e+00, 3.450e+00],\n",
       "       [6.170e+00, 4.260e+00, 8.540e+00],\n",
       "       [8.050e+00, 3.300e+00, 7.860e+00],\n",
       "       [7.490e+00, 3.740e+00, 8.460e+00],\n",
       "       [7.870e+00, 2.900e+00, 6.220e+00],\n",
       "       [1.555e+01, 1.300e+00, 2.460e+00],\n",
       "       [1.101e+01, 1.840e+00, 4.580e+00],\n",
       "       [1.078e+01, 2.310e+00, 3.190e+00],\n",
       "       [1.293e+01, 2.700e+00, 2.400e+00],\n",
       "       [1.190e+01, 2.250e+00, 4.490e+00],\n",
       "       [8.410e+00, 2.560e+00, 5.500e+00],\n",
       "       [9.020e+00, 2.530e+00, 6.610e+00],\n",
       "       [1.058e+01, 2.050e+00, 5.430e+00],\n",
       "       [1.142e+01, 1.930e+00, 3.150e+00],\n",
       "       [8.550e+00, 2.660e+00, 6.000e+00],\n",
       "       [6.790e+00, 5.420e+00, 1.186e+01],\n",
       "       [9.470e+00, 2.180e+00, 5.460e+00],\n",
       "       [8.700e+00, 2.530e+00, 5.150e+00],\n",
       "       [9.060e+00, 2.290e+00, 5.050e+00],\n",
       "       [1.000e+00, 1.000e+00, 1.000e+00],\n",
       "       [5.380e+00, 6.170e+00, 1.249e+01],\n",
       "       [8.760e+00, 2.390e+00, 5.450e+00],\n",
       "       [2.794e+01, 2.250e+00, 4.100e-01],\n",
       "       [1.934e+01, 7.000e-02, 1.570e+00],\n",
       "       [2.748e+01, 3.030e+00, 2.900e-01],\n",
       "       [2.263e+01, 9.200e-01, 4.200e-01],\n",
       "       [2.570e+01, 1.570e+00, 1.500e-01],\n",
       "       [3.550e+01, 6.430e+00, 1.710e+00],\n",
       "       [1.613e+01, 9.800e-01, 5.480e+00],\n",
       "       [3.078e+01, 4.450e+00, 9.700e-01],\n",
       "       [2.575e+01, 1.680e+00, 7.400e-01],\n",
       "       [3.049e+01, 4.920e+00, 6.600e-01],\n",
       "       [1.945e+01, 1.140e+00, 4.200e-01],\n",
       "       [2.122e+01, 6.300e-01, 6.500e-01],\n",
       "       [1.965e+01, 1.460e+00, 5.000e-02],\n",
       "       [1.947e+01, 0.000e+00, 2.140e+00],\n",
       "       [2.098e+01, 2.700e-01, 1.510e+00],\n",
       "       [4.000e+00, 0.000e+00, 0.000e+00],\n",
       "       [6.250e+00, 4.000e-02, 9.000e-02],\n",
       "       [3.661e+01, 8.620e+00, 2.260e+00],\n",
       "       [4.000e+00, 0.000e+00, 0.000e+00],\n",
       "       [1.898e+01, 4.300e-01, 2.550e+00],\n",
       "       [2.436e+01, 1.070e+00, 5.000e-02],\n",
       "       [1.814e+01, 1.100e-01, 2.110e+00],\n",
       "       [4.000e+00, 0.000e+00, 0.000e+00],\n",
       "       [1.460e+01, 3.700e-01, 8.000e-01],\n",
       "       [2.449e+01, 2.140e+00, 0.000e+00],\n",
       "       [6.250e+00, 2.250e+00, 2.500e-01],\n",
       "       [1.711e+01, 4.200e-01, 1.400e+00],\n",
       "       [1.729e+01, 4.600e-01, 1.180e+00],\n",
       "       [1.970e+01, 8.500e-01, 1.000e-01],\n",
       "       [2.539e+01, 3.300e+00, 6.000e-01],\n",
       "       [2.981e+01, 4.200e+00, 9.400e-01],\n",
       "       [3.382e+01, 8.490e+00, 2.190e+00],\n",
       "       [2.431e+01, 9.800e-01, 3.600e-01],\n",
       "       [1.836e+01, 7.100e-01, 1.130e+00],\n",
       "       [2.202e+01, 8.900e-01, 1.350e+00],\n",
       "       [8.000e+00, 9.000e-02, 4.000e-02],\n",
       "       [2.414e+01, 1.690e+00, 2.700e-01],\n",
       "       [2.167e+01, 1.140e+00, 2.600e-01],\n",
       "       [1.659e+01, 4.200e-01, 1.480e+00],\n",
       "       [2.286e+01, 1.970e+00, 1.700e-01],\n",
       "       [2.511e+01, 1.880e+00, 1.400e-01],\n",
       "       [2.149e+01, 1.900e+00, 4.800e-01],\n",
       "       [1.934e+01, 7.000e-02, 1.570e+00],\n",
       "       [2.721e+01, 2.600e+00, 1.000e-01],\n",
       "       [2.609e+01, 2.380e+00, 1.600e-01],\n",
       "       [2.089e+01, 3.800e-01, 3.800e-01],\n",
       "       [6.560e+00, 1.000e-02, 4.000e-02],\n",
       "       [2.042e+01, 9.300e-01, 3.900e-01],\n",
       "       [2.210e+01, 1.310e+00, 3.900e-01],\n",
       "       [1.560e+01, 5.000e-02, 1.000e+00]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memberships (10)\n",
    "M, m_empty = _update_memberships(d2=D2, beta=2, delta2=Delta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90726799, 0.02314459, 0.01738667])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05220074850292656"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_empty[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_centers(X, c, random_state=None):\n",
    "    \"\"\"\n",
    "    Initialize cluster centers by filling missing values with column means\n",
    "    and picking random rows as initial centers.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n, s = X.shape\n",
    "    mask = ~np.isnan(X)\n",
    "    col_means = np.where(mask, X, np.nan).mean(axis=0)\n",
    "    X_filled = np.where(mask, X, col_means)\n",
    "    idx = rng.choice(n, size=c, replace=False)\n",
    "    return X_filled[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_partial_distances(X, V, mask):\n",
    "    \"\"\"\n",
    "    Compute squared partial distances d_ij^2 between each object and each center:\n",
    "      d_ij^2 = sum_p lambda_ip (x_ip - v_jp)^2\n",
    "    where lambda_ip = 1 if observed, 0 if missing.\n",
    "    \"\"\"\n",
    "    # X_zero has 0 for missing values to avoid nan propagation\n",
    "    X_zero = np.where(mask, X, 0.0)\n",
    "    # shape: (n, c, s)\n",
    "    diff = X_zero[:, None, :] - V[None, :, :]\n",
    "    # mask3: (n, 1, s) -> broadcast to (n, c, s)\n",
    "    mask3 = mask[:, None, :]\n",
    "    diff_masked = diff * mask3\n",
    "    d2 = (diff_masked ** 2).sum(axis=2)  # (n, c)\n",
    "    return d2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_memberships(d2, beta, delta2):\n",
    "    \"\"\"\n",
    "    Update membership masses m_ij and m_i_empty (for outliers) given distances d2.\n",
    "    Uses formulas (10) in the paper with numeric stabilisation.\n",
    "    \"\"\"\n",
    "    power = -1.0 / (beta - 1.0)\n",
    "    # avoid division by zero\n",
    "    d2_safe = d2 + 1e-12\n",
    "    tmp = d2_safe ** power  # shape (n, c)\n",
    "    delta_term = (delta2 + 1e-12) ** power\n",
    "    denom = tmp.sum(axis=1, keepdims=True) + delta_term  # shape (n, 1)\n",
    "    m = tmp / denom  # m_ij\n",
    "    m_empty = delta_term / denom.squeeze()  # m_i∅\n",
    "    return m, m_empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_centers(X, mask, M, beta):\n",
    "    \"\"\"\n",
    "    Update cluster centers via formula (11).\n",
    "    \"\"\"\n",
    "    n, s = X.shape\n",
    "    c = M.shape[1]\n",
    "    V = np.zeros((c, s), dtype=float)\n",
    "    m_beta = M ** beta  # (n, c)\n",
    "\n",
    "    for j in range(c):\n",
    "        # weights for cluster j: shape (n, 1)\n",
    "        w = m_beta[:, j:j+1] * mask  # (n, s)\n",
    "        num = (w * X).sum(axis=0)    # (s,)\n",
    "        den = w.sum(axis=0)          # (s,)\n",
    "        # avoid division by zero: if den[p] == 0, keep center dim as 0\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            v_j = np.where(den > 0, num / den, 0.0)\n",
    "        V[j] = v_j\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  Step 1: Preliminary partial-distance evidential clustering\n",
    "# ============================================================\n",
    "\n",
    "def preliminary_pec(X, c, beta=2.0, eps=1e-4, max_iter=200, random_state=None):\n",
    "    \"\"\"\n",
    "    Step 1: Perform evidential clustering with partial distances (no imputation).\n",
    "    Returns:\n",
    "      V          : final cluster centers (c, s)\n",
    "      M          : mass matrix for singleton clusters (n, c)\n",
    "      m_empty    : mass vector for outlier (n,)\n",
    "      info       : dict with Lambda sets, completeness, and preliminary labels\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n, s = X.shape\n",
    "    mask = ~np.isnan(X)  # True if observed\n",
    "\n",
    "    # 1) Initialize centers\n",
    "    V = _init_centers(X, c, random_state=random_state)\n",
    "\n",
    "    # 2) Iterative optimization of objective (5)\n",
    "    for _ in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "        # distances and delta^2 (7)\n",
    "        d2 = _compute_partial_distances(X, V, mask)  # (n, c)\n",
    "        delta2 = d2.sum() / (c * n)\n",
    "        # memberships (10)\n",
    "        M, m_empty = _update_memberships(d2, beta, delta2)\n",
    "        # centers (11)\n",
    "        print(V_prev)\n",
    "        V = _update_centers(X, mask, M, beta)\n",
    "        # check convergence\n",
    "        if np.linalg.norm(V - V_prev) < eps:\n",
    "            break\n",
    "\n",
    "    # 3) Compute threshold phi (14)\n",
    "    m_bar = M.mean(axis=1, keepdims=True)   # (n,1)\n",
    "    phi = ((M - m_bar) ** 2).mean()         # scalar\n",
    "\n",
    "    # 4) Compute Lambda_i sets and preliminary labels\n",
    "    Lambda = []\n",
    "    is_complete = mask.all(axis=1)  # (n,)\n",
    "\n",
    "    prelim_type = np.empty(n, dtype=object)\n",
    "    prelim_cluster = np.full(n, -1, dtype=int)  # singleton cluster index or -1\n",
    "    # 'noise' = pure outlier; 'singleton' = definite cluster; 'meta' = meta-cluster; 'uncertain_incomplete' = go to Step 2\n",
    "\n",
    "    for i in range(n):\n",
    "        # extended masses: index 0 = empty, 1..c = clusters 0..c-1\n",
    "        masses = np.concatenate(([m_empty[i]], M[i]))  # shape (c+1,)\n",
    "        max_idx = np.argmax(masses)\n",
    "        max_val = masses[max_idx]\n",
    "\n",
    "        Lambda_i = set()\n",
    "        for j in range(c + 1):\n",
    "            if abs(max_val - masses[j]) < phi:\n",
    "                if j == 0:\n",
    "                    Lambda_i.add('empty')\n",
    "                else:\n",
    "                    Lambda_i.add(j - 1)\n",
    "        Lambda.append(Lambda_i)\n",
    "\n",
    "        # classify preliminarily\n",
    "        if len(Lambda_i) == 0:\n",
    "            prelim_type[i] = 'noise'\n",
    "            prelim_cluster[i] = -1\n",
    "        elif len(Lambda_i) == 1 and 'empty' in Lambda_i:\n",
    "            prelim_type[i] = 'noise'\n",
    "            prelim_cluster[i] = -1\n",
    "        elif len(Lambda_i) == 1 and 'empty' not in Lambda_i:\n",
    "            # definite singleton cluster\n",
    "            prelim_type[i] = 'singleton'\n",
    "            (g,) = Lambda_i\n",
    "            prelim_cluster[i] = g\n",
    "        else:\n",
    "            # ambiguous between >=2 elements\n",
    "            if is_complete[i]:\n",
    "                prelim_type[i] = 'meta'\n",
    "                prelim_cluster[i] = -1\n",
    "            else:\n",
    "                prelim_type[i] = 'uncertain_incomplete'\n",
    "                prelim_cluster[i] = -1\n",
    "\n",
    "    info = {\n",
    "        \"mask\": mask,\n",
    "        \"Lambda\": Lambda,\n",
    "        \"is_complete\": is_complete,\n",
    "        \"prelim_type\": prelim_type,\n",
    "        \"prelim_cluster\": prelim_cluster,\n",
    "        \"delta2\": delta2\n",
    "    }\n",
    "    return V, M, m_empty, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  Step 2: Multiple imputation + DST redistribution\n",
    "# ============================================================\n",
    "\n",
    "def _single_membership_from_distances(d2_row, beta, delta2):\n",
    "    \"\"\"\n",
    "    Compute membership (m_j, m_empty) for a single object given distances d2_row (shape (c,)).\n",
    "    \"\"\"\n",
    "    power = -1.0 / (beta - 1.0)\n",
    "    d2_safe = d2_row + 1e-12\n",
    "    tmp = d2_safe ** power\n",
    "    delta_term = (delta2 + 1e-12) ** power\n",
    "    denom = tmp.sum() + delta_term\n",
    "    m = tmp / denom\n",
    "    m_empty = delta_term / denom\n",
    "    return m, m_empty\n",
    "\n",
    "\n",
    "def _softmax_negative(distances):\n",
    "    \"\"\"\n",
    "    Turn distances (list/array) into reliability factors via softmax(-d).\n",
    "    \"\"\"\n",
    "    d = np.asarray(distances, dtype=float)\n",
    "    m = d.min()\n",
    "    exp_vals = np.exp(-(d - m))  # stabilise\n",
    "    s = exp_vals.sum()\n",
    "    if s == 0:\n",
    "        return np.ones_like(distances) / len(distances)\n",
    "    return exp_vals / s\n",
    "\n",
    "\n",
    "def _build_bba_from_membership(m_clusters, m_empty, c):\n",
    "    \"\"\"\n",
    "    Build a basic belief assignment (BBA) dict from membership vector and outlier mass.\n",
    "    Keys:\n",
    "      'empty' -> m_empty\n",
    "      0..c-1  -> m_clusters[j]\n",
    "    \"\"\"\n",
    "    bba = {'empty': float(m_empty)}\n",
    "    for j in range(c):\n",
    "        bba[j] = float(m_clusters[j])\n",
    "    return bba\n",
    "\n",
    "\n",
    "def _discount_bba_with_meta(bba, alpha, cluster_candidates, c):\n",
    "    \"\"\"\n",
    "    Apply reliability discounting as in (20)-(21), placing uncertainty\n",
    "    on the meta-cluster corresponding to cluster_candidates.\n",
    "    Returns a new BBA with keys: 'empty', 0..c-1, 'meta'.\n",
    "    \"\"\"\n",
    "    # original masses\n",
    "    m_empty = bba.get('empty', 0.0)\n",
    "    m_clusters = np.array([bba.get(j, 0.0) for j in range(c)], dtype=float)\n",
    "\n",
    "    m_tilde = {}\n",
    "    m_tilde['empty'] = m_empty\n",
    "    # discounted singletons\n",
    "    m_tilde_clusters = alpha * m_clusters\n",
    "    for j in range(c):\n",
    "        m_tilde[j] = float(m_tilde_clusters[j])\n",
    "\n",
    "    # meta mass\n",
    "    meta_mass = 1.0 - m_empty - m_tilde_clusters.sum()\n",
    "    # clip for numerical stability\n",
    "    meta_mass = max(0.0, min(1.0, meta_mass))\n",
    "    m_tilde['meta'] = meta_mass\n",
    "    return m_tilde\n",
    "\n",
    "\n",
    "def _subset_from_key(key, cluster_candidates_set):\n",
    "    \"\"\"\n",
    "    Map a BBA key to a subset of Ω:\n",
    "      'empty' -> empty set\n",
    "      int j   -> {j}\n",
    "      'meta'  -> cluster_candidates_set\n",
    "    \"\"\"\n",
    "    if key == 'empty':\n",
    "        return frozenset()\n",
    "    elif key == 'meta':\n",
    "        return frozenset(cluster_candidates_set)\n",
    "    else:\n",
    "        return frozenset([key])\n",
    "\n",
    "\n",
    "def _key_from_subset(subset, cluster_candidates_set):\n",
    "    \"\"\"\n",
    "    Map subset back to a key. We only allow:\n",
    "      ∅       -> 'empty'\n",
    "      {j}     -> j\n",
    "      full Λ  -> 'meta'\n",
    "    \"\"\"\n",
    "    if len(subset) == 0:\n",
    "        return 'empty'\n",
    "    if subset == frozenset(cluster_candidates_set):\n",
    "        return 'meta'\n",
    "    if len(subset) == 1:\n",
    "        (j,) = tuple(subset)\n",
    "        return j\n",
    "    # In this implementation, we don't create new meta-clusters besides Λ;\n",
    "    # any other subset is ignored (it should not appear in our restricted setup).\n",
    "    return None\n",
    "\n",
    "\n",
    "def _fuse_two_bbas(bba1, bba2, cluster_candidates_set, c):\n",
    "    \"\"\"\n",
    "    Fuse two BBAs using the modified Dempster–Shafer rule (22)-(23).\n",
    "    Keys: 'empty', 0..c-1, 'meta'.\n",
    "    \"\"\"\n",
    "    keys = ['empty'] + list(range(c)) + ['meta']\n",
    "\n",
    "    # compute conflict K\n",
    "    K = 0.0\n",
    "    for B in keys:\n",
    "        for C in keys:\n",
    "            if B == 'empty' or C == 'empty':\n",
    "                continue\n",
    "            setB = _subset_from_key(B, cluster_candidates_set)\n",
    "            setC = _subset_from_key(C, cluster_candidates_set)\n",
    "            inter = setB & setC\n",
    "            if len(inter) == 0:\n",
    "                K += bba1.get(B, 0.0) * bba2.get(C, 0.0)\n",
    "\n",
    "    denom = 1.0 - K + 1e-12\n",
    "\n",
    "    # fuse non-empty subsets\n",
    "    num = {k: 0.0 for k in keys}\n",
    "    for B in keys:\n",
    "        for C in keys:\n",
    "            setB = _subset_from_key(B, cluster_candidates_set)\n",
    "            setC = _subset_from_key(C, cluster_candidates_set)\n",
    "            inter = setB & setC\n",
    "            keyA = _key_from_subset(inter, cluster_candidates_set)\n",
    "            if keyA is None:\n",
    "                continue\n",
    "            num[keyA] += bba1.get(B, 0.0) * bba2.get(C, 0.0)\n",
    "\n",
    "    fused = {k: 0.0 for k in keys}\n",
    "    # empty case special formula\n",
    "    fused['empty'] = (\n",
    "        bba1.get('empty', 0.0)\n",
    "        + bba2.get('empty', 0.0)\n",
    "        - bba1.get('empty', 0.0) * bba2.get('empty', 0.0)\n",
    "    ) / denom\n",
    "\n",
    "    for k in keys:\n",
    "        if k == 'empty':\n",
    "            continue\n",
    "        fused[k] = num[k] / denom\n",
    "\n",
    "    return fused\n",
    "\n",
    "\n",
    "def _fuse_bba_list(bba_list, cluster_candidates_set, c):\n",
    "    \"\"\"\n",
    "    Fuse a list of BBAs with the above fusion rule.\n",
    "    \"\"\"\n",
    "    if not bba_list:\n",
    "        return None\n",
    "    fused = bba_list[0]\n",
    "    for bba in bba_list[1:]:\n",
    "        fused = _fuse_two_bbas(fused, bba, cluster_candidates_set, c)\n",
    "    return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pec(X, c, beta=2.0, eps=1e-4, max_iter=200, random_state=None):\n",
    "    \"\"\"\n",
    "    Full PEC algorithm (Steps 1 and 2) as described in the paper:\n",
    "      - Partial-distance evidential clustering (no imputation)\n",
    "      - Multiple imputation for uncertain incomplete objects\n",
    "      - DST-based reliability discounting and evidence fusion\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Data matrix with np.nan for missing values.\n",
    "    c : int\n",
    "        Number of specific clusters.\n",
    "    beta : float, optional\n",
    "        Fuzzifier (usually 2).\n",
    "    eps : float, optional\n",
    "        Convergence tolerance for centers.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations in Step 1.\n",
    "    random_state : int or None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    final_assignments : list\n",
    "        For each object i: a tuple describing its decision:\n",
    "          ('noise',)\n",
    "          ('singleton', j)\n",
    "          ('meta', (j1, j2, ...))\n",
    "    final_bbas : list of dict\n",
    "        For each object, its final BBA over:\n",
    "          'empty', 0..c-1, optionally 'meta'\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n, s = X.shape\n",
    "\n",
    "    # Step 1: preliminary evidential clustering via partial distances\n",
    "    V, M, m_empty, info = preliminary_pec(\n",
    "        X, c, beta=beta, eps=eps, max_iter=max_iter, random_state=random_state\n",
    "    )\n",
    "    mask = info[\"mask\"]\n",
    "    Lambda = info[\"Lambda\"]\n",
    "    is_complete = info[\"is_complete\"]\n",
    "    prelim_type = info[\"prelim_type\"]\n",
    "    prelim_cluster = info[\"prelim_cluster\"]\n",
    "    delta2 = info[\"delta2\"]\n",
    "\n",
    "    # Build neighbor pools: complete objects firmly assigned to singleton clusters\n",
    "    cluster_neighbors = {g: [] for g in range(c)}\n",
    "    for i in range(n):\n",
    "        if is_complete[i] and prelim_type[i] == 'singleton':\n",
    "            g = prelim_cluster[i]\n",
    "            cluster_neighbors[g].append(i)\n",
    "\n",
    "    # Step 2: handle uncertain incomplete objects via multiple imputation + DST\n",
    "    final_bbas = [None] * n\n",
    "    final_assignments = [None] * n\n",
    "\n",
    "    # Precompute once: base BBAs from Step 1 for all objects\n",
    "    base_bbas = []\n",
    "    for i in range(n):\n",
    "        bba = _build_bba_from_membership(M[i], m_empty[i], c)\n",
    "        base_bbas.append(bba)\n",
    "\n",
    "    for i in range(n):\n",
    "        # Case 1: uncertain incomplete -> full PEC redistribution\n",
    "        if prelim_type[i] == 'uncertain_incomplete':\n",
    "            # candidate clusters (ignore 'empty')\n",
    "            cluster_candidates = sorted([j for j in Lambda[i] if j != 'empty'])\n",
    "            if len(cluster_candidates) < 2:\n",
    "                # not enough info -> fall back to base BBA\n",
    "                bba_final = base_bbas[i]\n",
    "                final_bbas[i] = bba_final\n",
    "            else:\n",
    "                # multiple imputation: one version per candidate cluster\n",
    "                Xi = X[i].copy()\n",
    "                Xi_mask = mask[i]\n",
    "                Xi_zero = np.where(Xi_mask, Xi, 0.0)\n",
    "\n",
    "                bba_versions = []\n",
    "                reliabilities_dist = []\n",
    "\n",
    "                for g in cluster_candidates:\n",
    "                    neigh_idx = cluster_neighbors[g]\n",
    "                    if len(neigh_idx) == 0:\n",
    "                        continue  # cannot impute from this cluster\n",
    "\n",
    "                    neighbors = X[neigh_idx]  # (K, s)\n",
    "                    neigh_mask = mask[neigh_idx]\n",
    "                    # distances for weights (15)\n",
    "                    # use Xi's observed dims only\n",
    "                    diffs = (neighbors - Xi_zero) * Xi_mask  # broadcast Xi_mask (s,)\n",
    "                    dists = np.sqrt((diffs ** 2).sum(axis=1))  # (K,)\n",
    "                    # if Xi has no observed dims, give equal weights\n",
    "                    if Xi_mask.sum() == 0:\n",
    "                        theta = np.ones(len(neigh_idx)) / len(neigh_idx)\n",
    "                    else:\n",
    "                        # weights (16)\n",
    "                        exp_vals = np.exp(-dists)\n",
    "                        s_exp = exp_vals.sum()\n",
    "                        if s_exp == 0:\n",
    "                            theta = np.ones(len(neigh_idx)) / len(neigh_idx)\n",
    "                        else:\n",
    "                            theta = exp_vals / s_exp\n",
    "\n",
    "                    # impute missing attributes using (17)\n",
    "                    Xi_imputed = Xi.copy()\n",
    "                    missing_dims = ~Xi_mask\n",
    "                    if missing_dims.any():\n",
    "                        Xi_imputed[missing_dims] = (theta[:, None] * neighbors[:, missing_dims]).sum(axis=0)\n",
    "\n",
    "                    # compute membership for imputed version using centers V (one step of (10))\n",
    "                    Xi_imputed = Xi_imputed[None, :]  # shape (1, s)\n",
    "                    Xi_imputed_mask = np.ones_like(Xi_imputed, dtype=bool)\n",
    "                    d2_row = _compute_partial_distances(Xi_imputed, V, Xi_imputed_mask)[0]\n",
    "                    m_clusters_i, m_empty_i = _single_membership_from_distances(d2_row, beta, delta2)\n",
    "\n",
    "                    # build BBA for this version\n",
    "                    bba_i_g = _build_bba_from_membership(m_clusters_i, m_empty_i, c)\n",
    "\n",
    "                    # compute reliability distance (18)-(19) using neighbors' BBAs\n",
    "                    # here we measure in the space of {empty} + singletons\n",
    "                    vec_i = np.array(\n",
    "                        [bba_i_g['empty']] + [bba_i_g[j] for j in range(c)],\n",
    "                        dtype=float\n",
    "                    )\n",
    "                    dist_sum = 0.0\n",
    "                    for k_idx in neigh_idx:\n",
    "                        bba_k = base_bbas[k_idx]\n",
    "                        vec_k = np.array(\n",
    "                            [bba_k['empty']] + [bba_k[j] for j in range(c)],\n",
    "                            dtype=float\n",
    "                        )\n",
    "                        dist_sum += np.linalg.norm(vec_i - vec_k)\n",
    "                    bba_versions.append(bba_i_g)\n",
    "                    reliabilities_dist.append(dist_sum)\n",
    "\n",
    "                if len(bba_versions) == 0:\n",
    "                    # could not build any version -> fall back\n",
    "                    bba_final = base_bbas[i]\n",
    "                    final_bbas[i] = bba_final\n",
    "                else:\n",
    "                    # compute reliability factors alpha via softmax(-distance)\n",
    "                    alphas = _softmax_negative(reliabilities_dist)\n",
    "                    # discount each BBA and put ignorance on meta-cluster Λ̂_i\n",
    "                    cluster_candidates_set = set(cluster_candidates)\n",
    "                    discounted_list = []\n",
    "                    for bba_v, alpha in zip(bba_versions, alphas):\n",
    "                        discounted = _discount_bba_with_meta(\n",
    "                            bba_v, alpha, cluster_candidates, c\n",
    "                        )\n",
    "                        discounted_list.append(discounted)\n",
    "\n",
    "                    # fuse them using modified DS rule\n",
    "                    bba_final = _fuse_bba_list(\n",
    "                        discounted_list, cluster_candidates_set, c\n",
    "                    )\n",
    "                    final_bbas[i] = bba_final\n",
    "\n",
    "        # Case 2: all other points -> use simple evidential interpretation of Step 1\n",
    "        else:\n",
    "            bba_final = base_bbas[i].copy()\n",
    "            # If complete and ambiguous (meta), push mass of ambiguous clusters to 'meta'\n",
    "            if prelim_type[i] == 'meta':\n",
    "                cluster_candidates = sorted([j for j in Lambda[i] if j != 'empty'])\n",
    "                cluster_candidates_set = set(cluster_candidates)\n",
    "                if cluster_candidates:\n",
    "                    # collect mass for meta from those candidates\n",
    "                    meta_mass = sum(bba_final.get(j, 0.0) for j in cluster_candidates)\n",
    "                    for j in cluster_candidates:\n",
    "                        bba_final[j] = 0.0\n",
    "                    bba_final['meta'] = bba_final.get('meta', 0.0) + meta_mass\n",
    "            final_bbas[i] = bba_final\n",
    "\n",
    "        # now derive final assignment from final_bbas[i]\n",
    "        bba_i = final_bbas[i]\n",
    "        # ensure keys exist\n",
    "        mass_empty = bba_i.get('empty', 0.0)\n",
    "        masses_clusters = np.array([bba_i.get(j, 0.0) for j in range(c)])\n",
    "        mass_meta = bba_i.get('meta', 0.0)\n",
    "\n",
    "        # choose argmax over {empty, singletons, meta}\n",
    "        elems = ['empty'] + list(range(c)) + ['meta']\n",
    "        vals = [mass_empty] + list(masses_clusters) + [mass_meta]\n",
    "        k_max = elems[int(np.argmax(vals))]\n",
    "\n",
    "        if k_max == 'empty':\n",
    "            final_assignments[i] = ('noise',)\n",
    "        elif k_max == 'meta':\n",
    "            # for meta we can retrieve candidates from Lambda[i] (excluding 'empty')\n",
    "            cluster_candidates = sorted([j for j in Lambda[i] if j != 'empty'])\n",
    "            final_assignments[i] = ('meta', tuple(cluster_candidates))\n",
    "        else:\n",
    "            final_assignments[i] = ('singleton', int(k_max))\n",
    "\n",
    "    return final_assignments, final_bbas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.7 2.5 5.8 1.8]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [4.6 3.1 1.5 0.2]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'fillna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# X: your data with np.nan as missing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# c: number of clusters\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mpreliminary_pec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[110], line 31\u001b[0m, in \u001b[0;36mpreliminary_pec\u001b[0;34m(X, c, beta, eps, max_iter, random_state)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# centers (11)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(V_prev)\n\u001b[0;32m---> 31\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[43m_update_centers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# check convergence\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(V \u001b[38;5;241m-\u001b[39m V_prev) \u001b[38;5;241m<\u001b[39m eps:\n",
      "Cell \u001b[0;32mIn[112], line 13\u001b[0m, in \u001b[0;36m_update_centers\u001b[0;34m(X, mask, M, beta)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(c):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# weights for cluster j: shape (n, 1)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     w \u001b[38;5;241m=\u001b[39m m_beta[:, j:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m mask  \u001b[38;5;66;03m# (n, s)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     num \u001b[38;5;241m=\u001b[39m (w \u001b[38;5;241m*\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)    \u001b[38;5;66;03m# (s,)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     den \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)          \u001b[38;5;66;03m# (s,)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# avoid division by zero: if den[p] == 0, keep center dim as 0\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'fillna'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X: your data with np.nan as missing\n",
    "# c: number of clusters\n",
    "\n",
    "preliminary_pec(df.drop(\"target\",axis=1), c=3,eps=1e-4, max_iter=10, beta=2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan},\n",
       " {'empty': nan, 0: nan, 1: nan, 2: nan}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"assignments\"]=assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('singleton', 1), ('meta', (0, 2)), ('singleton', 0),\n",
       "       ('singleton', 2), ('meta', (0, 1))], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"assignments\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(tuple_1):\n",
    "    if tuple_1[0]==\"singleton\":\n",
    "        return tuple_1[0]\n",
    "    elif tuple_1[0]==\"meta\":\n",
    "        return tuple_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"type\"]=df[\"assignments\"].apply(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clusters\"]=df[\"assignments\"].apply(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>assignments</th>\n",
       "      <th>clusters</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>(singleton, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>singleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>(singleton, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>singleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>(singleton, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>singleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>(singleton, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>singleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>(singleton, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>singleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>(singleton, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>singleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "      <td>(singleton, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>singleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>(singleton, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>singleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>(singleton, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>singleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>(singleton, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>singleton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "143                6.8               3.2                5.9               2.3   \n",
       "144                6.7               3.3                5.7               2.5   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "\n",
       "     target     assignments clusters       type  \n",
       "0         0  (singleton, 1)        1  singleton  \n",
       "1         0  (singleton, 1)        1  singleton  \n",
       "2         0  (singleton, 1)        1  singleton  \n",
       "3         0  (singleton, 1)        1  singleton  \n",
       "4         0  (singleton, 1)        1  singleton  \n",
       "..      ...             ...      ...        ...  \n",
       "143       2  (singleton, 2)        2  singleton  \n",
       "144       2  (singleton, 2)        2  singleton  \n",
       "145       2  (singleton, 2)        2  singleton  \n",
       "147       2  (singleton, 2)        2  singleton  \n",
       "148       2  (singleton, 2)        2  singleton  \n",
       "\n",
       "[142 rows x 8 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"type\"]==\"singleton\"][\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7994679577381433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Example cluster labels\n",
    "labels_true = df[df[\"type\"]==\"singleton\"][\"target\"]\n",
    "labels_pred = df[df[\"type\"]==\"singleton\"][\"clusters\"]\n",
    "\n",
    "# Compute Adjusted Rand Index\n",
    "ari = adjusted_rand_score(labels_true, labels_pred)\n",
    "print(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdOn": 1764482555320,
  "creator": "2018509",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env offus)",
   "language": "python",
   "name": "py-dku-venv-offus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "modifiedBy": "2018509",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
