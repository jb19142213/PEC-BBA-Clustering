{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/dataiku/DATA_DESIGNER/code-envs/python/offus/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Masked Variational Autoencoder\n",
    "# =========================\n",
    "\n",
    "class MaskedVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Variational Autoencoder for incomplete data.\n",
    "    Encoder input: [x âŠ™ m , m]\n",
    "    Latent output: mu (used as latent feature u_i)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: input_dim * 2 because of concatenation with mask\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, mask):\n",
    "        x_masked = x * mask\n",
    "        enc_input = torch.cat([x_masked, mask], dim=1)\n",
    "        h = self.encoder(enc_input)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mu, logvar = self.encode(x, mask)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Masked VAE Loss\n",
    "# =========================\n",
    "\n",
    "def masked_vae_loss(\n",
    "    recon_x,\n",
    "    x,\n",
    "    mask,\n",
    "    mu,\n",
    "    logvar,\n",
    "    recon_weight=1.0,\n",
    "    kl_weight=0.0,\n",
    "    eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Masked reconstruction + KL divergence loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Masked reconstruction loss (MSE over observed entries only)\n",
    "    se = (recon_x - x) ** 2\n",
    "    masked_se = se * mask\n",
    "    recon_loss = masked_se.sum(dim=1) / (mask.sum(dim=1) + eps)\n",
    "    recon_loss = recon_loss.mean()\n",
    "\n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * torch.mean(\n",
    "        torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    )\n",
    "\n",
    "    total_loss = recon_weight * recon_loss + kl_weight * kl_loss\n",
    "    return total_loss, recon_loss.item(), kl_loss.item()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Training Function\n",
    "# =========================\n",
    "\n",
    "def train_masked_vae(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    epochs=50,\n",
    "    lr=1e-3\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        total_rec = 0.0\n",
    "        total_kl = 0.0\n",
    "\n",
    "        for x, mask in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            mask = mask.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x, mask)\n",
    "            loss, rec, kl = masked_vae_loss(recon, x, mask, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_rec += rec\n",
    "            total_kl += kl\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Loss: {total_loss:.4f} | \"\n",
    "            f\"Recon: {total_rec:.4f} | \"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Latent Feature Extraction\n",
    "# =========================\n",
    "\n",
    "def extract_latent_features(model, X, mask, device):\n",
    "    \"\"\"\n",
    "    Returns latent feature vectors u_i = mu_i\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = X.to(device).float()\n",
    "        mask = mask.to(device).float()\n",
    "        mu, _ = model.encode(X, mask)\n",
    "    return mu.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Load Iris Dataset\n",
    "# ============================================\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df[\"target\"] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Inject Missing Values (MCAR)\n",
    "# ============================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = df.shape[0]\n",
    "missing_ratio = 0\n",
    "n_missing_rows = int(missing_ratio * n_samples)\n",
    "\n",
    "missing_rows = np.random.choice(df.index, size=n_missing_rows, replace=False)\n",
    "\n",
    "for row in missing_rows:\n",
    "    n_cols_missing = np.random.randint(1, len(iris.feature_names))\n",
    "    cols_missing = np.random.choice(\n",
    "        iris.feature_names, size=n_cols_missing, replace=False\n",
    "    )\n",
    "    df.loc[row, cols_missing] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Create Feature Matrix and Mask\n",
    "# ============================================\n",
    "\n",
    "X = df[iris.feature_names].values.astype(np.float32)\n",
    "\n",
    "# Binary mask: 1 = observed, 0 = missing\n",
    "mask = (~np.isnan(X)).astype(np.float32)\n",
    "\n",
    "# Fill missing values with zero (mask-aware)\n",
    "X_filled = np.nan_to_num(X, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. Mask-aware Standardization\n",
    "# ============================================\n",
    "class TorchScaler:\n",
    "    def fit(self, X: torch.Tensor):\n",
    "        self.mean = X.mean(dim=0, keepdim=True)\n",
    "        self.std = X.std(dim=0, unbiased=False, keepdim=True)\n",
    "        self.std[self.std == 0] = 1.0\n",
    "        return self\n",
    "    def transform(self, X: torch.Tensor):\n",
    "        return (X - self.mean) / self.std\n",
    "    def inverse_transform(self, X: torch.Tensor):\n",
    "        return X * self.std + self.mean\n",
    "\n",
    "X_scaled = X_filled.copy()\n",
    "X_tensor = torch.from_numpy(np.array(X_scaled))\n",
    "scaler = TorchScaler().fit(X_tensor)\n",
    "X_scaled = scaler.transform(X_tensor)\n",
    "mask_tensor = torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_933443/2934702538.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 5. PyTorch Dataset and DataLoader\n",
    "# ============================================\n",
    "    \n",
    "class MaskedDataset:\n",
    "    def __init__(self, X, mask):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.mask[idx]\n",
    "\n",
    "\n",
    "dataset = MaskedDataset(X_tensor, mask)\n",
    "torch.manual_seed(42)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAssumes your DataLoader yields (x, mask)\\nx    : tensor of shape [batch_size, input_dim]\\nmask : tensor of shape [batch_size, input_dim], binary {0,1}\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Example Usage (Skeleton)\n",
    "# =========================\n",
    "\"\"\"\n",
    "Assumes your DataLoader yields (x, mask)\n",
    "x    : tensor of shape [batch_size, input_dim]\n",
    "mask : tensor of shape [batch_size, input_dim], binary {0,1}\n",
    "\"\"\"\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = MaskedVAE(input_dim=INPUT_DIM, latent_dim=LATENT_DIM)\n",
    "# model = train_masked_vae(model, dataloader, device)\n",
    "# U = extract_latent_features(model, X_full, mask_full, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 85.6686 | Recon: 85.6686 | \n",
      "Epoch 002 | Loss: 84.4715 | Recon: 84.4715 | \n",
      "Epoch 003 | Loss: 84.4547 | Recon: 84.4547 | \n",
      "Epoch 004 | Loss: 84.4784 | Recon: 84.4784 | \n",
      "Epoch 005 | Loss: 83.7439 | Recon: 83.7439 | \n",
      "Epoch 006 | Loss: 83.1238 | Recon: 83.1238 | \n",
      "Epoch 007 | Loss: 82.7328 | Recon: 82.7328 | \n",
      "Epoch 008 | Loss: 81.6244 | Recon: 81.6244 | \n",
      "Epoch 009 | Loss: 80.0135 | Recon: 80.0135 | \n",
      "Epoch 010 | Loss: 80.4837 | Recon: 80.4837 | \n",
      "Epoch 011 | Loss: 79.9134 | Recon: 79.9134 | \n",
      "Epoch 012 | Loss: 78.0758 | Recon: 78.0758 | \n",
      "Epoch 013 | Loss: 77.2927 | Recon: 77.2927 | \n",
      "Epoch 014 | Loss: 76.7037 | Recon: 76.7037 | \n",
      "Epoch 015 | Loss: 74.6539 | Recon: 74.6539 | \n",
      "Epoch 016 | Loss: 73.2527 | Recon: 73.2527 | \n",
      "Epoch 017 | Loss: 71.9707 | Recon: 71.9707 | \n",
      "Epoch 018 | Loss: 69.0535 | Recon: 69.0535 | \n",
      "Epoch 019 | Loss: 66.7725 | Recon: 66.7725 | \n",
      "Epoch 020 | Loss: 64.6864 | Recon: 64.6864 | \n",
      "Epoch 021 | Loss: 60.1799 | Recon: 60.1799 | \n",
      "Epoch 022 | Loss: 55.8924 | Recon: 55.8924 | \n",
      "Epoch 023 | Loss: 51.7071 | Recon: 51.7071 | \n",
      "Epoch 024 | Loss: 45.2451 | Recon: 45.2451 | \n",
      "Epoch 025 | Loss: 40.9480 | Recon: 40.9480 | \n",
      "Epoch 026 | Loss: 33.7784 | Recon: 33.7784 | \n",
      "Epoch 027 | Loss: 25.4356 | Recon: 25.4356 | \n",
      "Epoch 028 | Loss: 23.2275 | Recon: 23.2275 | \n",
      "Epoch 029 | Loss: 20.5214 | Recon: 20.5214 | \n",
      "Epoch 030 | Loss: 23.4426 | Recon: 23.4426 | \n",
      "Epoch 031 | Loss: 19.3504 | Recon: 19.3504 | \n",
      "Epoch 032 | Loss: 18.7145 | Recon: 18.7145 | \n",
      "Epoch 033 | Loss: 15.4718 | Recon: 15.4718 | \n",
      "Epoch 034 | Loss: 13.7408 | Recon: 13.7408 | \n",
      "Epoch 035 | Loss: 13.7101 | Recon: 13.7101 | \n",
      "Epoch 036 | Loss: 12.8416 | Recon: 12.8416 | \n",
      "Epoch 037 | Loss: 9.3823 | Recon: 9.3823 | \n",
      "Epoch 038 | Loss: 9.3159 | Recon: 9.3159 | \n",
      "Epoch 039 | Loss: 10.0398 | Recon: 10.0398 | \n",
      "Epoch 040 | Loss: 9.0063 | Recon: 9.0063 | \n",
      "Epoch 041 | Loss: 8.5126 | Recon: 8.5126 | \n",
      "Epoch 042 | Loss: 6.4738 | Recon: 6.4738 | \n",
      "Epoch 043 | Loss: 6.7020 | Recon: 6.7020 | \n",
      "Epoch 044 | Loss: 6.4158 | Recon: 6.4158 | \n",
      "Epoch 045 | Loss: 6.3075 | Recon: 6.3075 | \n",
      "Epoch 046 | Loss: 5.9351 | Recon: 5.9351 | \n",
      "Epoch 047 | Loss: 5.8423 | Recon: 5.8423 | \n",
      "Epoch 048 | Loss: 5.4498 | Recon: 5.4498 | \n",
      "Epoch 049 | Loss: 5.0124 | Recon: 5.0124 | \n",
      "Epoch 050 | Loss: 5.1119 | Recon: 5.1119 | \n",
      "Epoch 051 | Loss: 5.0029 | Recon: 5.0029 | \n",
      "Epoch 052 | Loss: 4.5497 | Recon: 4.5497 | \n",
      "Epoch 053 | Loss: 4.6376 | Recon: 4.6376 | \n",
      "Epoch 054 | Loss: 4.4812 | Recon: 4.4812 | \n",
      "Epoch 055 | Loss: 5.0351 | Recon: 5.0351 | \n",
      "Epoch 056 | Loss: 4.1413 | Recon: 4.1413 | \n",
      "Epoch 057 | Loss: 4.0864 | Recon: 4.0864 | \n",
      "Epoch 058 | Loss: 3.8615 | Recon: 3.8615 | \n",
      "Epoch 059 | Loss: 4.1444 | Recon: 4.1444 | \n",
      "Epoch 060 | Loss: 3.9679 | Recon: 3.9679 | \n",
      "Epoch 061 | Loss: 4.0457 | Recon: 4.0457 | \n",
      "Epoch 062 | Loss: 3.7460 | Recon: 3.7460 | \n",
      "Epoch 063 | Loss: 4.0675 | Recon: 4.0675 | \n",
      "Epoch 064 | Loss: 3.8298 | Recon: 3.8298 | \n",
      "Epoch 065 | Loss: 3.7234 | Recon: 3.7234 | \n",
      "Epoch 066 | Loss: 4.0793 | Recon: 4.0793 | \n",
      "Epoch 067 | Loss: 3.8336 | Recon: 3.8336 | \n",
      "Epoch 068 | Loss: 3.8815 | Recon: 3.8815 | \n",
      "Epoch 069 | Loss: 3.6905 | Recon: 3.6905 | \n",
      "Epoch 070 | Loss: 3.6068 | Recon: 3.6068 | \n",
      "Epoch 071 | Loss: 3.5903 | Recon: 3.5903 | \n",
      "Epoch 072 | Loss: 3.4667 | Recon: 3.4667 | \n",
      "Epoch 073 | Loss: 3.5389 | Recon: 3.5389 | \n",
      "Epoch 074 | Loss: 3.3745 | Recon: 3.3745 | \n",
      "Epoch 075 | Loss: 3.4790 | Recon: 3.4790 | \n",
      "Epoch 076 | Loss: 3.4896 | Recon: 3.4896 | \n",
      "Epoch 077 | Loss: 3.6239 | Recon: 3.6239 | \n",
      "Epoch 078 | Loss: 3.2741 | Recon: 3.2741 | \n",
      "Epoch 079 | Loss: 3.4485 | Recon: 3.4485 | \n",
      "Epoch 080 | Loss: 3.3440 | Recon: 3.3440 | \n",
      "Epoch 081 | Loss: 3.5800 | Recon: 3.5800 | \n",
      "Epoch 082 | Loss: 3.4228 | Recon: 3.4228 | \n",
      "Epoch 083 | Loss: 3.5614 | Recon: 3.5614 | \n",
      "Epoch 084 | Loss: 3.4505 | Recon: 3.4505 | \n",
      "Epoch 085 | Loss: 3.3701 | Recon: 3.3701 | \n",
      "Epoch 086 | Loss: 3.5155 | Recon: 3.5155 | \n",
      "Epoch 087 | Loss: 3.2885 | Recon: 3.2885 | \n",
      "Epoch 088 | Loss: 3.3294 | Recon: 3.3294 | \n",
      "Epoch 089 | Loss: 3.3468 | Recon: 3.3468 | \n",
      "Epoch 090 | Loss: 3.3139 | Recon: 3.3139 | \n",
      "Epoch 091 | Loss: 3.1912 | Recon: 3.1912 | \n",
      "Epoch 092 | Loss: 3.2359 | Recon: 3.2359 | \n",
      "Epoch 093 | Loss: 3.2205 | Recon: 3.2205 | \n",
      "Epoch 094 | Loss: 3.2103 | Recon: 3.2103 | \n",
      "Epoch 095 | Loss: 3.2889 | Recon: 3.2889 | \n",
      "Epoch 096 | Loss: 3.1894 | Recon: 3.1894 | \n",
      "Epoch 097 | Loss: 3.1263 | Recon: 3.1263 | \n",
      "Epoch 098 | Loss: 3.2760 | Recon: 3.2760 | \n",
      "Epoch 099 | Loss: 3.2839 | Recon: 3.2839 | \n",
      "Epoch 100 | Loss: 3.1564 | Recon: 3.1564 | \n",
      "Epoch 101 | Loss: 3.1275 | Recon: 3.1275 | \n",
      "Epoch 102 | Loss: 3.1048 | Recon: 3.1048 | \n",
      "Epoch 103 | Loss: 3.1651 | Recon: 3.1651 | \n",
      "Epoch 104 | Loss: 3.1312 | Recon: 3.1312 | \n",
      "Epoch 105 | Loss: 3.1038 | Recon: 3.1038 | \n",
      "Epoch 106 | Loss: 3.0686 | Recon: 3.0686 | \n",
      "Epoch 107 | Loss: 3.0452 | Recon: 3.0452 | \n",
      "Epoch 108 | Loss: 3.0134 | Recon: 3.0134 | \n",
      "Epoch 109 | Loss: 2.9362 | Recon: 2.9362 | \n",
      "Epoch 110 | Loss: 3.0160 | Recon: 3.0160 | \n",
      "Epoch 111 | Loss: 3.0883 | Recon: 3.0883 | \n",
      "Epoch 112 | Loss: 2.9166 | Recon: 2.9166 | \n",
      "Epoch 113 | Loss: 3.0008 | Recon: 3.0008 | \n",
      "Epoch 114 | Loss: 2.9913 | Recon: 2.9913 | \n",
      "Epoch 115 | Loss: 2.9799 | Recon: 2.9799 | \n",
      "Epoch 116 | Loss: 2.9732 | Recon: 2.9732 | \n",
      "Epoch 117 | Loss: 2.9101 | Recon: 2.9101 | \n",
      "Epoch 118 | Loss: 3.0590 | Recon: 3.0590 | \n",
      "Epoch 119 | Loss: 2.8538 | Recon: 2.8538 | \n",
      "Epoch 120 | Loss: 2.9252 | Recon: 2.9252 | \n",
      "Epoch 121 | Loss: 2.9595 | Recon: 2.9595 | \n",
      "Epoch 122 | Loss: 2.8904 | Recon: 2.8904 | \n",
      "Epoch 123 | Loss: 2.9392 | Recon: 2.9392 | \n",
      "Epoch 124 | Loss: 2.9275 | Recon: 2.9275 | \n",
      "Epoch 125 | Loss: 2.8956 | Recon: 2.8956 | \n",
      "Epoch 126 | Loss: 2.9054 | Recon: 2.9054 | \n",
      "Epoch 127 | Loss: 2.8534 | Recon: 2.8534 | \n",
      "Epoch 128 | Loss: 2.8574 | Recon: 2.8574 | \n",
      "Epoch 129 | Loss: 2.7876 | Recon: 2.7876 | \n",
      "Epoch 130 | Loss: 2.8340 | Recon: 2.8340 | \n",
      "Epoch 131 | Loss: 2.8219 | Recon: 2.8219 | \n",
      "Epoch 132 | Loss: 2.8231 | Recon: 2.8231 | \n",
      "Epoch 133 | Loss: 2.8922 | Recon: 2.8922 | \n",
      "Epoch 134 | Loss: 2.8898 | Recon: 2.8898 | \n",
      "Epoch 135 | Loss: 2.8640 | Recon: 2.8640 | \n",
      "Epoch 136 | Loss: 2.8301 | Recon: 2.8301 | \n",
      "Epoch 137 | Loss: 2.8829 | Recon: 2.8829 | \n",
      "Epoch 138 | Loss: 2.7307 | Recon: 2.7307 | \n",
      "Epoch 139 | Loss: 2.7795 | Recon: 2.7795 | \n",
      "Epoch 140 | Loss: 2.7991 | Recon: 2.7991 | \n",
      "Epoch 141 | Loss: 2.8034 | Recon: 2.8034 | \n",
      "Epoch 142 | Loss: 2.8145 | Recon: 2.8145 | \n",
      "Epoch 143 | Loss: 2.7623 | Recon: 2.7623 | \n",
      "Epoch 144 | Loss: 2.7634 | Recon: 2.7634 | \n",
      "Epoch 145 | Loss: 2.8212 | Recon: 2.8212 | \n",
      "Epoch 146 | Loss: 2.7525 | Recon: 2.7525 | \n",
      "Epoch 147 | Loss: 2.8292 | Recon: 2.8292 | \n",
      "Epoch 148 | Loss: 2.7949 | Recon: 2.7949 | \n",
      "Epoch 149 | Loss: 2.7428 | Recon: 2.7428 | \n",
      "Epoch 150 | Loss: 2.7078 | Recon: 2.7078 | \n",
      "Epoch 151 | Loss: 2.7147 | Recon: 2.7147 | \n",
      "Epoch 152 | Loss: 2.7782 | Recon: 2.7782 | \n",
      "Epoch 153 | Loss: 2.7000 | Recon: 2.7000 | \n",
      "Epoch 154 | Loss: 2.6785 | Recon: 2.6785 | \n",
      "Epoch 155 | Loss: 2.7014 | Recon: 2.7014 | \n",
      "Epoch 156 | Loss: 2.7155 | Recon: 2.7155 | \n",
      "Epoch 157 | Loss: 2.6690 | Recon: 2.6690 | \n",
      "Epoch 158 | Loss: 2.6558 | Recon: 2.6558 | \n",
      "Epoch 159 | Loss: 2.6833 | Recon: 2.6833 | \n",
      "Epoch 160 | Loss: 2.7299 | Recon: 2.7299 | \n",
      "Epoch 161 | Loss: 2.7143 | Recon: 2.7143 | \n",
      "Epoch 162 | Loss: 2.6488 | Recon: 2.6488 | \n",
      "Epoch 163 | Loss: 2.7053 | Recon: 2.7053 | \n",
      "Epoch 164 | Loss: 2.6509 | Recon: 2.6509 | \n",
      "Epoch 165 | Loss: 2.6891 | Recon: 2.6891 | \n",
      "Epoch 166 | Loss: 2.6595 | Recon: 2.6595 | \n",
      "Epoch 167 | Loss: 2.7015 | Recon: 2.7015 | \n",
      "Epoch 168 | Loss: 2.6482 | Recon: 2.6482 | \n",
      "Epoch 169 | Loss: 2.6046 | Recon: 2.6046 | \n",
      "Epoch 170 | Loss: 2.6383 | Recon: 2.6383 | \n",
      "Epoch 171 | Loss: 2.6643 | Recon: 2.6643 | \n",
      "Epoch 172 | Loss: 2.6272 | Recon: 2.6272 | \n",
      "Epoch 173 | Loss: 2.6142 | Recon: 2.6142 | \n",
      "Epoch 174 | Loss: 2.6487 | Recon: 2.6487 | \n",
      "Epoch 175 | Loss: 2.6339 | Recon: 2.6339 | \n",
      "Epoch 176 | Loss: 2.6484 | Recon: 2.6484 | \n",
      "Epoch 177 | Loss: 2.6124 | Recon: 2.6124 | \n",
      "Epoch 178 | Loss: 2.6164 | Recon: 2.6164 | \n",
      "Epoch 179 | Loss: 2.6140 | Recon: 2.6140 | \n",
      "Epoch 180 | Loss: 2.5968 | Recon: 2.5968 | \n",
      "Epoch 181 | Loss: 2.5937 | Recon: 2.5937 | \n",
      "Epoch 182 | Loss: 2.6385 | Recon: 2.6385 | \n",
      "Epoch 183 | Loss: 2.6338 | Recon: 2.6338 | \n",
      "Epoch 184 | Loss: 2.6309 | Recon: 2.6309 | \n",
      "Epoch 185 | Loss: 2.5609 | Recon: 2.5609 | \n",
      "Epoch 186 | Loss: 2.6826 | Recon: 2.6826 | \n",
      "Epoch 187 | Loss: 2.5694 | Recon: 2.5694 | \n",
      "Epoch 188 | Loss: 2.5646 | Recon: 2.5646 | \n",
      "Epoch 189 | Loss: 2.5631 | Recon: 2.5631 | \n",
      "Epoch 190 | Loss: 2.5076 | Recon: 2.5076 | \n",
      "Epoch 191 | Loss: 2.5582 | Recon: 2.5582 | \n",
      "Epoch 192 | Loss: 2.5887 | Recon: 2.5887 | \n",
      "Epoch 193 | Loss: 2.5276 | Recon: 2.5276 | \n",
      "Epoch 194 | Loss: 2.5945 | Recon: 2.5945 | \n",
      "Epoch 195 | Loss: 2.5093 | Recon: 2.5093 | \n",
      "Epoch 196 | Loss: 2.5316 | Recon: 2.5316 | \n",
      "Epoch 197 | Loss: 2.5539 | Recon: 2.5539 | \n",
      "Epoch 198 | Loss: 2.4619 | Recon: 2.4619 | \n",
      "Epoch 199 | Loss: 2.5026 | Recon: 2.5026 | \n",
      "Epoch 200 | Loss: 2.5533 | Recon: 2.5533 | \n",
      "Epoch 201 | Loss: 2.5753 | Recon: 2.5753 | \n",
      "Epoch 202 | Loss: 2.5801 | Recon: 2.5801 | \n",
      "Epoch 203 | Loss: 2.4716 | Recon: 2.4716 | \n",
      "Epoch 204 | Loss: 2.5095 | Recon: 2.5095 | \n",
      "Epoch 205 | Loss: 2.4942 | Recon: 2.4942 | \n",
      "Epoch 206 | Loss: 2.4776 | Recon: 2.4776 | \n",
      "Epoch 207 | Loss: 2.5013 | Recon: 2.5013 | \n",
      "Epoch 208 | Loss: 2.5297 | Recon: 2.5297 | \n",
      "Epoch 209 | Loss: 2.5281 | Recon: 2.5281 | \n",
      "Epoch 210 | Loss: 2.5068 | Recon: 2.5068 | \n",
      "Epoch 211 | Loss: 2.5403 | Recon: 2.5403 | \n",
      "Epoch 212 | Loss: 2.5066 | Recon: 2.5066 | \n",
      "Epoch 213 | Loss: 2.5015 | Recon: 2.5015 | \n",
      "Epoch 214 | Loss: 2.5133 | Recon: 2.5133 | \n",
      "Epoch 215 | Loss: 2.4584 | Recon: 2.4584 | \n",
      "Epoch 216 | Loss: 2.4878 | Recon: 2.4878 | \n",
      "Epoch 217 | Loss: 2.5313 | Recon: 2.5313 | \n",
      "Epoch 218 | Loss: 2.4705 | Recon: 2.4705 | \n",
      "Epoch 219 | Loss: 2.5602 | Recon: 2.5602 | \n",
      "Epoch 220 | Loss: 2.4863 | Recon: 2.4863 | \n",
      "Epoch 221 | Loss: 2.5146 | Recon: 2.5146 | \n",
      "Epoch 222 | Loss: 2.5083 | Recon: 2.5083 | \n",
      "Epoch 223 | Loss: 2.5189 | Recon: 2.5189 | \n",
      "Epoch 224 | Loss: 2.4396 | Recon: 2.4396 | \n",
      "Epoch 225 | Loss: 2.5009 | Recon: 2.5009 | \n",
      "Epoch 226 | Loss: 2.4451 | Recon: 2.4451 | \n",
      "Epoch 227 | Loss: 2.4254 | Recon: 2.4254 | \n",
      "Epoch 228 | Loss: 2.5497 | Recon: 2.5497 | \n",
      "Epoch 229 | Loss: 2.4588 | Recon: 2.4588 | \n",
      "Epoch 230 | Loss: 2.4026 | Recon: 2.4026 | \n",
      "Epoch 231 | Loss: 2.5217 | Recon: 2.5217 | \n",
      "Epoch 232 | Loss: 2.3792 | Recon: 2.3792 | \n",
      "Epoch 233 | Loss: 2.4549 | Recon: 2.4549 | \n",
      "Epoch 234 | Loss: 2.4480 | Recon: 2.4480 | \n",
      "Epoch 235 | Loss: 2.4455 | Recon: 2.4455 | \n",
      "Epoch 236 | Loss: 2.3998 | Recon: 2.3998 | \n",
      "Epoch 237 | Loss: 2.4526 | Recon: 2.4526 | \n",
      "Epoch 238 | Loss: 2.4445 | Recon: 2.4445 | \n",
      "Epoch 239 | Loss: 2.4324 | Recon: 2.4324 | \n",
      "Epoch 240 | Loss: 2.4393 | Recon: 2.4393 | \n",
      "Epoch 241 | Loss: 2.4358 | Recon: 2.4358 | \n",
      "Epoch 242 | Loss: 2.4854 | Recon: 2.4854 | \n",
      "Epoch 243 | Loss: 2.4266 | Recon: 2.4266 | \n",
      "Epoch 244 | Loss: 2.4088 | Recon: 2.4088 | \n",
      "Epoch 245 | Loss: 2.4204 | Recon: 2.4204 | \n",
      "Epoch 246 | Loss: 2.3886 | Recon: 2.3886 | \n",
      "Epoch 247 | Loss: 2.4277 | Recon: 2.4277 | \n",
      "Epoch 248 | Loss: 2.3531 | Recon: 2.3531 | \n",
      "Epoch 249 | Loss: 2.4061 | Recon: 2.4061 | \n",
      "Epoch 250 | Loss: 2.4056 | Recon: 2.4056 | \n",
      "Epoch 251 | Loss: 2.4035 | Recon: 2.4035 | \n",
      "Epoch 252 | Loss: 2.3636 | Recon: 2.3636 | \n",
      "Epoch 253 | Loss: 2.3797 | Recon: 2.3797 | \n",
      "Epoch 254 | Loss: 2.3628 | Recon: 2.3628 | \n",
      "Epoch 255 | Loss: 2.3414 | Recon: 2.3414 | \n",
      "Epoch 256 | Loss: 2.4128 | Recon: 2.4128 | \n",
      "Epoch 257 | Loss: 2.3373 | Recon: 2.3373 | \n",
      "Epoch 258 | Loss: 2.3584 | Recon: 2.3584 | \n",
      "Epoch 259 | Loss: 2.3361 | Recon: 2.3361 | \n",
      "Epoch 260 | Loss: 2.3769 | Recon: 2.3769 | \n",
      "Epoch 261 | Loss: 2.3694 | Recon: 2.3694 | \n",
      "Epoch 262 | Loss: 2.3495 | Recon: 2.3495 | \n",
      "Epoch 263 | Loss: 2.3065 | Recon: 2.3065 | \n",
      "Epoch 264 | Loss: 2.3070 | Recon: 2.3070 | \n",
      "Epoch 265 | Loss: 2.2933 | Recon: 2.2933 | \n",
      "Epoch 266 | Loss: 2.3557 | Recon: 2.3557 | \n",
      "Epoch 267 | Loss: 2.2834 | Recon: 2.2834 | \n",
      "Epoch 268 | Loss: 2.3348 | Recon: 2.3348 | \n",
      "Epoch 269 | Loss: 2.3437 | Recon: 2.3437 | \n",
      "Epoch 270 | Loss: 2.2588 | Recon: 2.2588 | \n",
      "Epoch 271 | Loss: 2.3455 | Recon: 2.3455 | \n",
      "Epoch 272 | Loss: 2.3367 | Recon: 2.3367 | \n",
      "Epoch 273 | Loss: 2.2759 | Recon: 2.2759 | \n",
      "Epoch 274 | Loss: 2.3131 | Recon: 2.3131 | \n",
      "Epoch 275 | Loss: 2.2884 | Recon: 2.2884 | \n",
      "Epoch 276 | Loss: 2.3019 | Recon: 2.3019 | \n",
      "Epoch 277 | Loss: 2.3152 | Recon: 2.3152 | \n",
      "Epoch 278 | Loss: 2.3312 | Recon: 2.3312 | \n",
      "Epoch 279 | Loss: 2.3469 | Recon: 2.3469 | \n",
      "Epoch 280 | Loss: 2.2937 | Recon: 2.2937 | \n",
      "Epoch 281 | Loss: 2.2624 | Recon: 2.2624 | \n",
      "Epoch 282 | Loss: 2.2430 | Recon: 2.2430 | \n",
      "Epoch 283 | Loss: 2.3072 | Recon: 2.3072 | \n",
      "Epoch 284 | Loss: 2.2749 | Recon: 2.2749 | \n",
      "Epoch 285 | Loss: 2.2691 | Recon: 2.2691 | \n",
      "Epoch 286 | Loss: 2.2380 | Recon: 2.2380 | \n",
      "Epoch 287 | Loss: 2.2270 | Recon: 2.2270 | \n",
      "Epoch 288 | Loss: 2.2413 | Recon: 2.2413 | \n",
      "Epoch 289 | Loss: 2.3069 | Recon: 2.3069 | \n",
      "Epoch 290 | Loss: 2.2659 | Recon: 2.2659 | \n",
      "Epoch 291 | Loss: 2.2515 | Recon: 2.2515 | \n",
      "Epoch 292 | Loss: 2.1919 | Recon: 2.1919 | \n",
      "Epoch 293 | Loss: 2.2300 | Recon: 2.2300 | \n",
      "Epoch 294 | Loss: 2.2345 | Recon: 2.2345 | \n",
      "Epoch 295 | Loss: 2.1840 | Recon: 2.1840 | \n",
      "Epoch 296 | Loss: 2.2203 | Recon: 2.2203 | \n",
      "Epoch 297 | Loss: 2.2371 | Recon: 2.2371 | \n",
      "Epoch 298 | Loss: 2.2540 | Recon: 2.2540 | \n",
      "Epoch 299 | Loss: 2.2319 | Recon: 2.2319 | \n",
      "Epoch 300 | Loss: 2.1707 | Recon: 2.1707 | \n",
      "Epoch 301 | Loss: 2.1513 | Recon: 2.1513 | \n",
      "Epoch 302 | Loss: 2.2150 | Recon: 2.2150 | \n",
      "Epoch 303 | Loss: 2.1690 | Recon: 2.1690 | \n",
      "Epoch 304 | Loss: 2.2264 | Recon: 2.2264 | \n",
      "Epoch 305 | Loss: 2.2000 | Recon: 2.2000 | \n",
      "Epoch 306 | Loss: 2.1515 | Recon: 2.1515 | \n",
      "Epoch 307 | Loss: 2.1743 | Recon: 2.1743 | \n",
      "Epoch 308 | Loss: 2.1464 | Recon: 2.1464 | \n",
      "Epoch 309 | Loss: 2.1132 | Recon: 2.1132 | \n",
      "Epoch 310 | Loss: 2.1375 | Recon: 2.1375 | \n",
      "Epoch 311 | Loss: 2.1416 | Recon: 2.1416 | \n",
      "Epoch 312 | Loss: 2.1681 | Recon: 2.1681 | \n",
      "Epoch 313 | Loss: 2.1001 | Recon: 2.1001 | \n",
      "Epoch 314 | Loss: 2.1348 | Recon: 2.1348 | \n",
      "Epoch 315 | Loss: 2.1278 | Recon: 2.1278 | \n",
      "Epoch 316 | Loss: 2.1548 | Recon: 2.1548 | \n",
      "Epoch 317 | Loss: 2.1369 | Recon: 2.1369 | \n",
      "Epoch 318 | Loss: 2.1222 | Recon: 2.1222 | \n",
      "Epoch 319 | Loss: 2.1380 | Recon: 2.1380 | \n",
      "Epoch 320 | Loss: 2.0624 | Recon: 2.0624 | \n",
      "Epoch 321 | Loss: 2.0558 | Recon: 2.0558 | \n",
      "Epoch 322 | Loss: 2.0834 | Recon: 2.0834 | \n",
      "Epoch 323 | Loss: 2.0465 | Recon: 2.0465 | \n",
      "Epoch 324 | Loss: 2.0596 | Recon: 2.0596 | \n",
      "Epoch 325 | Loss: 2.1231 | Recon: 2.1231 | \n",
      "Epoch 326 | Loss: 2.0407 | Recon: 2.0407 | \n",
      "Epoch 327 | Loss: 2.0734 | Recon: 2.0734 | \n",
      "Epoch 328 | Loss: 2.0500 | Recon: 2.0500 | \n",
      "Epoch 329 | Loss: 2.0719 | Recon: 2.0719 | \n",
      "Epoch 330 | Loss: 2.0949 | Recon: 2.0949 | \n",
      "Epoch 331 | Loss: 2.0164 | Recon: 2.0164 | \n",
      "Epoch 332 | Loss: 2.0245 | Recon: 2.0245 | \n",
      "Epoch 333 | Loss: 2.0749 | Recon: 2.0749 | \n",
      "Epoch 334 | Loss: 2.0045 | Recon: 2.0045 | \n",
      "Epoch 335 | Loss: 2.0086 | Recon: 2.0086 | \n",
      "Epoch 336 | Loss: 2.0151 | Recon: 2.0151 | \n",
      "Epoch 337 | Loss: 1.9639 | Recon: 1.9639 | \n",
      "Epoch 338 | Loss: 1.9687 | Recon: 1.9687 | \n",
      "Epoch 339 | Loss: 2.0298 | Recon: 2.0298 | \n",
      "Epoch 340 | Loss: 1.9744 | Recon: 1.9744 | \n",
      "Epoch 341 | Loss: 2.0303 | Recon: 2.0303 | \n",
      "Epoch 342 | Loss: 1.9586 | Recon: 1.9586 | \n",
      "Epoch 343 | Loss: 2.0033 | Recon: 2.0033 | \n",
      "Epoch 344 | Loss: 2.0019 | Recon: 2.0019 | \n",
      "Epoch 345 | Loss: 1.9696 | Recon: 1.9696 | \n",
      "Epoch 346 | Loss: 1.9505 | Recon: 1.9505 | \n",
      "Epoch 347 | Loss: 1.9349 | Recon: 1.9349 | \n",
      "Epoch 348 | Loss: 1.9441 | Recon: 1.9441 | \n",
      "Epoch 349 | Loss: 1.9698 | Recon: 1.9698 | \n",
      "Epoch 350 | Loss: 1.8935 | Recon: 1.8935 | \n",
      "Epoch 351 | Loss: 1.8502 | Recon: 1.8502 | \n",
      "Epoch 352 | Loss: 1.8786 | Recon: 1.8786 | \n",
      "Epoch 353 | Loss: 1.9733 | Recon: 1.9733 | \n",
      "Epoch 354 | Loss: 1.8710 | Recon: 1.8710 | \n",
      "Epoch 355 | Loss: 1.8838 | Recon: 1.8838 | \n",
      "Epoch 356 | Loss: 1.8910 | Recon: 1.8910 | \n",
      "Epoch 357 | Loss: 1.7899 | Recon: 1.7899 | \n",
      "Epoch 358 | Loss: 1.8176 | Recon: 1.8176 | \n",
      "Epoch 359 | Loss: 1.7896 | Recon: 1.7896 | \n",
      "Epoch 360 | Loss: 1.8736 | Recon: 1.8736 | \n",
      "Epoch 361 | Loss: 1.8452 | Recon: 1.8452 | \n",
      "Epoch 362 | Loss: 1.8064 | Recon: 1.8064 | \n",
      "Epoch 363 | Loss: 1.7397 | Recon: 1.7397 | \n",
      "Epoch 364 | Loss: 1.8036 | Recon: 1.8036 | \n",
      "Epoch 365 | Loss: 1.7897 | Recon: 1.7897 | \n",
      "Epoch 366 | Loss: 1.7390 | Recon: 1.7390 | \n",
      "Epoch 367 | Loss: 1.8038 | Recon: 1.8038 | \n",
      "Epoch 368 | Loss: 1.7577 | Recon: 1.7577 | \n",
      "Epoch 369 | Loss: 1.7532 | Recon: 1.7532 | \n",
      "Epoch 370 | Loss: 1.6887 | Recon: 1.6887 | \n",
      "Epoch 371 | Loss: 1.7225 | Recon: 1.7225 | \n",
      "Epoch 372 | Loss: 1.7366 | Recon: 1.7366 | \n",
      "Epoch 373 | Loss: 1.6865 | Recon: 1.6865 | \n",
      "Epoch 374 | Loss: 1.7070 | Recon: 1.7070 | \n",
      "Epoch 375 | Loss: 1.7683 | Recon: 1.7683 | \n",
      "Epoch 376 | Loss: 1.7251 | Recon: 1.7251 | \n",
      "Epoch 377 | Loss: 1.7129 | Recon: 1.7129 | \n",
      "Epoch 378 | Loss: 1.6373 | Recon: 1.6373 | \n",
      "Epoch 379 | Loss: 1.6779 | Recon: 1.6779 | \n",
      "Epoch 380 | Loss: 1.6466 | Recon: 1.6466 | \n",
      "Epoch 381 | Loss: 1.6405 | Recon: 1.6405 | \n",
      "Epoch 382 | Loss: 1.6177 | Recon: 1.6177 | \n",
      "Epoch 383 | Loss: 1.6838 | Recon: 1.6838 | \n",
      "Epoch 384 | Loss: 1.5670 | Recon: 1.5670 | \n",
      "Epoch 385 | Loss: 1.5959 | Recon: 1.5959 | \n",
      "Epoch 386 | Loss: 1.5576 | Recon: 1.5576 | \n",
      "Epoch 387 | Loss: 1.6080 | Recon: 1.6080 | \n",
      "Epoch 388 | Loss: 1.4905 | Recon: 1.4905 | \n",
      "Epoch 389 | Loss: 1.5101 | Recon: 1.5101 | \n",
      "Epoch 390 | Loss: 1.5807 | Recon: 1.5807 | \n",
      "Epoch 391 | Loss: 1.4787 | Recon: 1.4787 | \n",
      "Epoch 392 | Loss: 1.4505 | Recon: 1.4505 | \n",
      "Epoch 393 | Loss: 1.5273 | Recon: 1.5273 | \n",
      "Epoch 394 | Loss: 1.4743 | Recon: 1.4743 | \n",
      "Epoch 395 | Loss: 1.4585 | Recon: 1.4585 | \n",
      "Epoch 396 | Loss: 1.4825 | Recon: 1.4825 | \n",
      "Epoch 397 | Loss: 1.4576 | Recon: 1.4576 | \n",
      "Epoch 398 | Loss: 1.4738 | Recon: 1.4738 | \n",
      "Epoch 399 | Loss: 1.5021 | Recon: 1.5021 | \n",
      "Epoch 400 | Loss: 1.3642 | Recon: 1.3642 | \n",
      "Epoch 401 | Loss: 1.4085 | Recon: 1.4085 | \n",
      "Epoch 402 | Loss: 1.3877 | Recon: 1.3877 | \n",
      "Epoch 403 | Loss: 1.3335 | Recon: 1.3335 | \n",
      "Epoch 404 | Loss: 1.3198 | Recon: 1.3198 | \n",
      "Epoch 405 | Loss: 1.2713 | Recon: 1.2713 | \n",
      "Epoch 406 | Loss: 1.2885 | Recon: 1.2885 | \n",
      "Epoch 407 | Loss: 1.3095 | Recon: 1.3095 | \n",
      "Epoch 408 | Loss: 1.3254 | Recon: 1.3254 | \n",
      "Epoch 409 | Loss: 1.3065 | Recon: 1.3065 | \n",
      "Epoch 410 | Loss: 1.2339 | Recon: 1.2339 | \n",
      "Epoch 411 | Loss: 1.2258 | Recon: 1.2258 | \n",
      "Epoch 412 | Loss: 1.1733 | Recon: 1.1733 | \n",
      "Epoch 413 | Loss: 1.1778 | Recon: 1.1778 | \n",
      "Epoch 414 | Loss: 1.2095 | Recon: 1.2095 | \n",
      "Epoch 415 | Loss: 1.0968 | Recon: 1.0968 | \n",
      "Epoch 416 | Loss: 1.1351 | Recon: 1.1351 | \n",
      "Epoch 417 | Loss: 1.2056 | Recon: 1.2056 | \n",
      "Epoch 418 | Loss: 1.1231 | Recon: 1.1231 | \n",
      "Epoch 419 | Loss: 1.1688 | Recon: 1.1688 | \n",
      "Epoch 420 | Loss: 1.1227 | Recon: 1.1227 | \n",
      "Epoch 421 | Loss: 1.1516 | Recon: 1.1516 | \n",
      "Epoch 422 | Loss: 1.0315 | Recon: 1.0315 | \n",
      "Epoch 423 | Loss: 1.0340 | Recon: 1.0340 | \n",
      "Epoch 424 | Loss: 0.9409 | Recon: 0.9409 | \n",
      "Epoch 425 | Loss: 1.0233 | Recon: 1.0233 | \n",
      "Epoch 426 | Loss: 0.9544 | Recon: 0.9544 | \n",
      "Epoch 427 | Loss: 1.0540 | Recon: 1.0540 | \n",
      "Epoch 428 | Loss: 0.9497 | Recon: 0.9497 | \n",
      "Epoch 429 | Loss: 0.9676 | Recon: 0.9676 | \n",
      "Epoch 430 | Loss: 1.0071 | Recon: 1.0071 | \n",
      "Epoch 431 | Loss: 0.8625 | Recon: 0.8625 | \n",
      "Epoch 432 | Loss: 0.8646 | Recon: 0.8646 | \n",
      "Epoch 433 | Loss: 0.8936 | Recon: 0.8936 | \n",
      "Epoch 434 | Loss: 0.8836 | Recon: 0.8836 | \n",
      "Epoch 435 | Loss: 0.8855 | Recon: 0.8855 | \n",
      "Epoch 436 | Loss: 0.8312 | Recon: 0.8312 | \n",
      "Epoch 437 | Loss: 0.8745 | Recon: 0.8745 | \n",
      "Epoch 438 | Loss: 0.8323 | Recon: 0.8323 | \n",
      "Epoch 439 | Loss: 0.7942 | Recon: 0.7942 | \n",
      "Epoch 440 | Loss: 0.8374 | Recon: 0.8374 | \n",
      "Epoch 441 | Loss: 0.8132 | Recon: 0.8132 | \n",
      "Epoch 442 | Loss: 0.8025 | Recon: 0.8025 | \n",
      "Epoch 443 | Loss: 0.7341 | Recon: 0.7341 | \n",
      "Epoch 444 | Loss: 0.7190 | Recon: 0.7190 | \n",
      "Epoch 445 | Loss: 0.7381 | Recon: 0.7381 | \n",
      "Epoch 446 | Loss: 0.7080 | Recon: 0.7080 | \n",
      "Epoch 447 | Loss: 0.6798 | Recon: 0.6798 | \n",
      "Epoch 448 | Loss: 0.6792 | Recon: 0.6792 | \n",
      "Epoch 449 | Loss: 0.6782 | Recon: 0.6782 | \n",
      "Epoch 450 | Loss: 0.7161 | Recon: 0.7161 | \n",
      "Epoch 451 | Loss: 0.6177 | Recon: 0.6177 | \n",
      "Epoch 452 | Loss: 0.6660 | Recon: 0.6660 | \n",
      "Epoch 453 | Loss: 0.6337 | Recon: 0.6337 | \n",
      "Epoch 454 | Loss: 0.6200 | Recon: 0.6200 | \n",
      "Epoch 455 | Loss: 0.6689 | Recon: 0.6689 | \n",
      "Epoch 456 | Loss: 0.6045 | Recon: 0.6045 | \n",
      "Epoch 457 | Loss: 0.6442 | Recon: 0.6442 | \n",
      "Epoch 458 | Loss: 0.5903 | Recon: 0.5903 | \n",
      "Epoch 459 | Loss: 0.5234 | Recon: 0.5234 | \n",
      "Epoch 460 | Loss: 0.5735 | Recon: 0.5735 | \n",
      "Epoch 461 | Loss: 0.5388 | Recon: 0.5388 | \n",
      "Epoch 462 | Loss: 0.5302 | Recon: 0.5302 | \n",
      "Epoch 463 | Loss: 0.5515 | Recon: 0.5515 | \n",
      "Epoch 464 | Loss: 0.5908 | Recon: 0.5908 | \n",
      "Epoch 465 | Loss: 0.5303 | Recon: 0.5303 | \n",
      "Epoch 466 | Loss: 0.5605 | Recon: 0.5605 | \n",
      "Epoch 467 | Loss: 0.5671 | Recon: 0.5671 | \n",
      "Epoch 468 | Loss: 0.5346 | Recon: 0.5346 | \n",
      "Epoch 469 | Loss: 0.5150 | Recon: 0.5150 | \n",
      "Epoch 470 | Loss: 0.5511 | Recon: 0.5511 | \n",
      "Epoch 471 | Loss: 0.4820 | Recon: 0.4820 | \n",
      "Epoch 472 | Loss: 0.5078 | Recon: 0.5078 | \n",
      "Epoch 473 | Loss: 0.4911 | Recon: 0.4911 | \n",
      "Epoch 474 | Loss: 0.5254 | Recon: 0.5254 | \n",
      "Epoch 475 | Loss: 0.5338 | Recon: 0.5338 | \n",
      "Epoch 476 | Loss: 0.5253 | Recon: 0.5253 | \n",
      "Epoch 477 | Loss: 0.5196 | Recon: 0.5196 | \n",
      "Epoch 478 | Loss: 0.4933 | Recon: 0.4933 | \n",
      "Epoch 479 | Loss: 0.4971 | Recon: 0.4971 | \n",
      "Epoch 480 | Loss: 0.5611 | Recon: 0.5611 | \n",
      "Epoch 481 | Loss: 0.4410 | Recon: 0.4410 | \n",
      "Epoch 482 | Loss: 0.4954 | Recon: 0.4954 | \n",
      "Epoch 483 | Loss: 0.4903 | Recon: 0.4903 | \n",
      "Epoch 484 | Loss: 0.4785 | Recon: 0.4785 | \n",
      "Epoch 485 | Loss: 0.4514 | Recon: 0.4514 | \n",
      "Epoch 486 | Loss: 0.4707 | Recon: 0.4707 | \n",
      "Epoch 487 | Loss: 0.4761 | Recon: 0.4761 | \n",
      "Epoch 488 | Loss: 0.4259 | Recon: 0.4259 | \n",
      "Epoch 489 | Loss: 0.4482 | Recon: 0.4482 | \n",
      "Epoch 490 | Loss: 0.4575 | Recon: 0.4575 | \n",
      "Epoch 491 | Loss: 0.4773 | Recon: 0.4773 | \n",
      "Epoch 492 | Loss: 0.4923 | Recon: 0.4923 | \n",
      "Epoch 493 | Loss: 0.4640 | Recon: 0.4640 | \n",
      "Epoch 494 | Loss: 0.4193 | Recon: 0.4193 | \n",
      "Epoch 495 | Loss: 0.4391 | Recon: 0.4391 | \n",
      "Epoch 496 | Loss: 0.4340 | Recon: 0.4340 | \n",
      "Epoch 497 | Loss: 0.4589 | Recon: 0.4589 | \n",
      "Epoch 498 | Loss: 0.4571 | Recon: 0.4571 | \n",
      "Epoch 499 | Loss: 0.4317 | Recon: 0.4317 | \n",
      "Epoch 500 | Loss: 0.3965 | Recon: 0.3965 | \n",
      "Epoch 501 | Loss: 0.4100 | Recon: 0.4100 | \n",
      "Epoch 502 | Loss: 0.4523 | Recon: 0.4523 | \n",
      "Epoch 503 | Loss: 0.4038 | Recon: 0.4038 | \n",
      "Epoch 504 | Loss: 0.4163 | Recon: 0.4163 | \n",
      "Epoch 505 | Loss: 0.4123 | Recon: 0.4123 | \n",
      "Epoch 506 | Loss: 0.4049 | Recon: 0.4049 | \n",
      "Epoch 507 | Loss: 0.4390 | Recon: 0.4390 | \n",
      "Epoch 508 | Loss: 0.4096 | Recon: 0.4096 | \n",
      "Epoch 509 | Loss: 0.4661 | Recon: 0.4661 | \n",
      "Epoch 510 | Loss: 0.4758 | Recon: 0.4758 | \n",
      "Epoch 511 | Loss: 0.4236 | Recon: 0.4236 | \n",
      "Epoch 512 | Loss: 0.4165 | Recon: 0.4165 | \n",
      "Epoch 513 | Loss: 0.4255 | Recon: 0.4255 | \n",
      "Epoch 514 | Loss: 0.4485 | Recon: 0.4485 | \n",
      "Epoch 515 | Loss: 0.4365 | Recon: 0.4365 | \n",
      "Epoch 516 | Loss: 0.4220 | Recon: 0.4220 | \n",
      "Epoch 517 | Loss: 0.4226 | Recon: 0.4226 | \n",
      "Epoch 518 | Loss: 0.4308 | Recon: 0.4308 | \n",
      "Epoch 519 | Loss: 0.4311 | Recon: 0.4311 | \n",
      "Epoch 520 | Loss: 0.3643 | Recon: 0.3643 | \n",
      "Epoch 521 | Loss: 0.4171 | Recon: 0.4171 | \n",
      "Epoch 522 | Loss: 0.4134 | Recon: 0.4134 | \n",
      "Epoch 523 | Loss: 0.3937 | Recon: 0.3937 | \n",
      "Epoch 524 | Loss: 0.4033 | Recon: 0.4033 | \n",
      "Epoch 525 | Loss: 0.4165 | Recon: 0.4165 | \n",
      "Epoch 526 | Loss: 0.4048 | Recon: 0.4048 | \n",
      "Epoch 527 | Loss: 0.4167 | Recon: 0.4167 | \n",
      "Epoch 528 | Loss: 0.4313 | Recon: 0.4313 | \n",
      "Epoch 529 | Loss: 0.4009 | Recon: 0.4009 | \n",
      "Epoch 530 | Loss: 0.3953 | Recon: 0.3953 | \n",
      "Epoch 531 | Loss: 0.3727 | Recon: 0.3727 | \n",
      "Epoch 532 | Loss: 0.4056 | Recon: 0.4056 | \n",
      "Epoch 533 | Loss: 0.3881 | Recon: 0.3881 | \n",
      "Epoch 534 | Loss: 0.3608 | Recon: 0.3608 | \n",
      "Epoch 535 | Loss: 0.3942 | Recon: 0.3942 | \n",
      "Epoch 536 | Loss: 0.4041 | Recon: 0.4041 | \n",
      "Epoch 537 | Loss: 0.3749 | Recon: 0.3749 | \n",
      "Epoch 538 | Loss: 0.3975 | Recon: 0.3975 | \n",
      "Epoch 539 | Loss: 0.4226 | Recon: 0.4226 | \n",
      "Epoch 540 | Loss: 0.3789 | Recon: 0.3789 | \n",
      "Epoch 541 | Loss: 0.4170 | Recon: 0.4170 | \n",
      "Epoch 542 | Loss: 0.3849 | Recon: 0.3849 | \n",
      "Epoch 543 | Loss: 0.4014 | Recon: 0.4014 | \n",
      "Epoch 544 | Loss: 0.3705 | Recon: 0.3705 | \n",
      "Epoch 545 | Loss: 0.3900 | Recon: 0.3900 | \n",
      "Epoch 546 | Loss: 0.4461 | Recon: 0.4461 | \n",
      "Epoch 547 | Loss: 0.4163 | Recon: 0.4163 | \n",
      "Epoch 548 | Loss: 0.3982 | Recon: 0.3982 | \n",
      "Epoch 549 | Loss: 0.3902 | Recon: 0.3902 | \n",
      "Epoch 550 | Loss: 0.4083 | Recon: 0.4083 | \n",
      "Epoch 551 | Loss: 0.3926 | Recon: 0.3926 | \n",
      "Epoch 552 | Loss: 0.3947 | Recon: 0.3947 | \n",
      "Epoch 553 | Loss: 0.3995 | Recon: 0.3995 | \n",
      "Epoch 554 | Loss: 0.3823 | Recon: 0.3823 | \n",
      "Epoch 555 | Loss: 0.3798 | Recon: 0.3798 | \n",
      "Epoch 556 | Loss: 0.3853 | Recon: 0.3853 | \n",
      "Epoch 557 | Loss: 0.4138 | Recon: 0.4138 | \n",
      "Epoch 558 | Loss: 0.3748 | Recon: 0.3748 | \n",
      "Epoch 559 | Loss: 0.3597 | Recon: 0.3597 | \n",
      "Epoch 560 | Loss: 0.4208 | Recon: 0.4208 | \n",
      "Epoch 561 | Loss: 0.3968 | Recon: 0.3968 | \n",
      "Epoch 562 | Loss: 0.3928 | Recon: 0.3928 | \n",
      "Epoch 563 | Loss: 0.4086 | Recon: 0.4086 | \n",
      "Epoch 564 | Loss: 0.3851 | Recon: 0.3851 | \n",
      "Epoch 565 | Loss: 0.4046 | Recon: 0.4046 | \n",
      "Epoch 566 | Loss: 0.3948 | Recon: 0.3948 | \n",
      "Epoch 567 | Loss: 0.3853 | Recon: 0.3853 | \n",
      "Epoch 568 | Loss: 0.3963 | Recon: 0.3963 | \n",
      "Epoch 569 | Loss: 0.4152 | Recon: 0.4152 | \n",
      "Epoch 570 | Loss: 0.3793 | Recon: 0.3793 | \n",
      "Epoch 571 | Loss: 0.3986 | Recon: 0.3986 | \n",
      "Epoch 572 | Loss: 0.3934 | Recon: 0.3934 | \n",
      "Epoch 573 | Loss: 0.3671 | Recon: 0.3671 | \n",
      "Epoch 574 | Loss: 0.4006 | Recon: 0.4006 | \n",
      "Epoch 575 | Loss: 0.4098 | Recon: 0.4098 | \n",
      "Epoch 576 | Loss: 0.3836 | Recon: 0.3836 | \n",
      "Epoch 577 | Loss: 0.4065 | Recon: 0.4065 | \n",
      "Epoch 578 | Loss: 0.3718 | Recon: 0.3718 | \n",
      "Epoch 579 | Loss: 0.3854 | Recon: 0.3854 | \n",
      "Epoch 580 | Loss: 0.3881 | Recon: 0.3881 | \n",
      "Epoch 581 | Loss: 0.3745 | Recon: 0.3745 | \n",
      "Epoch 582 | Loss: 0.3793 | Recon: 0.3793 | \n",
      "Epoch 583 | Loss: 0.3751 | Recon: 0.3751 | \n",
      "Epoch 584 | Loss: 0.3943 | Recon: 0.3943 | \n",
      "Epoch 585 | Loss: 0.3722 | Recon: 0.3722 | \n",
      "Epoch 586 | Loss: 0.3778 | Recon: 0.3778 | \n",
      "Epoch 587 | Loss: 0.3864 | Recon: 0.3864 | \n",
      "Epoch 588 | Loss: 0.3838 | Recon: 0.3838 | \n",
      "Epoch 589 | Loss: 0.3942 | Recon: 0.3942 | \n",
      "Epoch 590 | Loss: 0.3733 | Recon: 0.3733 | \n",
      "Epoch 591 | Loss: 0.3805 | Recon: 0.3805 | \n",
      "Epoch 592 | Loss: 0.3999 | Recon: 0.3999 | \n",
      "Epoch 593 | Loss: 0.3724 | Recon: 0.3724 | \n",
      "Epoch 594 | Loss: 0.3853 | Recon: 0.3853 | \n",
      "Epoch 595 | Loss: 0.3664 | Recon: 0.3664 | \n",
      "Epoch 596 | Loss: 0.3871 | Recon: 0.3871 | \n",
      "Epoch 597 | Loss: 0.3919 | Recon: 0.3919 | \n",
      "Epoch 598 | Loss: 0.3944 | Recon: 0.3944 | \n",
      "Epoch 599 | Loss: 0.3872 | Recon: 0.3872 | \n",
      "Epoch 600 | Loss: 0.4147 | Recon: 0.4147 | \n",
      "Epoch 601 | Loss: 0.3884 | Recon: 0.3884 | \n",
      "Epoch 602 | Loss: 0.3750 | Recon: 0.3750 | \n",
      "Epoch 603 | Loss: 0.3729 | Recon: 0.3729 | \n",
      "Epoch 604 | Loss: 0.3672 | Recon: 0.3672 | \n",
      "Epoch 605 | Loss: 0.3821 | Recon: 0.3821 | \n",
      "Epoch 606 | Loss: 0.3761 | Recon: 0.3761 | \n",
      "Epoch 607 | Loss: 0.3578 | Recon: 0.3578 | \n",
      "Epoch 608 | Loss: 0.3791 | Recon: 0.3791 | \n",
      "Epoch 609 | Loss: 0.3663 | Recon: 0.3663 | \n",
      "Epoch 610 | Loss: 0.3811 | Recon: 0.3811 | \n",
      "Epoch 611 | Loss: 0.3754 | Recon: 0.3754 | \n",
      "Epoch 612 | Loss: 0.3365 | Recon: 0.3365 | \n",
      "Epoch 613 | Loss: 0.3937 | Recon: 0.3937 | \n",
      "Epoch 614 | Loss: 0.3736 | Recon: 0.3736 | \n",
      "Epoch 615 | Loss: 0.3756 | Recon: 0.3756 | \n",
      "Epoch 616 | Loss: 0.3825 | Recon: 0.3825 | \n",
      "Epoch 617 | Loss: 0.3674 | Recon: 0.3674 | \n",
      "Epoch 618 | Loss: 0.4003 | Recon: 0.4003 | \n",
      "Epoch 619 | Loss: 0.3666 | Recon: 0.3666 | \n",
      "Epoch 620 | Loss: 0.3617 | Recon: 0.3617 | \n",
      "Epoch 621 | Loss: 0.3611 | Recon: 0.3611 | \n",
      "Epoch 622 | Loss: 0.3729 | Recon: 0.3729 | \n",
      "Epoch 623 | Loss: 0.3939 | Recon: 0.3939 | \n",
      "Epoch 624 | Loss: 0.3623 | Recon: 0.3623 | \n",
      "Epoch 625 | Loss: 0.3576 | Recon: 0.3576 | \n",
      "Epoch 626 | Loss: 0.3432 | Recon: 0.3432 | \n",
      "Epoch 627 | Loss: 0.3868 | Recon: 0.3868 | \n",
      "Epoch 628 | Loss: 0.3931 | Recon: 0.3931 | \n",
      "Epoch 629 | Loss: 0.3761 | Recon: 0.3761 | \n",
      "Epoch 630 | Loss: 0.3634 | Recon: 0.3634 | \n",
      "Epoch 631 | Loss: 0.3777 | Recon: 0.3777 | \n",
      "Epoch 632 | Loss: 0.3717 | Recon: 0.3717 | \n",
      "Epoch 633 | Loss: 0.3474 | Recon: 0.3474 | \n",
      "Epoch 634 | Loss: 0.3482 | Recon: 0.3482 | \n",
      "Epoch 635 | Loss: 0.3814 | Recon: 0.3814 | \n",
      "Epoch 636 | Loss: 0.3689 | Recon: 0.3689 | \n",
      "Epoch 637 | Loss: 0.3735 | Recon: 0.3735 | \n",
      "Epoch 638 | Loss: 0.3787 | Recon: 0.3787 | \n",
      "Epoch 639 | Loss: 0.3658 | Recon: 0.3658 | \n",
      "Epoch 640 | Loss: 0.3649 | Recon: 0.3649 | \n",
      "Epoch 641 | Loss: 0.3702 | Recon: 0.3702 | \n",
      "Epoch 642 | Loss: 0.3526 | Recon: 0.3526 | \n",
      "Epoch 643 | Loss: 0.3940 | Recon: 0.3940 | \n",
      "Epoch 644 | Loss: 0.3761 | Recon: 0.3761 | \n",
      "Epoch 645 | Loss: 0.3525 | Recon: 0.3525 | \n",
      "Epoch 646 | Loss: 0.3722 | Recon: 0.3722 | \n",
      "Epoch 647 | Loss: 0.3759 | Recon: 0.3759 | \n",
      "Epoch 648 | Loss: 0.3673 | Recon: 0.3673 | \n",
      "Epoch 649 | Loss: 0.3782 | Recon: 0.3782 | \n",
      "Epoch 650 | Loss: 0.3658 | Recon: 0.3658 | \n",
      "Epoch 651 | Loss: 0.3835 | Recon: 0.3835 | \n",
      "Epoch 652 | Loss: 0.3578 | Recon: 0.3578 | \n",
      "Epoch 653 | Loss: 0.3815 | Recon: 0.3815 | \n",
      "Epoch 654 | Loss: 0.3903 | Recon: 0.3903 | \n",
      "Epoch 655 | Loss: 0.3665 | Recon: 0.3665 | \n",
      "Epoch 656 | Loss: 0.3520 | Recon: 0.3520 | \n",
      "Epoch 657 | Loss: 0.3699 | Recon: 0.3699 | \n",
      "Epoch 658 | Loss: 0.3864 | Recon: 0.3864 | \n",
      "Epoch 659 | Loss: 0.3768 | Recon: 0.3768 | \n",
      "Epoch 660 | Loss: 0.3920 | Recon: 0.3920 | \n",
      "Epoch 661 | Loss: 0.3649 | Recon: 0.3649 | \n",
      "Epoch 662 | Loss: 0.3898 | Recon: 0.3898 | \n",
      "Epoch 663 | Loss: 0.3692 | Recon: 0.3692 | \n",
      "Epoch 664 | Loss: 0.3627 | Recon: 0.3627 | \n",
      "Epoch 665 | Loss: 0.3841 | Recon: 0.3841 | \n",
      "Epoch 666 | Loss: 0.3691 | Recon: 0.3691 | \n",
      "Epoch 667 | Loss: 0.3734 | Recon: 0.3734 | \n",
      "Epoch 668 | Loss: 0.3746 | Recon: 0.3746 | \n",
      "Epoch 669 | Loss: 0.3824 | Recon: 0.3824 | \n",
      "Epoch 670 | Loss: 0.3778 | Recon: 0.3778 | \n",
      "Epoch 671 | Loss: 0.3767 | Recon: 0.3767 | \n",
      "Epoch 672 | Loss: 0.3618 | Recon: 0.3618 | \n",
      "Epoch 673 | Loss: 0.3711 | Recon: 0.3711 | \n",
      "Epoch 674 | Loss: 0.3676 | Recon: 0.3676 | \n",
      "Epoch 675 | Loss: 0.3673 | Recon: 0.3673 | \n",
      "Epoch 676 | Loss: 0.3718 | Recon: 0.3718 | \n",
      "Epoch 677 | Loss: 0.3771 | Recon: 0.3771 | \n",
      "Epoch 678 | Loss: 0.3754 | Recon: 0.3754 | \n",
      "Epoch 679 | Loss: 0.3659 | Recon: 0.3659 | \n",
      "Epoch 680 | Loss: 0.3472 | Recon: 0.3472 | \n",
      "Epoch 681 | Loss: 0.3927 | Recon: 0.3927 | \n",
      "Epoch 682 | Loss: 0.3684 | Recon: 0.3684 | \n",
      "Epoch 683 | Loss: 0.3604 | Recon: 0.3604 | \n",
      "Epoch 684 | Loss: 0.3560 | Recon: 0.3560 | \n",
      "Epoch 685 | Loss: 0.3772 | Recon: 0.3772 | \n",
      "Epoch 686 | Loss: 0.3550 | Recon: 0.3550 | \n",
      "Epoch 687 | Loss: 0.3639 | Recon: 0.3639 | \n",
      "Epoch 688 | Loss: 0.3721 | Recon: 0.3721 | \n",
      "Epoch 689 | Loss: 0.3651 | Recon: 0.3651 | \n",
      "Epoch 690 | Loss: 0.3766 | Recon: 0.3766 | \n",
      "Epoch 691 | Loss: 0.3687 | Recon: 0.3687 | \n",
      "Epoch 692 | Loss: 0.3617 | Recon: 0.3617 | \n",
      "Epoch 693 | Loss: 0.3841 | Recon: 0.3841 | \n",
      "Epoch 694 | Loss: 0.3609 | Recon: 0.3609 | \n",
      "Epoch 695 | Loss: 0.3592 | Recon: 0.3592 | \n",
      "Epoch 696 | Loss: 0.3722 | Recon: 0.3722 | \n",
      "Epoch 697 | Loss: 0.3610 | Recon: 0.3610 | \n",
      "Epoch 698 | Loss: 0.3388 | Recon: 0.3388 | \n",
      "Epoch 699 | Loss: 0.3624 | Recon: 0.3624 | \n",
      "Epoch 700 | Loss: 0.3577 | Recon: 0.3577 | \n",
      "Epoch 701 | Loss: 0.3738 | Recon: 0.3738 | \n",
      "Epoch 702 | Loss: 0.3582 | Recon: 0.3582 | \n",
      "Epoch 703 | Loss: 0.3547 | Recon: 0.3547 | \n",
      "Epoch 704 | Loss: 0.3782 | Recon: 0.3782 | \n",
      "Epoch 705 | Loss: 0.3890 | Recon: 0.3890 | \n",
      "Epoch 706 | Loss: 0.3707 | Recon: 0.3707 | \n",
      "Epoch 707 | Loss: 0.3482 | Recon: 0.3482 | \n",
      "Epoch 708 | Loss: 0.3588 | Recon: 0.3588 | \n",
      "Epoch 709 | Loss: 0.3312 | Recon: 0.3312 | \n",
      "Epoch 710 | Loss: 0.3466 | Recon: 0.3466 | \n",
      "Epoch 711 | Loss: 0.3563 | Recon: 0.3563 | \n",
      "Epoch 712 | Loss: 0.3809 | Recon: 0.3809 | \n",
      "Epoch 713 | Loss: 0.3588 | Recon: 0.3588 | \n",
      "Epoch 714 | Loss: 0.3668 | Recon: 0.3668 | \n",
      "Epoch 715 | Loss: 0.3665 | Recon: 0.3665 | \n",
      "Epoch 716 | Loss: 0.3614 | Recon: 0.3614 | \n",
      "Epoch 717 | Loss: 0.3620 | Recon: 0.3620 | \n",
      "Epoch 718 | Loss: 0.3623 | Recon: 0.3623 | \n",
      "Epoch 719 | Loss: 0.3633 | Recon: 0.3633 | \n",
      "Epoch 720 | Loss: 0.3436 | Recon: 0.3436 | \n",
      "Epoch 721 | Loss: 0.3661 | Recon: 0.3661 | \n",
      "Epoch 722 | Loss: 0.3539 | Recon: 0.3539 | \n",
      "Epoch 723 | Loss: 0.3470 | Recon: 0.3470 | \n",
      "Epoch 724 | Loss: 0.3674 | Recon: 0.3674 | \n",
      "Epoch 725 | Loss: 0.3619 | Recon: 0.3619 | \n",
      "Epoch 726 | Loss: 0.3668 | Recon: 0.3668 | \n",
      "Epoch 727 | Loss: 0.3580 | Recon: 0.3580 | \n",
      "Epoch 728 | Loss: 0.3689 | Recon: 0.3689 | \n",
      "Epoch 729 | Loss: 0.3521 | Recon: 0.3521 | \n",
      "Epoch 730 | Loss: 0.3618 | Recon: 0.3618 | \n",
      "Epoch 731 | Loss: 0.3500 | Recon: 0.3500 | \n",
      "Epoch 732 | Loss: 0.3530 | Recon: 0.3530 | \n",
      "Epoch 733 | Loss: 0.3640 | Recon: 0.3640 | \n",
      "Epoch 734 | Loss: 0.3489 | Recon: 0.3489 | \n",
      "Epoch 735 | Loss: 0.3465 | Recon: 0.3465 | \n",
      "Epoch 736 | Loss: 0.3677 | Recon: 0.3677 | \n",
      "Epoch 737 | Loss: 0.3512 | Recon: 0.3512 | \n",
      "Epoch 738 | Loss: 0.3626 | Recon: 0.3626 | \n",
      "Epoch 739 | Loss: 0.3542 | Recon: 0.3542 | \n",
      "Epoch 740 | Loss: 0.3500 | Recon: 0.3500 | \n",
      "Epoch 741 | Loss: 0.3432 | Recon: 0.3432 | \n",
      "Epoch 742 | Loss: 0.3766 | Recon: 0.3766 | \n",
      "Epoch 743 | Loss: 0.3666 | Recon: 0.3666 | \n",
      "Epoch 744 | Loss: 0.3614 | Recon: 0.3614 | \n",
      "Epoch 745 | Loss: 0.3575 | Recon: 0.3575 | \n",
      "Epoch 746 | Loss: 0.3469 | Recon: 0.3469 | \n",
      "Epoch 747 | Loss: 0.3596 | Recon: 0.3596 | \n",
      "Epoch 748 | Loss: 0.3622 | Recon: 0.3622 | \n",
      "Epoch 749 | Loss: 0.3583 | Recon: 0.3583 | \n",
      "Epoch 750 | Loss: 0.3463 | Recon: 0.3463 | \n",
      "Epoch 751 | Loss: 0.3633 | Recon: 0.3633 | \n",
      "Epoch 752 | Loss: 0.3750 | Recon: 0.3750 | \n",
      "Epoch 753 | Loss: 0.3551 | Recon: 0.3551 | \n",
      "Epoch 754 | Loss: 0.3602 | Recon: 0.3602 | \n",
      "Epoch 755 | Loss: 0.3513 | Recon: 0.3513 | \n",
      "Epoch 756 | Loss: 0.3581 | Recon: 0.3581 | \n",
      "Epoch 757 | Loss: 0.3597 | Recon: 0.3597 | \n",
      "Epoch 758 | Loss: 0.3641 | Recon: 0.3641 | \n",
      "Epoch 759 | Loss: 0.3543 | Recon: 0.3543 | \n",
      "Epoch 760 | Loss: 0.3593 | Recon: 0.3593 | \n",
      "Epoch 761 | Loss: 0.3525 | Recon: 0.3525 | \n",
      "Epoch 762 | Loss: 0.3492 | Recon: 0.3492 | \n",
      "Epoch 763 | Loss: 0.3548 | Recon: 0.3548 | \n",
      "Epoch 764 | Loss: 0.3563 | Recon: 0.3563 | \n",
      "Epoch 765 | Loss: 0.3623 | Recon: 0.3623 | \n",
      "Epoch 766 | Loss: 0.3636 | Recon: 0.3636 | \n",
      "Epoch 767 | Loss: 0.3651 | Recon: 0.3651 | \n",
      "Epoch 768 | Loss: 0.3482 | Recon: 0.3482 | \n",
      "Epoch 769 | Loss: 0.3400 | Recon: 0.3400 | \n",
      "Epoch 770 | Loss: 0.3634 | Recon: 0.3634 | \n",
      "Epoch 771 | Loss: 0.3580 | Recon: 0.3580 | \n",
      "Epoch 772 | Loss: 0.3828 | Recon: 0.3828 | \n",
      "Epoch 773 | Loss: 0.3638 | Recon: 0.3638 | \n",
      "Epoch 774 | Loss: 0.3608 | Recon: 0.3608 | \n",
      "Epoch 775 | Loss: 0.3491 | Recon: 0.3491 | \n",
      "Epoch 776 | Loss: 0.3702 | Recon: 0.3702 | \n",
      "Epoch 777 | Loss: 0.3418 | Recon: 0.3418 | \n",
      "Epoch 778 | Loss: 0.3432 | Recon: 0.3432 | \n",
      "Epoch 779 | Loss: 0.3560 | Recon: 0.3560 | \n",
      "Epoch 780 | Loss: 0.3577 | Recon: 0.3577 | \n",
      "Epoch 781 | Loss: 0.3587 | Recon: 0.3587 | \n",
      "Epoch 782 | Loss: 0.3561 | Recon: 0.3561 | \n",
      "Epoch 783 | Loss: 0.3653 | Recon: 0.3653 | \n",
      "Epoch 784 | Loss: 0.3457 | Recon: 0.3457 | \n",
      "Epoch 785 | Loss: 0.3476 | Recon: 0.3476 | \n",
      "Epoch 786 | Loss: 0.3620 | Recon: 0.3620 | \n",
      "Epoch 787 | Loss: 0.3657 | Recon: 0.3657 | \n",
      "Epoch 788 | Loss: 0.3545 | Recon: 0.3545 | \n",
      "Epoch 789 | Loss: 0.3581 | Recon: 0.3581 | \n",
      "Epoch 790 | Loss: 0.3604 | Recon: 0.3604 | \n",
      "Epoch 791 | Loss: 0.3515 | Recon: 0.3515 | \n",
      "Epoch 792 | Loss: 0.3510 | Recon: 0.3510 | \n",
      "Epoch 793 | Loss: 0.3542 | Recon: 0.3542 | \n",
      "Epoch 794 | Loss: 0.3632 | Recon: 0.3632 | \n",
      "Epoch 795 | Loss: 0.3792 | Recon: 0.3792 | \n",
      "Epoch 796 | Loss: 0.3587 | Recon: 0.3587 | \n",
      "Epoch 797 | Loss: 0.3408 | Recon: 0.3408 | \n",
      "Epoch 798 | Loss: 0.3566 | Recon: 0.3566 | \n",
      "Epoch 799 | Loss: 0.3646 | Recon: 0.3646 | \n",
      "Epoch 800 | Loss: 0.3548 | Recon: 0.3548 | \n",
      "Epoch 801 | Loss: 0.3535 | Recon: 0.3535 | \n",
      "Epoch 802 | Loss: 0.3419 | Recon: 0.3419 | \n",
      "Epoch 803 | Loss: 0.3416 | Recon: 0.3416 | \n",
      "Epoch 804 | Loss: 0.3546 | Recon: 0.3546 | \n",
      "Epoch 805 | Loss: 0.3565 | Recon: 0.3565 | \n",
      "Epoch 806 | Loss: 0.3362 | Recon: 0.3362 | \n",
      "Epoch 807 | Loss: 0.3568 | Recon: 0.3568 | \n",
      "Epoch 808 | Loss: 0.3579 | Recon: 0.3579 | \n",
      "Epoch 809 | Loss: 0.3635 | Recon: 0.3635 | \n",
      "Epoch 810 | Loss: 0.3478 | Recon: 0.3478 | \n",
      "Epoch 811 | Loss: 0.3608 | Recon: 0.3608 | \n",
      "Epoch 812 | Loss: 0.3637 | Recon: 0.3637 | \n",
      "Epoch 813 | Loss: 0.3441 | Recon: 0.3441 | \n",
      "Epoch 814 | Loss: 0.3512 | Recon: 0.3512 | \n",
      "Epoch 815 | Loss: 0.3506 | Recon: 0.3506 | \n",
      "Epoch 816 | Loss: 0.3488 | Recon: 0.3488 | \n",
      "Epoch 817 | Loss: 0.3442 | Recon: 0.3442 | \n",
      "Epoch 818 | Loss: 0.3450 | Recon: 0.3450 | \n",
      "Epoch 819 | Loss: 0.3477 | Recon: 0.3477 | \n",
      "Epoch 820 | Loss: 0.3597 | Recon: 0.3597 | \n",
      "Epoch 821 | Loss: 0.3511 | Recon: 0.3511 | \n",
      "Epoch 822 | Loss: 0.3635 | Recon: 0.3635 | \n",
      "Epoch 823 | Loss: 0.3473 | Recon: 0.3473 | \n",
      "Epoch 824 | Loss: 0.3501 | Recon: 0.3501 | \n",
      "Epoch 825 | Loss: 0.3516 | Recon: 0.3516 | \n",
      "Epoch 826 | Loss: 0.3550 | Recon: 0.3550 | \n",
      "Epoch 827 | Loss: 0.3568 | Recon: 0.3568 | \n",
      "Epoch 828 | Loss: 0.3598 | Recon: 0.3598 | \n",
      "Epoch 829 | Loss: 0.3568 | Recon: 0.3568 | \n",
      "Epoch 830 | Loss: 0.3526 | Recon: 0.3526 | \n",
      "Epoch 831 | Loss: 0.3601 | Recon: 0.3601 | \n",
      "Epoch 832 | Loss: 0.3472 | Recon: 0.3472 | \n",
      "Epoch 833 | Loss: 0.3568 | Recon: 0.3568 | \n",
      "Epoch 834 | Loss: 0.3564 | Recon: 0.3564 | \n",
      "Epoch 835 | Loss: 0.3595 | Recon: 0.3595 | \n",
      "Epoch 836 | Loss: 0.3642 | Recon: 0.3642 | \n",
      "Epoch 837 | Loss: 0.3702 | Recon: 0.3702 | \n",
      "Epoch 838 | Loss: 0.3505 | Recon: 0.3505 | \n",
      "Epoch 839 | Loss: 0.3404 | Recon: 0.3404 | \n",
      "Epoch 840 | Loss: 0.3504 | Recon: 0.3504 | \n",
      "Epoch 841 | Loss: 0.3541 | Recon: 0.3541 | \n",
      "Epoch 842 | Loss: 0.3610 | Recon: 0.3610 | \n",
      "Epoch 843 | Loss: 0.3432 | Recon: 0.3432 | \n",
      "Epoch 844 | Loss: 0.3441 | Recon: 0.3441 | \n",
      "Epoch 845 | Loss: 0.3527 | Recon: 0.3527 | \n",
      "Epoch 846 | Loss: 0.3444 | Recon: 0.3444 | \n",
      "Epoch 847 | Loss: 0.3542 | Recon: 0.3542 | \n",
      "Epoch 848 | Loss: 0.3560 | Recon: 0.3560 | \n",
      "Epoch 849 | Loss: 0.3445 | Recon: 0.3445 | \n",
      "Epoch 850 | Loss: 0.3494 | Recon: 0.3494 | \n",
      "Epoch 851 | Loss: 0.3490 | Recon: 0.3490 | \n",
      "Epoch 852 | Loss: 0.3464 | Recon: 0.3464 | \n",
      "Epoch 853 | Loss: 0.3449 | Recon: 0.3449 | \n",
      "Epoch 854 | Loss: 0.3527 | Recon: 0.3527 | \n",
      "Epoch 855 | Loss: 0.3395 | Recon: 0.3395 | \n",
      "Epoch 856 | Loss: 0.3658 | Recon: 0.3658 | \n",
      "Epoch 857 | Loss: 0.3490 | Recon: 0.3490 | \n",
      "Epoch 858 | Loss: 0.3718 | Recon: 0.3718 | \n",
      "Epoch 859 | Loss: 0.3454 | Recon: 0.3454 | \n",
      "Epoch 860 | Loss: 0.3510 | Recon: 0.3510 | \n",
      "Epoch 861 | Loss: 0.3524 | Recon: 0.3524 | \n",
      "Epoch 862 | Loss: 0.3350 | Recon: 0.3350 | \n",
      "Epoch 863 | Loss: 0.3466 | Recon: 0.3466 | \n",
      "Epoch 864 | Loss: 0.3402 | Recon: 0.3402 | \n",
      "Epoch 865 | Loss: 0.3439 | Recon: 0.3439 | \n",
      "Epoch 866 | Loss: 0.3455 | Recon: 0.3455 | \n",
      "Epoch 867 | Loss: 0.3483 | Recon: 0.3483 | \n",
      "Epoch 868 | Loss: 0.3404 | Recon: 0.3404 | \n",
      "Epoch 869 | Loss: 0.3595 | Recon: 0.3595 | \n",
      "Epoch 870 | Loss: 0.3589 | Recon: 0.3589 | \n",
      "Epoch 871 | Loss: 0.3572 | Recon: 0.3572 | \n",
      "Epoch 872 | Loss: 0.3454 | Recon: 0.3454 | \n",
      "Epoch 873 | Loss: 0.3415 | Recon: 0.3415 | \n",
      "Epoch 874 | Loss: 0.3491 | Recon: 0.3491 | \n",
      "Epoch 875 | Loss: 0.3443 | Recon: 0.3443 | \n",
      "Epoch 876 | Loss: 0.3499 | Recon: 0.3499 | \n",
      "Epoch 877 | Loss: 0.3532 | Recon: 0.3532 | \n",
      "Epoch 878 | Loss: 0.3515 | Recon: 0.3515 | \n",
      "Epoch 879 | Loss: 0.3699 | Recon: 0.3699 | \n",
      "Epoch 880 | Loss: 0.3559 | Recon: 0.3559 | \n",
      "Epoch 881 | Loss: 0.3601 | Recon: 0.3601 | \n",
      "Epoch 882 | Loss: 0.3453 | Recon: 0.3453 | \n",
      "Epoch 883 | Loss: 0.3525 | Recon: 0.3525 | \n",
      "Epoch 884 | Loss: 0.3432 | Recon: 0.3432 | \n",
      "Epoch 885 | Loss: 0.3544 | Recon: 0.3544 | \n",
      "Epoch 886 | Loss: 0.3429 | Recon: 0.3429 | \n",
      "Epoch 887 | Loss: 0.3439 | Recon: 0.3439 | \n",
      "Epoch 888 | Loss: 0.3444 | Recon: 0.3444 | \n",
      "Epoch 889 | Loss: 0.3453 | Recon: 0.3453 | \n",
      "Epoch 890 | Loss: 0.3419 | Recon: 0.3419 | \n",
      "Epoch 891 | Loss: 0.3494 | Recon: 0.3494 | \n",
      "Epoch 892 | Loss: 0.3402 | Recon: 0.3402 | \n",
      "Epoch 893 | Loss: 0.3569 | Recon: 0.3569 | \n",
      "Epoch 894 | Loss: 0.3380 | Recon: 0.3380 | \n",
      "Epoch 895 | Loss: 0.3606 | Recon: 0.3606 | \n",
      "Epoch 896 | Loss: 0.3527 | Recon: 0.3527 | \n",
      "Epoch 897 | Loss: 0.3516 | Recon: 0.3516 | \n",
      "Epoch 898 | Loss: 0.3554 | Recon: 0.3554 | \n",
      "Epoch 899 | Loss: 0.3606 | Recon: 0.3606 | \n",
      "Epoch 900 | Loss: 0.3467 | Recon: 0.3467 | \n",
      "Epoch 901 | Loss: 0.3432 | Recon: 0.3432 | \n",
      "Epoch 902 | Loss: 0.3491 | Recon: 0.3491 | \n",
      "Epoch 903 | Loss: 0.3540 | Recon: 0.3540 | \n",
      "Epoch 904 | Loss: 0.3528 | Recon: 0.3528 | \n",
      "Epoch 905 | Loss: 0.3404 | Recon: 0.3404 | \n",
      "Epoch 906 | Loss: 0.3511 | Recon: 0.3511 | \n",
      "Epoch 907 | Loss: 0.3587 | Recon: 0.3587 | \n",
      "Epoch 908 | Loss: 0.3472 | Recon: 0.3472 | \n",
      "Epoch 909 | Loss: 0.3407 | Recon: 0.3407 | \n",
      "Epoch 910 | Loss: 0.3564 | Recon: 0.3564 | \n",
      "Epoch 911 | Loss: 0.3499 | Recon: 0.3499 | \n",
      "Epoch 912 | Loss: 0.3501 | Recon: 0.3501 | \n",
      "Epoch 913 | Loss: 0.3442 | Recon: 0.3442 | \n",
      "Epoch 914 | Loss: 0.3559 | Recon: 0.3559 | \n",
      "Epoch 915 | Loss: 0.3448 | Recon: 0.3448 | \n",
      "Epoch 916 | Loss: 0.3350 | Recon: 0.3350 | \n",
      "Epoch 917 | Loss: 0.3435 | Recon: 0.3435 | \n",
      "Epoch 918 | Loss: 0.3496 | Recon: 0.3496 | \n",
      "Epoch 919 | Loss: 0.3513 | Recon: 0.3513 | \n",
      "Epoch 920 | Loss: 0.3435 | Recon: 0.3435 | \n",
      "Epoch 921 | Loss: 0.3531 | Recon: 0.3531 | \n",
      "Epoch 922 | Loss: 0.3378 | Recon: 0.3378 | \n",
      "Epoch 923 | Loss: 0.3416 | Recon: 0.3416 | \n",
      "Epoch 924 | Loss: 0.3484 | Recon: 0.3484 | \n",
      "Epoch 925 | Loss: 0.3425 | Recon: 0.3425 | \n",
      "Epoch 926 | Loss: 0.3418 | Recon: 0.3418 | \n",
      "Epoch 927 | Loss: 0.3438 | Recon: 0.3438 | \n",
      "Epoch 928 | Loss: 0.3467 | Recon: 0.3467 | \n",
      "Epoch 929 | Loss: 0.3448 | Recon: 0.3448 | \n",
      "Epoch 930 | Loss: 0.3569 | Recon: 0.3569 | \n",
      "Epoch 931 | Loss: 0.3527 | Recon: 0.3527 | \n",
      "Epoch 932 | Loss: 0.3374 | Recon: 0.3374 | \n",
      "Epoch 933 | Loss: 0.3513 | Recon: 0.3513 | \n",
      "Epoch 934 | Loss: 0.3456 | Recon: 0.3456 | \n",
      "Epoch 935 | Loss: 0.3598 | Recon: 0.3598 | \n",
      "Epoch 936 | Loss: 0.3405 | Recon: 0.3405 | \n",
      "Epoch 937 | Loss: 0.3500 | Recon: 0.3500 | \n",
      "Epoch 938 | Loss: 0.3421 | Recon: 0.3421 | \n",
      "Epoch 939 | Loss: 0.3536 | Recon: 0.3536 | \n",
      "Epoch 940 | Loss: 0.3541 | Recon: 0.3541 | \n",
      "Epoch 941 | Loss: 0.3491 | Recon: 0.3491 | \n",
      "Epoch 942 | Loss: 0.3437 | Recon: 0.3437 | \n",
      "Epoch 943 | Loss: 0.3500 | Recon: 0.3500 | \n",
      "Epoch 944 | Loss: 0.3534 | Recon: 0.3534 | \n",
      "Epoch 945 | Loss: 0.3478 | Recon: 0.3478 | \n",
      "Epoch 946 | Loss: 0.3390 | Recon: 0.3390 | \n",
      "Epoch 947 | Loss: 0.3485 | Recon: 0.3485 | \n",
      "Epoch 948 | Loss: 0.3510 | Recon: 0.3510 | \n",
      "Epoch 949 | Loss: 0.3434 | Recon: 0.3434 | \n",
      "Epoch 950 | Loss: 0.3511 | Recon: 0.3511 | \n",
      "Epoch 951 | Loss: 0.3471 | Recon: 0.3471 | \n",
      "Epoch 952 | Loss: 0.3413 | Recon: 0.3413 | \n",
      "Epoch 953 | Loss: 0.3602 | Recon: 0.3602 | \n",
      "Epoch 954 | Loss: 0.3603 | Recon: 0.3603 | \n",
      "Epoch 955 | Loss: 0.3502 | Recon: 0.3502 | \n",
      "Epoch 956 | Loss: 0.3497 | Recon: 0.3497 | \n",
      "Epoch 957 | Loss: 0.3512 | Recon: 0.3512 | \n",
      "Epoch 958 | Loss: 0.3427 | Recon: 0.3427 | \n",
      "Epoch 959 | Loss: 0.3453 | Recon: 0.3453 | \n",
      "Epoch 960 | Loss: 0.3494 | Recon: 0.3494 | \n",
      "Epoch 961 | Loss: 0.3416 | Recon: 0.3416 | \n",
      "Epoch 962 | Loss: 0.3586 | Recon: 0.3586 | \n",
      "Epoch 963 | Loss: 0.3511 | Recon: 0.3511 | \n",
      "Epoch 964 | Loss: 0.3420 | Recon: 0.3420 | \n",
      "Epoch 965 | Loss: 0.3487 | Recon: 0.3487 | \n",
      "Epoch 966 | Loss: 0.3546 | Recon: 0.3546 | \n",
      "Epoch 967 | Loss: 0.3388 | Recon: 0.3388 | \n",
      "Epoch 968 | Loss: 0.3364 | Recon: 0.3364 | \n",
      "Epoch 969 | Loss: 0.3520 | Recon: 0.3520 | \n",
      "Epoch 970 | Loss: 0.3388 | Recon: 0.3388 | \n",
      "Epoch 971 | Loss: 0.3425 | Recon: 0.3425 | \n",
      "Epoch 972 | Loss: 0.3491 | Recon: 0.3491 | \n",
      "Epoch 973 | Loss: 0.3361 | Recon: 0.3361 | \n",
      "Epoch 974 | Loss: 0.3455 | Recon: 0.3455 | \n",
      "Epoch 975 | Loss: 0.3492 | Recon: 0.3492 | \n",
      "Epoch 976 | Loss: 0.3444 | Recon: 0.3444 | \n",
      "Epoch 977 | Loss: 0.3515 | Recon: 0.3515 | \n",
      "Epoch 978 | Loss: 0.3458 | Recon: 0.3458 | \n",
      "Epoch 979 | Loss: 0.3408 | Recon: 0.3408 | \n",
      "Epoch 980 | Loss: 0.3445 | Recon: 0.3445 | \n",
      "Epoch 981 | Loss: 0.3407 | Recon: 0.3407 | \n",
      "Epoch 982 | Loss: 0.3415 | Recon: 0.3415 | \n",
      "Epoch 983 | Loss: 0.3521 | Recon: 0.3521 | \n",
      "Epoch 984 | Loss: 0.3443 | Recon: 0.3443 | \n",
      "Epoch 985 | Loss: 0.3515 | Recon: 0.3515 | \n",
      "Epoch 986 | Loss: 0.3443 | Recon: 0.3443 | \n",
      "Epoch 987 | Loss: 0.3442 | Recon: 0.3442 | \n",
      "Epoch 988 | Loss: 0.3478 | Recon: 0.3478 | \n",
      "Epoch 989 | Loss: 0.3445 | Recon: 0.3445 | \n",
      "Epoch 990 | Loss: 0.3594 | Recon: 0.3594 | \n",
      "Epoch 991 | Loss: 0.3411 | Recon: 0.3411 | \n",
      "Epoch 992 | Loss: 0.3310 | Recon: 0.3310 | \n",
      "Epoch 993 | Loss: 0.3323 | Recon: 0.3323 | \n",
      "Epoch 994 | Loss: 0.3474 | Recon: 0.3474 | \n",
      "Epoch 995 | Loss: 0.3437 | Recon: 0.3437 | \n",
      "Epoch 996 | Loss: 0.3368 | Recon: 0.3368 | \n",
      "Epoch 997 | Loss: 0.3465 | Recon: 0.3465 | \n",
      "Epoch 998 | Loss: 0.3351 | Recon: 0.3351 | \n",
      "Epoch 999 | Loss: 0.3448 | Recon: 0.3448 | \n",
      "Epoch 1000 | Loss: 0.3384 | Recon: 0.3384 | \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vae = MaskedVAE(input_dim=4, hidden_dim=4, latent_dim=4)\n",
    "vae = train_masked_vae(vae, dataloader, device,epochs=1000,lr=1e-3)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code ran for 7.68 seconds\n"
     ]
    }
   ],
   "source": [
    "elapsed = end_time - start_time\n",
    "print(f\"Code ran for {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = extract_latent_features(vae, X_scaled,mask_tensor,device)\n",
    "latent_np = latent.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.16801834e+00,  1.01500216e+01, -5.59021807e+00,\n",
       "        -2.36622143e+00],\n",
       "       [ 8.58580112e+00,  9.47179508e+00, -5.20281649e+00,\n",
       "        -2.19520760e+00],\n",
       "       [ 9.53030586e+00,  1.05721827e+01, -5.83131647e+00,\n",
       "        -2.47265291e+00],\n",
       "       [ 9.28427315e+00,  1.02860813e+01, -5.66782141e+00,\n",
       "        -2.40036058e+00],\n",
       "       [ 9.57657528e+00,  1.06261225e+01, -5.86213112e+00,\n",
       "        -2.48622799e+00],\n",
       "       [ 8.56329250e+00,  9.44621086e+00, -5.18813229e+00,\n",
       "        -2.18853235e+00],\n",
       "       [ 9.81326771e+00,  1.09023275e+01, -6.01981211e+00,\n",
       "        -2.55574560e+00],\n",
       "       [ 9.04937649e+00,  1.00121031e+01, -5.51139450e+00,\n",
       "        -2.33136058e+00],\n",
       "       [ 9.42915821e+00,  1.04547939e+01, -5.76418924e+00,\n",
       "        -2.44293261e+00],\n",
       "       [ 8.85642433e+00,  9.78722191e+00, -5.38295603e+00,\n",
       "        -2.27469516e+00],\n",
       "       [ 8.81447983e+00,  9.73812389e+00, -5.35496759e+00,\n",
       "        -2.26236033e+00],\n",
       "       [ 9.33929253e+00,  1.03502874e+01, -5.70448542e+00,\n",
       "        -2.41650653e+00],\n",
       "       [ 8.99256134e+00,  9.94567013e+00, -5.47347593e+00,\n",
       "        -2.31469703e+00],\n",
       "       [ 1.04179916e+01,  1.16061363e+01, -6.42189693e+00,\n",
       "        -2.73343396e+00],\n",
       "       [ 8.96175194e+00,  9.90869331e+00, -5.45255566e+00,\n",
       "        -2.30565381e+00],\n",
       "       [ 9.19163704e+00,  1.01776190e+01, -5.60599566e+00,\n",
       "        -2.37312007e+00],\n",
       "       [ 9.07285023e+00,  1.00389414e+01, -5.52681875e+00,\n",
       "        -2.33825731e+00],\n",
       "       [ 8.96990967e+00,  9.91932869e+00, -5.45843935e+00,\n",
       "        -2.30802059e+00],\n",
       "       [ 7.93554068e+00,  8.71443462e+00, -4.77023506e+00,\n",
       "        -2.00414920e+00],\n",
       "       [ 9.44223309e+00,  1.04699011e+01, -5.77286530e+00,\n",
       "        -2.44674373e+00],\n",
       "       [ 7.95998812e+00,  8.74300385e+00, -4.78652668e+00,\n",
       "        -2.01134324e+00],\n",
       "       [ 9.04421997e+00,  1.00062904e+01, -5.50805378e+00,\n",
       "        -2.32982469e+00],\n",
       "       [ 1.09207430e+01,  1.21915884e+01, -6.75634289e+00,\n",
       "        -2.88110805e+00],\n",
       "       [ 7.79171371e+00,  8.54755688e+00, -4.67480087e+00,\n",
       "        -1.96188831e+00],\n",
       "       [ 8.95712376e+00,  9.90573883e+00, -5.45047045e+00,\n",
       "        -2.30421281e+00],\n",
       "       [ 8.12236881e+00,  8.93224621e+00, -4.89459229e+00,\n",
       "        -2.05905652e+00],\n",
       "       [ 8.52576923e+00,  9.40253448e+00, -5.16316509e+00,\n",
       "        -2.17752719e+00],\n",
       "       [ 8.83197689e+00,  9.75865459e+00, -5.36666489e+00,\n",
       "        -2.26750135e+00],\n",
       "       [ 8.75946140e+00,  9.67391872e+00, -5.31830406e+00,\n",
       "        -2.24621439e+00],\n",
       "       [ 9.14813614e+00,  1.01276350e+01, -5.57730150e+00,\n",
       "        -2.36035919e+00],\n",
       "       [ 8.73957825e+00,  9.65153122e+00, -5.30538654e+00,\n",
       "        -2.24035168e+00],\n",
       "       [ 7.81854916e+00,  8.57798386e+00, -4.69231224e+00,\n",
       "        -1.96980405e+00],\n",
       "       [ 1.02295141e+01,  1.13868599e+01, -6.29664040e+00,\n",
       "        -2.67801142e+00],\n",
       "       [ 9.73273945e+00,  1.08077154e+01, -5.96592140e+00,\n",
       "        -2.53209376e+00],\n",
       "       [ 8.65831566e+00,  9.55653000e+00, -5.25117731e+00,\n",
       "        -2.21649456e+00],\n",
       "       [ 9.03173542e+00,  9.99081230e+00, -5.49934340e+00,\n",
       "        -2.32621741e+00],\n",
       "       [ 8.46079826e+00,  9.32546902e+00, -5.11936474e+00,\n",
       "        -2.15849757e+00],\n",
       "       [ 9.98333740e+00,  1.10999994e+01, -6.13279200e+00,\n",
       "        -2.60571766e+00],\n",
       "       [ 9.75645161e+00,  1.08358946e+01, -5.98189402e+00,\n",
       "        -2.53908229e+00],\n",
       "       [ 8.84072495e+00,  9.76891994e+00, -5.37251329e+00,\n",
       "        -2.27007174e+00],\n",
       "       [ 9.30595112e+00,  1.03106947e+01, -5.68199158e+00,\n",
       "        -2.40674043e+00],\n",
       "       [ 7.95035887e+00,  8.73158836e+00, -4.78000450e+00,\n",
       "        -2.00856519e+00],\n",
       "       [ 1.01562605e+01,  1.13017321e+01, -6.24795961e+00,\n",
       "        -2.65651870e+00],\n",
       "       [ 8.32945633e+00,  9.17406940e+00, -5.03264093e+00,\n",
       "        -2.11984396e+00],\n",
       "       [ 8.73456764e+00,  9.64647865e+00, -5.30240059e+00,\n",
       "        -2.23881817e+00],\n",
       "       [ 8.59634304e+00,  9.48428440e+00, -5.20991802e+00,\n",
       "        -2.19829488e+00],\n",
       "       [ 9.51295471e+00,  1.05524120e+01, -5.81997299e+00,\n",
       "        -2.46751380e+00],\n",
       "       [ 9.61156750e+00,  1.06671829e+01, -5.88552570e+00,\n",
       "        -2.49651003e+00],\n",
       "       [ 9.02313232e+00,  9.98130608e+00, -5.49384785e+00,\n",
       "        -2.32364869e+00],\n",
       "       [ 8.97686100e+00,  9.92736721e+00, -5.46303320e+00,\n",
       "        -2.31007338e+00],\n",
       "       [-1.47650933e+00, -1.68907285e+00,  1.70199943e+00,\n",
       "         7.09076583e-01],\n",
       "       [-1.15845704e+00, -1.39169705e+00,  1.45293927e+00,\n",
       "         6.01413310e-01],\n",
       "       [-1.60261762e+00, -1.80460298e+00,  1.80116081e+00,\n",
       "         7.51635194e-01],\n",
       "       [-4.66367155e-01, -8.49435806e-01,  9.49795246e-01,\n",
       "         4.11289573e-01],\n",
       "       [-1.27231848e+00, -1.47959018e+00,  1.53579593e+00,\n",
       "         6.32386148e-01],\n",
       "       [-6.65836930e-01, -1.00533950e+00,  1.09438133e+00,\n",
       "         4.66125786e-01],\n",
       "       [-1.21138108e+00, -1.43276548e+00,  1.49127221e+00,\n",
       "         6.15907371e-01],\n",
       "       [ 3.38110423e+00,  3.41327143e+00, -1.74162984e+00,\n",
       "        -6.66097641e-01],\n",
       "       [-1.21404922e+00, -1.43452787e+00,  1.49346590e+00,\n",
       "         6.16497159e-01],\n",
       "       [ 1.00119507e+00,  7.37026811e-01, -1.52750820e-01,\n",
       "         2.71417648e-02],\n",
       "       [ 2.01267028e+00,  1.82079744e+00, -8.31849396e-01,\n",
       "        -2.63966322e-01],\n",
       "       [-7.47681022e-01, -1.07016039e+00,  1.15376544e+00,\n",
       "         4.88791466e-01],\n",
       "       [-6.87066793e-01, -1.02275002e+00,  1.11135793e+00,\n",
       "         4.71682847e-01],\n",
       "       [-1.06550097e+00, -1.31815040e+00,  1.38559651e+00,\n",
       "         5.75656593e-01],\n",
       "       [ 1.10398495e+00,  8.42484474e-01, -2.22817957e-01,\n",
       "        -1.86228752e-03],\n",
       "       [-1.22624505e+00, -1.44475424e+00,  1.50222945e+00,\n",
       "         6.20020866e-01],\n",
       "       [-6.16566300e-01, -9.66228843e-01,  1.05782616e+00,\n",
       "         4.52692688e-01],\n",
       "       [-2.26449102e-01, -6.49138510e-01,  7.71810770e-01,\n",
       "         3.46538067e-01],\n",
       "       [-1.22256339e+00, -1.43459558e+00,  1.49968648e+00,\n",
       "         6.17410481e-01],\n",
       "       [ 4.78404164e-02, -3.09027612e-01,  5.41737735e-01,\n",
       "         2.82823741e-01],\n",
       "       [-1.16451252e+00, -1.39549065e+00,  1.45726061e+00,\n",
       "         6.02895916e-01],\n",
       "       [-7.65579581e-01, -1.08502150e+00,  1.16742277e+00,\n",
       "         4.93703306e-01],\n",
       "       [-1.41535282e+00, -1.58420110e+00,  1.64047360e+00,\n",
       "         6.70965254e-01],\n",
       "       [-9.81221318e-01, -1.25232565e+00,  1.32448685e+00,\n",
       "         5.52503049e-01],\n",
       "       [-1.03259325e+00, -1.29332149e+00,  1.36176872e+00,\n",
       "         5.66793919e-01],\n",
       "       [-1.19065499e+00, -1.41672671e+00,  1.47644305e+00,\n",
       "         6.10188365e-01],\n",
       "       [-1.49979174e+00, -1.69385886e+00,  1.71585226e+00,\n",
       "         7.10997462e-01],\n",
       "       [-1.74326789e+00, -1.92048955e+00,  1.90803719e+00,\n",
       "         7.94492781e-01],\n",
       "       [-1.00651670e+00, -1.27232194e+00,  1.34278774e+00,\n",
       "         5.59515357e-01],\n",
       "       [ 1.21516180e+00,  9.53162789e-01, -2.97224104e-01,\n",
       "        -3.35486829e-02],\n",
       "       [ 3.31050634e-01,  4.35957015e-02,  3.03794563e-01,\n",
       "         2.17162594e-01],\n",
       "       [ 9.63395357e-01,  6.93088651e-01, -1.25671715e-01,\n",
       "         3.76884192e-02],\n",
       "       [-3.28591734e-01, -7.39817560e-01,  8.48185897e-01,\n",
       "         3.73512089e-01],\n",
       "       [-1.34162986e+00, -1.52506828e+00,  1.58563066e+00,\n",
       "         6.49691045e-01],\n",
       "       [-3.94157618e-01, -7.90226936e-01,  8.95203769e-01,\n",
       "         3.91496658e-01],\n",
       "       [-8.77955675e-01, -1.17188334e+00,  1.24799943e+00,\n",
       "         5.24640262e-01],\n",
       "       [-1.38048172e+00, -1.58851755e+00,  1.62300968e+00,\n",
       "         6.72172368e-01],\n",
       "       [-1.07527471e+00, -1.32515955e+00,  1.39305329e+00,\n",
       "         5.78101814e-01],\n",
       "       [ 1.07919663e-01, -2.25402802e-01,  4.88947630e-01,\n",
       "         2.69576281e-01],\n",
       "       [-3.09590250e-01, -7.25664318e-01,  8.34693015e-01,\n",
       "         3.68346721e-01],\n",
       "       [-4.03982252e-01, -7.99561858e-01,  9.03423548e-01,\n",
       "         3.94230127e-01],\n",
       "       [-1.02554893e+00, -1.28734505e+00,  1.35655260e+00,\n",
       "         5.64787924e-01],\n",
       "       [-4.61375326e-01, -8.45683694e-01,  9.45476592e-01,\n",
       "         4.10149872e-01],\n",
       "       [ 2.93223786e+00,  2.89079118e+00, -1.44316077e+00,\n",
       "        -5.34192264e-01],\n",
       "       [-4.08042878e-01, -8.03067088e-01,  9.06172097e-01,\n",
       "         3.95473301e-01],\n",
       "       [-8.61872435e-02, -4.68255520e-01,  6.52339101e-01,\n",
       "         3.14483941e-01],\n",
       "       [-3.76997501e-01, -7.77691483e-01,  8.83014560e-01,\n",
       "         3.86896312e-01],\n",
       "       [-9.42252994e-01, -1.22273624e+00,  1.29618287e+00,\n",
       "         5.41993022e-01],\n",
       "       [ 3.29353523e+00,  3.31128645e+00, -1.68341184e+00,\n",
       "        -6.40324473e-01],\n",
       "       [-3.73162001e-01, -7.75683880e-01,  8.80456269e-01,\n",
       "         3.85983586e-01],\n",
       "       [-2.71337676e+00, -2.79136467e+00,  2.66497397e+00,\n",
       "         1.11567879e+00],\n",
       "       [-1.45153797e+00, -1.60977757e+00,  1.66657495e+00,\n",
       "         6.80519342e-01],\n",
       "       [-3.01044846e+00, -3.07190442e+00,  2.90104985e+00,\n",
       "         1.21889997e+00],\n",
       "       [-1.98976481e+00, -2.11539888e+00,  2.09295607e+00,\n",
       "         8.66699815e-01],\n",
       "       [-2.58667278e+00, -2.66925240e+00,  2.56391811e+00,\n",
       "         1.07071483e+00],\n",
       "       [-3.85991478e+00, -3.84745169e+00,  3.56779265e+00,\n",
       "         1.50471354e+00],\n",
       "       [-4.28325802e-01, -8.17670107e-01,  9.21283126e-01,\n",
       "         4.00664002e-01],\n",
       "       [-3.20919013e+00, -3.24407816e+00,  3.05447507e+00,\n",
       "         1.28245461e+00],\n",
       "       [-2.59385705e+00, -2.65416455e+00,  2.56374884e+00,\n",
       "         1.06536055e+00],\n",
       "       [-3.31802654e+00, -3.38164473e+00,  3.15020370e+00,\n",
       "         1.33277166e+00],\n",
       "       [-1.84425151e+00, -2.01614928e+00,  1.98812890e+00,\n",
       "         8.29732478e-01],\n",
       "       [-2.03249049e+00, -2.15422559e+00,  2.12658215e+00,\n",
       "         8.80983591e-01],\n",
       "       [-2.51446891e+00, -2.61864519e+00,  2.51163220e+00,\n",
       "         1.05186534e+00],\n",
       "       [-1.48328269e+00, -1.62804198e+00,  1.68856668e+00,\n",
       "         6.87361717e-01],\n",
       "       [-1.85213625e+00, -1.98313856e+00,  1.98308730e+00,\n",
       "         8.18034470e-01],\n",
       "       [-2.17328954e+00, -2.31056571e+00,  2.24466181e+00,\n",
       "         9.38314259e-01],\n",
       "       [-2.02646589e+00, -2.16241360e+00,  2.12555456e+00,\n",
       "         8.83865535e-01],\n",
       "       [-3.73917198e+00, -3.76161695e+00,  3.47886682e+00,\n",
       "         1.47293854e+00],\n",
       "       [-4.48788500e+00, -4.39758539e+00,  4.05433226e+00,\n",
       "         1.70770037e+00],\n",
       "       [-1.40381718e+00, -1.56507719e+00,  1.63057566e+00,\n",
       "         6.64980173e-01],\n",
       "       [-2.81725240e+00, -2.90507770e+00,  2.75199008e+00,\n",
       "         1.15732539e+00],\n",
       "       [-1.23447728e+00, -1.44518900e+00,  1.50799632e+00,\n",
       "         6.21053994e-01],\n",
       "       [-3.99929380e+00, -3.96475792e+00,  3.67449665e+00,\n",
       "         1.54804409e+00],\n",
       "       [-1.59581864e+00, -1.76076806e+00,  1.78533745e+00,\n",
       "         7.35913813e-01],\n",
       "       [-2.44913626e+00, -2.56703091e+00,  2.46243095e+00,\n",
       "         1.03278232e+00],\n",
       "       [-2.80135226e+00, -2.89073849e+00,  2.73958373e+00,\n",
       "         1.15203428e+00],\n",
       "       [-1.41054463e+00, -1.59590375e+00,  1.64107013e+00,\n",
       "         6.75114810e-01],\n",
       "       [-1.31949234e+00, -1.51783752e+00,  1.57090521e+00,\n",
       "         6.46307647e-01],\n",
       "       [-2.37256455e+00, -2.46426964e+00,  2.39329696e+00,\n",
       "         9.95266199e-01],\n",
       "       [-2.57255983e+00, -2.67493176e+00,  2.55818057e+00,\n",
       "         1.07256222e+00],\n",
       "       [-3.27763605e+00, -3.30935740e+00,  3.10906506e+00,\n",
       "         1.30646265e+00],\n",
       "       [-3.50187945e+00, -3.55213904e+00,  3.29454041e+00,\n",
       "         1.39565969e+00],\n",
       "       [-2.46124029e+00, -2.54572058e+00,  2.46303439e+00,\n",
       "         1.02527797e+00],\n",
       "       [-1.42401206e+00, -1.60321343e+00,  1.65022779e+00,\n",
       "         6.77870452e-01],\n",
       "       [-1.61542022e+00, -1.75002086e+00,  1.79261827e+00,\n",
       "         7.32306242e-01],\n",
       "       [-3.76812792e+00, -3.77750134e+00,  3.49966788e+00,\n",
       "         1.47876096e+00],\n",
       "       [-2.30791521e+00, -2.43464041e+00,  2.35051036e+00,\n",
       "         9.84053850e-01],\n",
       "       [-1.90969360e+00, -2.05789471e+00,  2.03442550e+00,\n",
       "         8.45334113e-01],\n",
       "       [-1.22884214e+00, -1.44531322e+00,  1.50406694e+00,\n",
       "         6.20432138e-01],\n",
       "       [-2.47717905e+00, -2.59503388e+00,  2.48527551e+00,\n",
       "         1.04304290e+00],\n",
       "       [-2.73222470e+00, -2.81882310e+00,  2.68285298e+00,\n",
       "         1.12563634e+00],\n",
       "       [-2.44902682e+00, -2.57689929e+00,  2.46533537e+00,\n",
       "         1.03626359e+00],\n",
       "       [-1.45153797e+00, -1.60977757e+00,  1.66657495e+00,\n",
       "         6.80519342e-01],\n",
       "       [-2.88026333e+00, -2.95514178e+00,  2.79931140e+00,\n",
       "         1.17587221e+00],\n",
       "       [-2.80383968e+00, -2.89283371e+00,  2.74138021e+00,\n",
       "         1.15282905e+00],\n",
       "       [-2.41232467e+00, -2.52988410e+00,  2.43273687e+00,\n",
       "         1.01909781e+00],\n",
       "       [-1.83855736e+00, -1.97034919e+00,  1.97256088e+00,\n",
       "         8.13279450e-01],\n",
       "       [-1.99831402e+00, -2.14427948e+00,  2.10561466e+00,\n",
       "         8.77086341e-01],\n",
       "       [-2.00824523e+00, -2.16187286e+00,  2.11554146e+00,\n",
       "         8.83518815e-01],\n",
       "       [-1.31288838e+00, -1.50716925e+00,  1.56484342e+00,\n",
       "         6.42756104e-01]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Stage 2: Distributional Evidential Clustering (Option 3)\n",
    "# ------------------------------------------------------------\n",
    "# Input  : U  -> latent features from Stage 1 (masked AE/VAE)\n",
    "# Output : fuzzy memberships, hard labels, cluster parameters\n",
    "#\n",
    "# Key design choices (intentional):\n",
    "# - Clusters are Gaussian distributions in latent space\n",
    "# - Memberships are evidential (fuzzy), not probabilistic\n",
    "# - Distance is negative log-likelihood (not Euclidean)\n",
    "# - Explicit clusterâ€“cluster divergence regularization\n",
    "# - Diagonal covariance for stability and efficiency\n",
    "# - No EM, no mixture weights, no DST\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Negative log-likelihood under a diagonal Gaussian\n",
    "# This replaces point-wise Euclidean distance\n",
    "# ------------------------------------------------------------\n",
    "def neg_log_likelihood(u, mu, var, eps=1e-6):\n",
    "    \"\"\"\n",
    "    u   : latent feature vector of a sample (d,)\n",
    "    mu  : cluster mean (d,)\n",
    "    var : diagonal variance of cluster (d,)\n",
    "    \"\"\"\n",
    "    d = u.shape[0]\n",
    "    return 0.5 * (\n",
    "        np.sum(np.log(var + eps)) +\n",
    "        np.sum((u - mu) ** 2 / (var + eps)) +\n",
    "        d * np.log(2 * np.pi)\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# KL divergence between two diagonal Gaussian clusters\n",
    "# Used to explicitly penalize cluster similarity\n",
    "# ------------------------------------------------------------\n",
    "def kl_divergence(mu1, var1, mu2, var2, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes KL( N(mu1,var1) || N(mu2,var2) )\n",
    "    \"\"\"\n",
    "    return 0.5 * np.sum(\n",
    "        np.log((var2 + eps) / (var1 + eps)) +\n",
    "        (var1 + (mu1 - mu2) ** 2) / (var2 + eps) - 1.0\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Initialization\n",
    "# - Means initialized from random latent points\n",
    "# - Variances initialized to ones\n",
    "# - Memberships initialized uniformly (maximum uncertainty)\n",
    "# ------------------------------------------------------------\n",
    "def initialize_clusters(U, n_clusters, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n, d = U.shape\n",
    "\n",
    "    idx = np.random.choice(n, n_clusters, replace=False)\n",
    "    mu = U[idx].copy()\n",
    "    var = np.ones((n_clusters, d))           # diagonal covariance\n",
    "    M = np.ones((n, n_clusters)) / n_clusters\n",
    "\n",
    "    return mu, var, M\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute divergence penalty for each cluster\n",
    "# Î”_j = sum_{kâ‰ j} KL(C_j || C_k)\n",
    "# ------------------------------------------------------------\n",
    "def compute_cluster_penalties(mu, var):\n",
    "    c = mu.shape[0]\n",
    "    Delta = np.zeros(c)\n",
    "\n",
    "    for j in range(c):\n",
    "        for k in range(c):\n",
    "            if j != k:\n",
    "                Delta[j] += kl_divergence(\n",
    "                    mu[j], var[j],\n",
    "                    mu[k], var[k]\n",
    "                )\n",
    "    return Delta\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Membership update (Option 3)\n",
    "# Implements Eq. (4) in the paper\n",
    "# Uses direct exponentiation (no logsumexp)\n",
    "# ------------------------------------------------------------\n",
    "def update_memberships(U, mu, var, beta, gamma, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Memberships are updated using likelihood + divergence cost.\n",
    "    This is NOT a posterior probability (unlike GMM).\n",
    "    \"\"\"\n",
    "    n, _ = U.shape\n",
    "    c = mu.shape[0]\n",
    "\n",
    "    Delta = compute_cluster_penalties(mu, var)\n",
    "    M = np.zeros((n, c))\n",
    "\n",
    "    for i in range(n):\n",
    "        numerators = np.zeros(c)\n",
    "\n",
    "        for j in range(c):\n",
    "            # Combined cost:\n",
    "            # data-to-cluster fit + cluster-separation penalty\n",
    "            cost_ij = (\n",
    "                neg_log_likelihood(U[i], mu[j], var[j])\n",
    "                - gamma * Delta[j]\n",
    "            )\n",
    "\n",
    "            numerators[j] = np.exp(-cost_ij / (beta - 1.0))\n",
    "\n",
    "        # Normalize memberships to sum to 1\n",
    "        M[i, :] = numerators / (np.sum(numerators) + eps)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cluster parameter update\n",
    "# Weighted maximum likelihood using evidential memberships\n",
    "# Implements Eq. (5) and Eq. (6)\n",
    "# ------------------------------------------------------------\n",
    "def update_clusters(U, M, beta, eps=1e-8):\n",
    "    n, d = U.shape\n",
    "    c = M.shape[1]\n",
    "\n",
    "    mu_new = np.zeros((c, d))\n",
    "    var_new = np.zeros((c, d))\n",
    "    M_beta = M ** beta\n",
    "\n",
    "    for j in range(c):\n",
    "        w = M_beta[:, j][:, None]\n",
    "        denom = np.sum(w) + eps\n",
    "\n",
    "        # Mean update\n",
    "        mu_new[j] = np.sum(w * U, axis=0) / denom\n",
    "\n",
    "        # Diagonal covariance update\n",
    "        var_new[j] = np.sum(\n",
    "            w * (U - mu_new[j]) ** 2, axis=0\n",
    "        ) / denom\n",
    "\n",
    "    return mu_new, var_new\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main optimization loop (Algorithm 1 in paper)\n",
    "# ------------------------------------------------------------\n",
    "def distributional_evidential_clustering(\n",
    "    U,\n",
    "    n_clusters,\n",
    "    beta=2.0,\n",
    "    gamma=0.05,\n",
    "    max_iter=3,\n",
    "    tol=1e-4,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs distributional evidential clustering on latent features.\n",
    "    \"\"\"\n",
    "    mu, var, M = initialize_clusters(U, n_clusters, seed)\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        mu_old = mu.copy()\n",
    "\n",
    "        # Step 1: update memberships\n",
    "        M = update_memberships(U, mu, var, beta, gamma)\n",
    "\n",
    "        # Step 2: update cluster distributions\n",
    "        mu, var = update_clusters(U, M, beta)\n",
    "\n",
    "        # Convergence check on cluster means\n",
    "        shift = np.max(np.linalg.norm(mu - mu_old, axis=1))\n",
    "        if shift < tol:\n",
    "            print(f\"Converged at iteration {it}\")\n",
    "            break\n",
    "\n",
    "    labels = np.argmax(M, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"memberships\": M,\n",
    "        \"labels\": labels,\n",
    "        \"mu\": mu,\n",
    "        \"var\": var\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = distributional_evidential_clustering(\n",
    "    latent_np,\n",
    "    n_clusters=3,\n",
    "    beta=2,\n",
    "    gamma=0.00075,\n",
    "    max_iter=500,\n",
    "    tol=1e-8,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memberships': array([[2.62368142e-194, 1.00000000e+000, 1.12920096e-282],\n",
       "        [9.94646984e-172, 1.00000000e+000, 5.92089163e-255],\n",
       "        [2.28552423e-208, 1.00000000e+000, 5.90354182e-300],\n",
       "        [7.92775254e-199, 1.00000000e+000, 3.14669877e-288],\n",
       "        [3.65593716e-210, 1.00000000e+000, 3.64920308e-302],\n",
       "        [7.28165494e-171, 1.00000000e+000, 6.78018138e-254],\n",
       "        [2.33294878e-219, 1.00000000e+000, 1.77575175e-313],\n",
       "        [1.04012669e-189, 1.00000000e+000, 5.01806475e-277],\n",
       "        [1.89610926e-204, 1.00000000e+000, 3.87834686e-295],\n",
       "        [3.17679200e-182, 1.00000000e+000, 7.79312415e-268],\n",
       "        [1.35681595e-180, 1.00000000e+000, 7.82559056e-266],\n",
       "        [5.79456711e-201, 1.00000000e+000, 7.44926149e-291],\n",
       "        [1.67473980e-187, 1.00000000e+000, 2.57964608e-274],\n",
       "        [7.77314353e-243, 9.99999996e-001, 0.00000000e+000],\n",
       "        [2.72459512e-186, 1.00000000e+000, 7.96184666e-273],\n",
       "        [3.17953475e-195, 1.00000000e+000, 8.43491040e-284],\n",
       "        [1.30090958e-190, 1.00000000e+000, 3.91298317e-278],\n",
       "        [1.26588397e-186, 1.00000000e+000, 3.08998516e-273],\n",
       "        [1.57253026e-146, 1.00000000e+000, 4.57110462e-224],\n",
       "        [5.93106739e-205, 1.00000000e+000, 9.28944963e-296],\n",
       "        [1.76946364e-147, 1.00000000e+000, 3.14746246e-225],\n",
       "        [1.63940832e-189, 1.00000000e+000, 8.76300860e-277],\n",
       "        [2.26989025e-262, 9.99993860e-001, 0.00000000e+000],\n",
       "        [5.72860214e-141, 9.99999999e-001, 2.94562053e-217],\n",
       "        [3.79965794e-186, 1.00000000e+000, 1.18388013e-272],\n",
       "        [9.01606490e-154, 1.00000000e+000, 6.14640328e-233],\n",
       "        [2.06966663e-169, 1.00000000e+000, 4.11654550e-252],\n",
       "        [2.82957459e-181, 1.00000000e+000, 1.14180339e-266],\n",
       "        [1.85058234e-178, 1.00000000e+000, 3.26683515e-263],\n",
       "        [1.50992330e-193, 1.00000000e+000, 9.65894770e-282],\n",
       "        [1.06433632e-177, 1.00000000e+000, 2.78641300e-262],\n",
       "        [5.37481526e-142, 9.99999999e-001, 1.63123734e-218],\n",
       "        [1.61981914e-235, 1.00000000e+000, 0.00000000e+000],\n",
       "        [3.20947405e-216, 1.00000000e+000, 1.29614992e-309],\n",
       "        [1.52290789e-174, 1.00000000e+000, 2.08015422e-258],\n",
       "        [5.14824941e-189, 1.00000000e+000, 3.59183233e-276],\n",
       "        [7.14410744e-167, 1.00000000e+000, 5.37402290e-249],\n",
       "        [5.90189281e-226, 1.00000000e+000, 1.40808709e-321],\n",
       "        [3.77352945e-217, 1.00000000e+000, 9.29165832e-311],\n",
       "        [1.29219030e-181, 1.00000000e+000, 4.36134166e-267],\n",
       "        [1.16820794e-199, 1.00000000e+000, 2.99960802e-289],\n",
       "        [4.19364324e-147, 1.00000000e+000, 9.07400060e-225],\n",
       "        [1.12415100e-232, 1.00000000e+000, 0.00000000e+000],\n",
       "        [8.36095293e-162, 1.00000000e+000, 8.74300520e-243],\n",
       "        [1.62558507e-177, 1.00000000e+000, 4.66418435e-262],\n",
       "        [3.85515945e-172, 1.00000000e+000, 1.84908285e-255],\n",
       "        [1.06343450e-207, 1.00000000e+000, 3.89847081e-299],\n",
       "        [1.58549911e-211, 1.00000000e+000, 7.68366787e-304],\n",
       "        [1.09286252e-188, 1.00000000e+000, 9.02489038e-276],\n",
       "        [6.81411464e-187, 1.00000000e+000, 1.44502369e-273],\n",
       "        [9.93991434e-001, 1.10944974e-214, 6.00856591e-003],\n",
       "        [9.99977669e-001, 5.87309260e-202, 2.23306081e-005],\n",
       "        [9.47614693e-001, 9.81392418e-220, 5.23853068e-002],\n",
       "        [1.00000000e+000, 3.35741608e-177, 4.10088524e-010],\n",
       "        [9.99867450e-001, 5.06498783e-206, 1.32550209e-004],\n",
       "        [9.99999990e-001, 2.56999180e-184, 9.50003240e-009],\n",
       "        [9.99948827e-001, 7.55055301e-204, 5.11734765e-005],\n",
       "        [8.10443253e-022, 6.38904757e-052, 4.63847959e-061],\n",
       "        [9.99946741e-001, 6.10514702e-204, 5.32593139e-005],\n",
       "        [9.99998928e-001, 1.80587878e-121, 2.93021241e-021],\n",
       "        [2.14884695e-003, 3.63100034e-086, 1.17656809e-031],\n",
       "        [9.99999965e-001, 2.98143827e-187, 3.46904586e-008],\n",
       "        [9.99999987e-001, 4.27446056e-185, 1.33975308e-008],\n",
       "        [9.99994853e-001, 1.28728032e-198, 5.14653616e-006],\n",
       "        [9.99994721e-001, 1.18045891e-117, 4.98055985e-022],\n",
       "        [9.99935166e-001, 2.18227330e-204, 6.48341690e-005],\n",
       "        [9.99999996e-001, 1.52125273e-182, 4.34481562e-009],\n",
       "        [1.00000000e+000, 1.57842996e-168, 8.66904310e-012],\n",
       "        [9.99942050e-001, 3.78881491e-204, 5.79496284e-005],\n",
       "        [1.00000000e+000, 1.16053579e-157, 5.69771033e-014],\n",
       "        [9.99975616e-001, 3.68486058e-202, 2.43844307e-005],\n",
       "        [9.99999954e-001, 6.57969772e-188, 4.63464513e-008],\n",
       "        [9.98795054e-001, 4.38318574e-211, 1.20494607e-003],\n",
       "        [9.99998632e-001, 1.33194174e-195, 1.36819346e-006],\n",
       "        [9.99996911e-001, 1.87842011e-197, 3.08850489e-006],\n",
       "        [9.99962991e-001, 4.13845037e-203, 3.70086059e-005],\n",
       "        [9.92483312e-001, 3.20336053e-215, 7.51668814e-003],\n",
       "        [6.38873493e-001, 3.24520412e-225, 3.61126507e-001],\n",
       "        [9.99997960e-001, 1.64605074e-196, 2.03988228e-006],\n",
       "        [9.99967891e-001, 1.44511327e-113, 7.48736822e-023],\n",
       "        [1.00000000e+000, 1.95496847e-146, 2.99753908e-016],\n",
       "        [9.99999406e-001, 6.23416088e-123, 5.82003244e-021],\n",
       "        [1.00000000e+000, 2.97908985e-172, 4.58718118e-011],\n",
       "        [9.99631613e-001, 2.24802361e-208, 3.68387293e-004],\n",
       "        [1.00000000e+000, 1.42448963e-174, 1.28043544e-010],\n",
       "        [9.99999731e-001, 6.64129587e-192, 2.69258146e-007],\n",
       "        [9.99024415e-001, 1.49646985e-210, 9.75584789e-004],\n",
       "        [9.99994023e-001, 5.84965030e-199, 5.97714499e-006],\n",
       "        [1.00000000e+000, 3.39786666e-155, 1.78579589e-014],\n",
       "        [1.00000000e+000, 1.37780993e-171, 3.41873563e-011],\n",
       "        [1.00000000e+000, 5.92693689e-175, 1.51716221e-010],\n",
       "        [9.99997246e-001, 3.41586445e-197, 2.75439645e-006],\n",
       "        [1.00000000e+000, 5.07627355e-177, 3.78879256e-010],\n",
       "        [5.95664566e-015, 2.42372980e-062, 1.45085274e-050],\n",
       "        [1.00000000e+000, 4.20891504e-175, 1.62073186e-010],\n",
       "        [1.00000000e+000, 6.64353488e-163, 6.41809907e-013],\n",
       "        [1.00000000e+000, 5.63026756e-174, 9.83833828e-011],\n",
       "        [9.99999254e-001, 3.20272464e-194, 7.46018870e-007],\n",
       "        [2.05131557e-020, 6.82004160e-054, 6.02429117e-059],\n",
       "        [1.00000000e+000, 7.43473058e-174, 9.33327302e-011],\n",
       "        [8.25588597e-008, 1.59158476e-270, 9.99999917e-001],\n",
       "        [9.97914484e-001, 2.39530077e-212, 2.08551569e-003],\n",
       "        [3.91928121e-010, 5.57632411e-285, 1.00000000e+000],\n",
       "        [3.19145211e-002, 1.22447064e-235, 9.68085479e-001],\n",
       "        [8.28525584e-007, 2.67522350e-264, 9.99999171e-001],\n",
       "        [1.15580425e-016, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.00000000e+000, 8.27759518e-176, 2.21290526e-010],\n",
       "        [1.30648699e-011, 3.21820001e-294, 1.00000000e+000],\n",
       "        [9.37651881e-007, 5.00327813e-264, 9.99999062e-001],\n",
       "        [1.20981298e-012, 1.63893495e-300, 1.00000000e+000],\n",
       "        [2.24645569e-001, 8.90479125e-230, 7.75354431e-001],\n",
       "        [1.53662041e-002, 1.16814598e-237, 9.84633796e-001],\n",
       "        [2.47352293e-006, 2.70607575e-261, 9.99997526e-001],\n",
       "        [9.96776543e-001, 2.33309333e-213, 3.22345654e-003],\n",
       "        [2.83959228e-001, 4.85395006e-229, 7.16040772e-001],\n",
       "        [9.59967118e-004, 3.85545828e-245, 9.99040033e-001],\n",
       "        [1.46422429e-002, 9.23680237e-238, 9.85357757e-001],\n",
       "        [7.33684737e-016, 1.51776966e-320, 1.00000000e+000],\n",
       "        [2.18825984e-021, 0.00000000e+000, 9.99998734e-001],\n",
       "        [9.99084053e-001, 1.77822852e-210, 9.15947173e-004],\n",
       "        [1.06021851e-008, 5.05098409e-276, 9.99999989e-001],\n",
       "        [9.99929490e-001, 1.36298443e-204, 7.05103506e-005],\n",
       "        [1.09941508e-017, 0.00000000e+000, 9.99999998e-001],\n",
       "        [9.68727254e-001, 1.45227543e-218, 3.12727460e-002],\n",
       "        [7.12357901e-006, 2.08475245e-258, 9.99992876e-001],\n",
       "        [1.40052137e-008, 2.86530651e-275, 9.99999986e-001],\n",
       "        [9.98677554e-001, 2.83780931e-211, 1.32244591e-003],\n",
       "        [9.99715139e-001, 9.08181789e-208, 2.84860691e-004],\n",
       "        [3.99690343e-005, 7.98145375e-254, 9.99960031e-001],\n",
       "        [8.56375754e-007, 3.71186150e-264, 9.99999144e-001],\n",
       "        [3.76824232e-012, 1.41955766e-297, 1.00000000e+000],\n",
       "        [4.52477311e-014, 2.21047568e-309, 1.00000000e+000],\n",
       "        [8.35760019e-006, 4.60292000e-258, 9.99991642e-001],\n",
       "        [9.98416578e-001, 1.08403287e-211, 1.58342227e-003],\n",
       "        [9.67987077e-001, 1.14618587e-218, 3.20129229e-002],\n",
       "        [4.97716697e-016, 8.15208316e-322, 1.00000000e+000],\n",
       "        [8.89559819e-005, 1.39998061e-251, 9.99911044e-001],\n",
       "        [1.01214391e-001, 2.61583158e-232, 8.98785609e-001],\n",
       "        [9.99933213e-001, 1.85367600e-204, 6.67871175e-005],\n",
       "        [4.22761254e-006, 8.17445099e-260, 9.99995772e-001],\n",
       "        [5.25273558e-008, 1.01818853e-271, 9.99999947e-001],\n",
       "        [6.35892873e-006, 1.09146472e-258, 9.99993641e-001],\n",
       "        [9.97914484e-001, 2.39530077e-212, 2.08551569e-003],\n",
       "        [3.80724649e-009, 8.08709130e-279, 9.99999996e-001],\n",
       "        [1.34346072e-008, 2.21089503e-275, 9.99999987e-001],\n",
       "        [1.41771799e-005, 1.48586713e-256, 9.99985823e-001],\n",
       "        [3.35543508e-001, 2.02945661e-228, 6.64456492e-001],\n",
       "        [2.18640571e-002, 1.21564177e-236, 9.78135943e-001],\n",
       "        [1.67736823e-002, 2.36886897e-237, 9.83226318e-001],\n",
       "        [9.99757173e-001, 2.06090026e-207, 2.42826886e-004]]),\n",
       " 'labels': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "        2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n",
       "        2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0]),\n",
       " 'mu': array([[-0.80556248, -1.07484992,  1.19306988,  0.50852853],\n",
       "        [ 9.05010832, 10.01291937, -5.51186742, -2.33157555],\n",
       "        [-2.73644453, -2.81976433,  2.68532429,  1.12601913]]),\n",
       " 'var': array([[0.49402821, 0.43008445, 0.27354458, 0.03623306],\n",
       "        [0.44075946, 0.59808969, 0.19512507, 0.03803489],\n",
       "        [0.42948869, 0.36333692, 0.26578664, 0.04932473]])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = result[\"labels\"]\n",
    "memberships = result[\"memberships\"]\n",
    "df[\"pred\"]=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Re': 0.7533333333333333, 'Ri': 0.0, 'EP': 0.8309053069719042, 'ERI': 0.8987919463087248, 'RI': 0.8987919463087248, 'ARI': 0.7733799163281198}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "ari = compute_all_metrics(df[\"target\"], df[\"pred\"])\n",
    "print(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "metrics.py\n",
    "Clustering evaluation metrics for algorithms that assign single labels (no meta-clusters)\n",
    "-1 indicates outliers\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.special import comb\n",
    "\n",
    "def Re(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Error Rate (Re)\n",
    "    \n",
    "    Calculates the ratio of misclassifications to total number of objects.\n",
    "    \n",
    "    Definition: Re = Ne / N\n",
    "    where Ne = number of misclassifications\n",
    "          N  = total number of objects\n",
    "    \n",
    "    Misclassification occurs when:\n",
    "    1. pred_label[i] != original_label[i] (wrong cluster assignment)\n",
    "    2. pred_label[i] == -1 (outlier) but original_label[i] is not an outlier*\n",
    "    \n",
    "    *Note: Assumes original_label has no outlier class (-1)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments (non-negative integers)\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments (-1 for outliers)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    re : float\n",
    "        Error rate (0 to 1, lower is better)\n",
    "    \"\"\"\n",
    "    n = len(original_label)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    ne = 0  # Number of misclassifications\n",
    "    \n",
    "    for i in range(n):\n",
    "        true_cluster = original_label[i]\n",
    "        pred_cluster = pred_label[i]\n",
    "        \n",
    "        # Misclassification if:\n",
    "        # 1. Wrong cluster assignment\n",
    "        # 2. Predicted as outlier (-1) but true is not outlier\n",
    "        if pred_cluster == -1:\n",
    "            # If predicted as outlier, always misclassification \n",
    "            # (since original_label has no outlier class)\n",
    "            ne += 1\n",
    "        elif pred_cluster != true_cluster:\n",
    "            ne += 1\n",
    "    \n",
    "    return ne / n\n",
    "\n",
    "def Ri(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Imprecision Rate (Ri)\n",
    "    \n",
    "    In your algorithm without meta-clusters, this should always return 0\n",
    "    since there are no meta-cluster assignments.\n",
    "    \n",
    "    Definition: Ri = Ni / N\n",
    "    where Ni = number of objects in meta-clusters\n",
    "          N  = total number of objects\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ri : float\n",
    "        Always 0 (no meta-clusters)\n",
    "    \"\"\"\n",
    "    return 0.0\n",
    "\n",
    "def EP(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Evidential Precision (EP) - Simplified for single label assignments\n",
    "    \n",
    "    Measures precision of non-outlier assignments.\n",
    "    \n",
    "    Definition: EP = TP* / (TP* + FP*)\n",
    "    where TP* = number of pairs of similar objects (same true cluster) \n",
    "                that are both assigned to the same cluster (and not outliers)\n",
    "          FP* = number of pairs of dissimilar objects (different true clusters)\n",
    "                that are both assigned to the same cluster (and not outliers)\n",
    "    \n",
    "    Only considers pairs where both objects are NOT outliers (pred_label != -1)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments (-1 for outliers)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ep : float\n",
    "        Evidential precision (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    n = len(original_label)\n",
    "    \n",
    "    # Find indices of non-outlier predictions\n",
    "    non_outlier_indices = [i for i in range(n) if pred_label[i] != -1]\n",
    "    \n",
    "    if len(non_outlier_indices) < 2:\n",
    "        return 0.0  # Need at least 2 non-outliers to compute pairs\n",
    "    \n",
    "    tp_star = 0  # True positive pairs (similar objects in same cluster)\n",
    "    fp_star = 0  # False positive pairs (dissimilar objects in same cluster)\n",
    "    \n",
    "    # Consider all pairs of non-outlier objects\n",
    "    for i, j in combinations(non_outlier_indices, 2):\n",
    "        if i >= j:\n",
    "            continue\n",
    "            \n",
    "        true_i = original_label[i]\n",
    "        true_j = original_label[j]\n",
    "        pred_i = pred_label[i]\n",
    "        pred_j = pred_label[j]\n",
    "        \n",
    "        # Check if assigned to the same cluster\n",
    "        if pred_i == pred_j:\n",
    "            if true_i == true_j:\n",
    "                tp_star += 1\n",
    "            else:\n",
    "                fp_star += 1\n",
    "    \n",
    "    denominator = tp_star + fp_star\n",
    "    return tp_star / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def ERI(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Evidential Rank Index (ERI) - Simplified for single label assignments\n",
    "    \n",
    "    Comprehensive evaluation considering both beneficial and detrimental elements.\n",
    "    \n",
    "    Definition: ERI = 2(TP* + TN*) / [N(N-1)]\n",
    "    where TP* = number of pairs of similar objects (same true cluster) \n",
    "                that are both assigned to the same cluster (and not outliers)\n",
    "          TN* = number of pairs of dissimilar objects (different true clusters)\n",
    "                that are assigned to different clusters (and both not outliers)\n",
    "          N   = total number of objects\n",
    "    \n",
    "    Only considers pairs where both objects are NOT outliers (pred_label != -1)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments (-1 for outliers)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    eri : float\n",
    "        Evidential rank index (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    n = len(original_label)\n",
    "    \n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Find indices of non-outlier predictions\n",
    "    non_outlier_indices = [i for i in range(n) if pred_label[i] != -1]\n",
    "    \n",
    "    tp_star = 0  # True positive pairs\n",
    "    tn_star = 0  # True negative pairs\n",
    "    \n",
    "    # Consider all pairs of non-outlier objects\n",
    "    for i, j in combinations(non_outlier_indices, 2):\n",
    "        if i >= j:\n",
    "            continue\n",
    "            \n",
    "        true_i = original_label[i]\n",
    "        true_j = original_label[j]\n",
    "        pred_i = pred_label[i]\n",
    "        pred_j = pred_label[j]\n",
    "        \n",
    "        same_true = (true_i == true_j)\n",
    "        same_pred = (pred_i == pred_j)\n",
    "        \n",
    "        if same_true and same_pred:\n",
    "            tp_star += 1\n",
    "        elif (not same_true) and (not same_pred):\n",
    "            tn_star += 1\n",
    "    \n",
    "    total_pairs = n * (n - 1)\n",
    "    return 2 * (tp_star + tn_star) / total_pairs if total_pairs > 0 else 0.0\n",
    "\n",
    "def rand_index(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Rand Index (RI) - Standard version\n",
    "    \n",
    "    Measures the similarity between two data clusterings.\n",
    "    \n",
    "    Definition: RI = (TP + TN) / (TP + FP + FN + TN)\n",
    "    where TP = number of pairs of similar objects (same true cluster) \n",
    "               that are both assigned to the same predicted cluster\n",
    "          TN = number of pairs of dissimilar objects (different true clusters)\n",
    "               that are assigned to different predicted clusters\n",
    "          FP = number of pairs of dissimilar objects \n",
    "               that are assigned to the same predicted cluster\n",
    "          FN = number of pairs of similar objects \n",
    "               that are assigned to different predicted clusters\n",
    "    \n",
    "    Treats -1 (outliers) as a separate cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ri : float\n",
    "        Rand Index (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    n = len(original_label)\n",
    "    \n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    tp = 0  # True positives\n",
    "    tn = 0  # True negatives\n",
    "    fp = 0  # False positives\n",
    "    fn = 0  # False negatives\n",
    "    \n",
    "    # Consider all pairs of objects\n",
    "    for i, j in combinations(range(n), 2):\n",
    "        if i >= j:\n",
    "            continue\n",
    "            \n",
    "        true_i = original_label[i]\n",
    "        true_j = original_label[j]\n",
    "        pred_i = pred_label[i]\n",
    "        pred_j = pred_label[j]\n",
    "        \n",
    "        same_true = (true_i == true_j)\n",
    "        same_pred = (pred_i == pred_j)\n",
    "        \n",
    "        if same_true and same_pred:\n",
    "            tp += 1\n",
    "        elif same_true and not same_pred:\n",
    "            fn += 1\n",
    "        elif not same_true and same_pred:\n",
    "            fp += 1\n",
    "        else:  # not same_true and not same_pred\n",
    "            tn += 1\n",
    "    \n",
    "    total = tp + tn + fp + fn\n",
    "    return (tp + tn) / total if total > 0 else 0.0\n",
    "\n",
    "def adjusted_rand_index(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Adjusted Rand Index (ARI)\n",
    "    \n",
    "    Adjusted for chance version of the Rand Index.\n",
    "    \n",
    "    Definition: ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
    "    where RI = Rand Index\n",
    "          Expected_RI = expected value of RI under random assignment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ari : float\n",
    "        Adjusted Rand Index (-1 to 1, higher is better, 0 = random)\n",
    "    \"\"\"\n",
    "    n = len(original_label)\n",
    "    \n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Create contingency table\n",
    "    true_clusters = np.unique(original_label)\n",
    "    pred_clusters = np.unique(pred_label)\n",
    "    \n",
    "    # Create contingency matrix\n",
    "    contingency = np.zeros((len(true_clusters), len(pred_clusters)), dtype=int)\n",
    "    \n",
    "    # Map clusters to indices\n",
    "    true_to_idx = {cluster: idx for idx, cluster in enumerate(true_clusters)}\n",
    "    pred_to_idx = {cluster: idx for idx, cluster in enumerate(pred_clusters)}\n",
    "    \n",
    "    # Fill contingency matrix\n",
    "    for i in range(n):\n",
    "        true_idx = true_to_idx[original_label[i]]\n",
    "        pred_idx = pred_to_idx[pred_label[i]]\n",
    "        contingency[true_idx, pred_idx] += 1\n",
    "    \n",
    "    # Calculate row and column sums\n",
    "    a = contingency.sum(axis=1)  # Row sums\n",
    "    b = contingency.sum(axis=0)  # Column sums\n",
    "    \n",
    "    # Calculate index (sum over cells of n_ij choose 2)\n",
    "    index = np.sum([comb(n_ij, 2) for n_ij in contingency.flatten() if n_ij >= 2])\n",
    "    \n",
    "    # Calculate expected index\n",
    "    sum_ai = np.sum([comb(ai, 2) for ai in a if ai >= 2])\n",
    "    sum_bj = np.sum([comb(bj, 2) for bj in b if bj >= 2])\n",
    "    expected_index = sum_ai * sum_bj / comb(n, 2)\n",
    "    \n",
    "    # Calculate max index\n",
    "    max_index = (sum_ai + sum_bj) / 2\n",
    "    \n",
    "    # Calculate ARI\n",
    "    if max_index == expected_index:\n",
    "        return 0.0\n",
    "    else:\n",
    "        ari = (index - expected_index) / (max_index - expected_index)\n",
    "        return ari\n",
    "\n",
    "def compute_all_metrics(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Compute all 6 clustering metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments (-1 for outliers)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Evidential clustering metrics (simplified)\n",
    "    metrics['Re'] = Re(original_label, pred_label)\n",
    "    metrics['Ri'] = Ri(original_label, pred_label)  # Always 0\n",
    "    metrics['EP'] = EP(original_label, pred_label)\n",
    "    metrics['ERI'] = ERI(original_label, pred_label)\n",
    "    \n",
    "    # Standard clustering metrics\n",
    "    metrics['RI'] = rand_index(original_label, pred_label)\n",
    "    metrics['ARI'] = adjusted_rand_index(original_label, pred_label)\n",
    "    \n",
    "    return metrics"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1769068019794,
  "creator": "2018509",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env offus)",
   "language": "python",
   "name": "py-dku-venv-offus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "modifiedBy": "2018509",
  "tags": [],
  "versionNumber": 1
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
