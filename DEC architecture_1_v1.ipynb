{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/dataiku/DATA_DESIGNER/code-envs/python/offus/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "metrics.py\n",
    "Clustering evaluation metrics for algorithms that assign single labels (no meta-clusters)\n",
    "-1 indicates outliers\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.special import comb\n",
    "\n",
    "def Re(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Error Rate (Re)\n",
    "    \n",
    "    Calculates the ratio of misclassifications to total number of objects.\n",
    "    \n",
    "    Definition: Re = Ne / N\n",
    "    where Ne = number of misclassifications\n",
    "          N  = total number of objects\n",
    "    \n",
    "    Misclassification occurs when:\n",
    "    1. pred_label[i] != original_label[i] (wrong cluster assignment)\n",
    "    2. pred_label[i] == -1 (outlier) but original_label[i] is not an outlier*\n",
    "    \n",
    "    *Note: Assumes original_label has no outlier class (-1)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments (non-negative integers)\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments (-1 for outliers)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    re : float\n",
    "        Error rate (0 to 1, lower is better)\n",
    "    \"\"\"\n",
    "    n = len(original_label)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    ne = 0  # Number of misclassifications\n",
    "    \n",
    "    for i in range(n):\n",
    "        true_cluster = original_label[i]\n",
    "        pred_cluster = pred_label[i]\n",
    "        \n",
    "        # Misclassification if:\n",
    "        # 1. Wrong cluster assignment\n",
    "        # 2. Predicted as outlier (-1) but true is not outlier\n",
    "        if pred_cluster == -1:\n",
    "            # If predicted as outlier, always misclassification \n",
    "            # (since original_label has no outlier class)\n",
    "            ne += 1\n",
    "        elif pred_cluster != true_cluster:\n",
    "            ne += 1\n",
    "    \n",
    "    return ne / n\n",
    "\n",
    "def Ri(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Imprecision Rate (Ri)\n",
    "    \n",
    "    In your algorithm without meta-clusters, this should always return 0\n",
    "    since there are no meta-cluster assignments.\n",
    "    \n",
    "    Definition: Ri = Ni / N\n",
    "    where Ni = number of objects in meta-clusters\n",
    "          N  = total number of objects\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ri : float\n",
    "        Always 0 (no meta-clusters)\n",
    "    \"\"\"\n",
    "    return 0.0\n",
    "\n",
    "def EP(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Evidential Precision (EP) - Simplified for single label assignments\n",
    "    \n",
    "    Measures precision of non-outlier assignments.\n",
    "    \n",
    "    Definition: EP = TP* / (TP* + FP*)\n",
    "    where TP* = number of pairs of similar objects (same true cluster) \n",
    "                that are both assigned to the same cluster (and not outliers)\n",
    "          FP* = number of pairs of dissimilar objects (different true clusters)\n",
    "                that are both assigned to the same cluster (and not outliers)\n",
    "    \n",
    "    Only considers pairs where both objects are NOT outliers (pred_label != -1)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments (-1 for outliers)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ep : float\n",
    "        Evidential precision (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    n = len(original_label)\n",
    "    \n",
    "    # Find indices of non-outlier predictions\n",
    "    non_outlier_indices = [i for i in range(n) if pred_label[i] != -1]\n",
    "    \n",
    "    if len(non_outlier_indices) < 2:\n",
    "        return 0.0  # Need at least 2 non-outliers to compute pairs\n",
    "    \n",
    "    tp_star = 0  # True positive pairs (similar objects in same cluster)\n",
    "    fp_star = 0  # False positive pairs (dissimilar objects in same cluster)\n",
    "    \n",
    "    # Consider all pairs of non-outlier objects\n",
    "    for i, j in combinations(non_outlier_indices, 2):\n",
    "        if i >= j:\n",
    "            continue\n",
    "            \n",
    "        true_i = original_label[i]\n",
    "        true_j = original_label[j]\n",
    "        pred_i = pred_label[i]\n",
    "        pred_j = pred_label[j]\n",
    "        \n",
    "        # Check if assigned to the same cluster\n",
    "        if pred_i == pred_j:\n",
    "            if true_i == true_j:\n",
    "                tp_star += 1\n",
    "            else:\n",
    "                fp_star += 1\n",
    "    \n",
    "    denominator = tp_star + fp_star\n",
    "    return tp_star / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def ERI(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Evidential Rank Index (ERI) - Simplified for single label assignments\n",
    "    \n",
    "    Comprehensive evaluation considering both beneficial and detrimental elements.\n",
    "    \n",
    "    Definition: ERI = 2(TP* + TN*) / [N(N-1)]\n",
    "    where TP* = number of pairs of similar objects (same true cluster) \n",
    "                that are both assigned to the same cluster (and not outliers)\n",
    "          TN* = number of pairs of dissimilar objects (different true clusters)\n",
    "                that are assigned to different clusters (and both not outliers)\n",
    "          N   = total number of objects\n",
    "    \n",
    "    Only considers pairs where both objects are NOT outliers (pred_label != -1)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments (-1 for outliers)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    eri : float\n",
    "        Evidential rank index (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    n = len(original_label)\n",
    "    \n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Find indices of non-outlier predictions\n",
    "    non_outlier_indices = [i for i in range(n) if pred_label[i] != -1]\n",
    "    \n",
    "    tp_star = 0  # True positive pairs\n",
    "    tn_star = 0  # True negative pairs\n",
    "    \n",
    "    # Consider all pairs of non-outlier objects\n",
    "    for i, j in combinations(non_outlier_indices, 2):\n",
    "        if i >= j:\n",
    "            continue\n",
    "            \n",
    "        true_i = original_label[i]\n",
    "        true_j = original_label[j]\n",
    "        pred_i = pred_label[i]\n",
    "        pred_j = pred_label[j]\n",
    "        \n",
    "        same_true = (true_i == true_j)\n",
    "        same_pred = (pred_i == pred_j)\n",
    "        \n",
    "        if same_true and same_pred:\n",
    "            tp_star += 1\n",
    "        elif (not same_true) and (not same_pred):\n",
    "            tn_star += 1\n",
    "    \n",
    "    total_pairs = n * (n - 1)\n",
    "    return 2 * (tp_star + tn_star) / total_pairs if total_pairs > 0 else 0.0\n",
    "\n",
    "def rand_index(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Rand Index (RI) - Standard version\n",
    "    \n",
    "    Measures the similarity between two data clusterings.\n",
    "    \n",
    "    Definition: RI = (TP + TN) / (TP + FP + FN + TN)\n",
    "    where TP = number of pairs of similar objects (same true cluster) \n",
    "               that are both assigned to the same predicted cluster\n",
    "          TN = number of pairs of dissimilar objects (different true clusters)\n",
    "               that are assigned to different predicted clusters\n",
    "          FP = number of pairs of dissimilar objects \n",
    "               that are assigned to the same predicted cluster\n",
    "          FN = number of pairs of similar objects \n",
    "               that are assigned to different predicted clusters\n",
    "    \n",
    "    Treats -1 (outliers) as a separate cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ri : float\n",
    "        Rand Index (0 to 1, higher is better)\n",
    "    \"\"\"\n",
    "    n = len(original_label)\n",
    "    \n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    tp = 0  # True positives\n",
    "    tn = 0  # True negatives\n",
    "    fp = 0  # False positives\n",
    "    fn = 0  # False negatives\n",
    "    \n",
    "    # Consider all pairs of objects\n",
    "    for i, j in combinations(range(n), 2):\n",
    "        if i >= j:\n",
    "            continue\n",
    "            \n",
    "        true_i = original_label[i]\n",
    "        true_j = original_label[j]\n",
    "        pred_i = pred_label[i]\n",
    "        pred_j = pred_label[j]\n",
    "        \n",
    "        same_true = (true_i == true_j)\n",
    "        same_pred = (pred_i == pred_j)\n",
    "        \n",
    "        if same_true and same_pred:\n",
    "            tp += 1\n",
    "        elif same_true and not same_pred:\n",
    "            fn += 1\n",
    "        elif not same_true and same_pred:\n",
    "            fp += 1\n",
    "        else:  # not same_true and not same_pred\n",
    "            tn += 1\n",
    "    \n",
    "    total = tp + tn + fp + fn\n",
    "    return (tp + tn) / total if total > 0 else 0.0\n",
    "\n",
    "def adjusted_rand_index(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Adjusted Rand Index (ARI)\n",
    "    \n",
    "    Adjusted for chance version of the Rand Index.\n",
    "    \n",
    "    Definition: ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
    "    where RI = Rand Index\n",
    "          Expected_RI = expected value of RI under random assignment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ari : float\n",
    "        Adjusted Rand Index (-1 to 1, higher is better, 0 = random)\n",
    "    \"\"\"\n",
    "    n = len(original_label)\n",
    "    \n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Create contingency table\n",
    "    true_clusters = np.unique(original_label)\n",
    "    pred_clusters = np.unique(pred_label)\n",
    "    \n",
    "    # Create contingency matrix\n",
    "    contingency = np.zeros((len(true_clusters), len(pred_clusters)), dtype=int)\n",
    "    \n",
    "    # Map clusters to indices\n",
    "    true_to_idx = {cluster: idx for idx, cluster in enumerate(true_clusters)}\n",
    "    pred_to_idx = {cluster: idx for idx, cluster in enumerate(pred_clusters)}\n",
    "    \n",
    "    # Fill contingency matrix\n",
    "    for i in range(n):\n",
    "        true_idx = true_to_idx[original_label[i]]\n",
    "        pred_idx = pred_to_idx[pred_label[i]]\n",
    "        contingency[true_idx, pred_idx] += 1\n",
    "    \n",
    "    # Calculate row and column sums\n",
    "    a = contingency.sum(axis=1)  # Row sums\n",
    "    b = contingency.sum(axis=0)  # Column sums\n",
    "    \n",
    "    # Calculate index (sum over cells of n_ij choose 2)\n",
    "    index = np.sum([comb(n_ij, 2) for n_ij in contingency.flatten() if n_ij >= 2])\n",
    "    \n",
    "    # Calculate expected index\n",
    "    sum_ai = np.sum([comb(ai, 2) for ai in a if ai >= 2])\n",
    "    sum_bj = np.sum([comb(bj, 2) for bj in b if bj >= 2])\n",
    "    expected_index = sum_ai * sum_bj / comb(n, 2)\n",
    "    \n",
    "    # Calculate max index\n",
    "    max_index = (sum_ai + sum_bj) / 2\n",
    "    \n",
    "    # Calculate ARI\n",
    "    if max_index == expected_index:\n",
    "        return 0.0\n",
    "    else:\n",
    "        ari = (index - expected_index) / (max_index - expected_index)\n",
    "        return ari\n",
    "\n",
    "def compute_all_metrics(original_label, pred_label):\n",
    "    \"\"\"\n",
    "    Compute all 6 clustering metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_label : array-like, shape (n_samples,)\n",
    "        Ground truth cluster assignments\n",
    "    pred_label : array-like, shape (n_samples,)\n",
    "        Predicted cluster assignments (-1 for outliers)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Evidential clustering metrics (simplified)\n",
    "    metrics['Re'] = Re(original_label, pred_label)\n",
    "    metrics['Ri'] = Ri(original_label, pred_label)  # Always 0\n",
    "    metrics['EP'] = EP(original_label, pred_label)\n",
    "    metrics['ERI'] = ERI(original_label, pred_label)\n",
    "    \n",
    "    # Standard clustering metrics\n",
    "    metrics['RI'] = rand_index(original_label, pred_label)\n",
    "    metrics['ARI'] = adjusted_rand_index(original_label, pred_label)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Masked Variational Autoencoder\n",
    "# =========================\n",
    "\n",
    "class MaskedVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Variational Autoencoder for incomplete data.\n",
    "    Encoder input: [x âŠ™ m , m]\n",
    "    Latent output: mu (used as latent feature u_i)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: input_dim * 2 because of concatenation with mask\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, mask):\n",
    "        x_masked = x * mask\n",
    "        enc_input = torch.cat([x_masked, mask], dim=1)\n",
    "        h = self.encoder(enc_input)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mu, logvar = self.encode(x, mask)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Masked VAE Loss\n",
    "# =========================\n",
    "\n",
    "def masked_vae_loss(\n",
    "    recon_x,\n",
    "    x,\n",
    "    mask,\n",
    "    mu,\n",
    "    logvar,\n",
    "    recon_weight=0.9999,\n",
    "    kl_weight=0.0001,\n",
    "    eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Masked reconstruction + KL divergence loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Masked reconstruction loss (MSE over observed entries only)\n",
    "    se = (recon_x - x) ** 2\n",
    "    masked_se = se * mask\n",
    "    recon_loss = masked_se.sum(dim=1) / (mask.sum(dim=1) + eps)\n",
    "    recon_loss = recon_loss.mean()\n",
    "\n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * torch.mean(\n",
    "        torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    )\n",
    "\n",
    "    total_loss = recon_weight * recon_loss + kl_weight * kl_loss\n",
    "    return total_loss, recon_loss.item(), kl_loss.item()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Training Function\n",
    "# =========================\n",
    "\n",
    "def train_masked_vae(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    epochs=50,\n",
    "    lr=1e-3\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        total_rec = 0.0\n",
    "        total_kl = 0.0\n",
    "\n",
    "        for x, mask in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            mask = mask.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x, mask)\n",
    "            loss, rec, kl = masked_vae_loss(recon, x, mask, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_rec += rec\n",
    "            total_kl += kl\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Loss: {total_loss:.4f} | \"\n",
    "            f\"Recon: {total_rec:.4f} | \"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Latent Feature Extraction\n",
    "# =========================\n",
    "\n",
    "def extract_latent_features(model, X, mask, device):\n",
    "    \"\"\"\n",
    "    Returns latent feature vectors u_i = mu_i\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = X.to(device).float()\n",
    "        mask = mask.to(device).float()\n",
    "        mu, _ = model.encode(X, mask)\n",
    "    return mu.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Load Iris Dataset\n",
    "# ============================================\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df[\"target\"] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Inject Missing Values (MCAR)\n",
    "# ============================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = df.shape[0]\n",
    "missing_ratio = 0.0\n",
    "n_missing_rows = int(missing_ratio * n_samples)\n",
    "\n",
    "missing_rows = np.random.choice(df.index, size=n_missing_rows, replace=False)\n",
    "\n",
    "for row in missing_rows:\n",
    "    n_cols_missing = np.random.randint(1, len(iris.feature_names))\n",
    "    cols_missing = np.random.choice(\n",
    "        iris.feature_names, size=n_cols_missing, replace=False\n",
    "    )\n",
    "    df.loc[row, cols_missing] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Create Feature Matrix and Mask\n",
    "# ============================================\n",
    "\n",
    "X = df[iris.feature_names].values.astype(np.float32)\n",
    "\n",
    "# Binary mask: 1 = observed, 0 = missing\n",
    "mask = (~np.isnan(X)).astype(np.float32)\n",
    "\n",
    "# Fill missing values with zero (mask-aware)\n",
    "X_filled = np.nan_to_num(X, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. Mask-aware Standardization\n",
    "# ============================================\n",
    "class TorchScaler:\n",
    "    def fit(self, X: torch.Tensor):\n",
    "        self.mean = X.mean(dim=0, keepdim=True)\n",
    "        self.std = X.std(dim=0, unbiased=False, keepdim=True)\n",
    "        self.std[self.std == 0] = 1.0\n",
    "        return self\n",
    "    def transform(self, X: torch.Tensor):\n",
    "        return (X - self.mean) / self.std\n",
    "    def inverse_transform(self, X: torch.Tensor):\n",
    "        return X * self.std + self.mean\n",
    "\n",
    "X_scaled = X_filled.copy()\n",
    "X_tensor = torch.from_numpy(np.array(X_scaled))\n",
    "scaler = TorchScaler().fit(X_tensor)\n",
    "X_scaled = scaler.transform(X_tensor)\n",
    "mask_tensor = torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_432313/2934702538.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 5. PyTorch Dataset and DataLoader\n",
    "# ============================================\n",
    "    \n",
    "class MaskedDataset:\n",
    "    def __init__(self, X, mask):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.mask[idx]\n",
    "\n",
    "\n",
    "dataset = MaskedDataset(X_tensor, mask)\n",
    "torch.manual_seed(42)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAssumes your DataLoader yields (x, mask)\\nx    : tensor of shape [batch_size, input_dim]\\nmask : tensor of shape [batch_size, input_dim], binary {0,1}\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Example Usage (Skeleton)\n",
    "# =========================\n",
    "\"\"\"\n",
    "Assumes your DataLoader yields (x, mask)\n",
    "x    : tensor of shape [batch_size, input_dim]\n",
    "mask : tensor of shape [batch_size, input_dim], binary {0,1}\n",
    "\"\"\"\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = MaskedVAE(input_dim=INPUT_DIM, latent_dim=LATENT_DIM)\n",
    "# model = train_masked_vae(model, dataloader, device)\n",
    "# U = extract_latent_features(model, X_full, mask_full, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 73.9595 | Recon: 73.9667 | \n",
      "Epoch 002 | Loss: 73.0749 | Recon: 73.0820 | \n",
      "Epoch 003 | Loss: 73.1223 | Recon: 73.1295 | \n",
      "Epoch 004 | Loss: 73.3170 | Recon: 73.3241 | \n",
      "Epoch 005 | Loss: 72.7025 | Recon: 72.7096 | \n",
      "Epoch 006 | Loss: 72.1613 | Recon: 72.1683 | \n",
      "Epoch 007 | Loss: 71.6517 | Recon: 71.6588 | \n",
      "Epoch 008 | Loss: 71.2895 | Recon: 71.2965 | \n",
      "Epoch 009 | Loss: 71.9160 | Recon: 71.9230 | \n",
      "Epoch 010 | Loss: 71.1367 | Recon: 71.1436 | \n",
      "Epoch 011 | Loss: 70.6394 | Recon: 70.6463 | \n",
      "Epoch 012 | Loss: 70.6905 | Recon: 70.6974 | \n",
      "Epoch 013 | Loss: 70.4336 | Recon: 70.4405 | \n",
      "Epoch 014 | Loss: 70.1265 | Recon: 70.1334 | \n",
      "Epoch 015 | Loss: 69.5880 | Recon: 69.5947 | \n",
      "Epoch 016 | Loss: 69.3607 | Recon: 69.3674 | \n",
      "Epoch 017 | Loss: 68.5534 | Recon: 68.5600 | \n",
      "Epoch 018 | Loss: 68.5506 | Recon: 68.5571 | \n",
      "Epoch 019 | Loss: 67.9993 | Recon: 68.0056 | \n",
      "Epoch 020 | Loss: 67.3226 | Recon: 67.3287 | \n",
      "Epoch 021 | Loss: 67.1909 | Recon: 67.1968 | \n",
      "Epoch 022 | Loss: 65.8484 | Recon: 65.8538 | \n",
      "Epoch 023 | Loss: 65.1757 | Recon: 65.1806 | \n",
      "Epoch 024 | Loss: 63.2169 | Recon: 63.2209 | \n",
      "Epoch 025 | Loss: 63.0512 | Recon: 63.0541 | \n",
      "Epoch 026 | Loss: 59.7359 | Recon: 59.7369 | \n",
      "Epoch 027 | Loss: 57.2553 | Recon: 57.2536 | \n",
      "Epoch 028 | Loss: 53.7666 | Recon: 53.7605 | \n",
      "Epoch 029 | Loss: 49.7474 | Recon: 49.7337 | \n",
      "Epoch 030 | Loss: 42.7796 | Recon: 42.7513 | \n",
      "Epoch 031 | Loss: 37.0038 | Recon: 36.9436 | \n",
      "Epoch 032 | Loss: 31.2737 | Recon: 31.1481 | \n",
      "Epoch 033 | Loss: 29.2531 | Recon: 29.0200 | \n",
      "Epoch 034 | Loss: 28.3907 | Recon: 28.1631 | \n",
      "Epoch 035 | Loss: 22.5582 | Recon: 22.4066 | \n",
      "Epoch 036 | Loss: 20.6326 | Recon: 20.5149 | \n",
      "Epoch 037 | Loss: 19.1173 | Recon: 19.0087 | \n",
      "Epoch 038 | Loss: 15.4050 | Recon: 15.2940 | \n",
      "Epoch 039 | Loss: 12.7320 | Recon: 12.6072 | \n",
      "Epoch 040 | Loss: 10.9629 | Recon: 10.8280 | \n",
      "Epoch 041 | Loss: 9.4685 | Recon: 9.3313 | \n",
      "Epoch 042 | Loss: 7.8330 | Recon: 7.7041 | \n",
      "Epoch 043 | Loss: 6.6277 | Recon: 6.5062 | \n",
      "Epoch 044 | Loss: 5.9478 | Recon: 5.8306 | \n",
      "Epoch 045 | Loss: 5.2767 | Recon: 5.1677 | \n",
      "Epoch 046 | Loss: 4.6961 | Recon: 4.5933 | \n",
      "Epoch 047 | Loss: 4.1788 | Recon: 4.0842 | \n",
      "Epoch 048 | Loss: 3.7193 | Recon: 3.6283 | \n",
      "Epoch 049 | Loss: 3.6428 | Recon: 3.5515 | \n",
      "Epoch 050 | Loss: 3.4610 | Recon: 3.3713 | \n",
      "Epoch 051 | Loss: 3.6653 | Recon: 3.5796 | \n",
      "Epoch 052 | Loss: 3.3911 | Recon: 3.3079 | \n",
      "Epoch 053 | Loss: 3.3980 | Recon: 3.3168 | \n",
      "Epoch 054 | Loss: 3.4957 | Recon: 3.4156 | \n",
      "Epoch 055 | Loss: 3.3231 | Recon: 3.2444 | \n",
      "Epoch 056 | Loss: 3.3720 | Recon: 3.2947 | \n",
      "Epoch 057 | Loss: 3.2648 | Recon: 3.1889 | \n",
      "Epoch 058 | Loss: 3.1675 | Recon: 3.0921 | \n",
      "Epoch 059 | Loss: 3.1845 | Recon: 3.1094 | \n",
      "Epoch 060 | Loss: 3.2333 | Recon: 3.1594 | \n",
      "Epoch 061 | Loss: 3.2848 | Recon: 3.2109 | \n",
      "Epoch 062 | Loss: 3.2031 | Recon: 3.1305 | \n",
      "Epoch 063 | Loss: 3.2476 | Recon: 3.1752 | \n",
      "Epoch 064 | Loss: 3.1195 | Recon: 3.0481 | \n",
      "Epoch 065 | Loss: 3.3077 | Recon: 3.2362 | \n",
      "Epoch 066 | Loss: 3.2780 | Recon: 3.2079 | \n",
      "Epoch 067 | Loss: 3.1202 | Recon: 3.0511 | \n",
      "Epoch 068 | Loss: 3.1402 | Recon: 3.0719 | \n",
      "Epoch 069 | Loss: 3.2382 | Recon: 3.1701 | \n",
      "Epoch 070 | Loss: 3.1749 | Recon: 3.1075 | \n",
      "Epoch 071 | Loss: 3.2039 | Recon: 3.1367 | \n",
      "Epoch 072 | Loss: 3.1292 | Recon: 3.0627 | \n",
      "Epoch 073 | Loss: 3.1442 | Recon: 3.0775 | \n",
      "Epoch 074 | Loss: 3.2366 | Recon: 3.1705 | \n",
      "Epoch 075 | Loss: 3.1300 | Recon: 3.0652 | \n",
      "Epoch 076 | Loss: 3.1226 | Recon: 3.0580 | \n",
      "Epoch 077 | Loss: 3.1016 | Recon: 3.0363 | \n",
      "Epoch 078 | Loss: 3.1272 | Recon: 3.0628 | \n",
      "Epoch 079 | Loss: 3.1453 | Recon: 3.0821 | \n",
      "Epoch 080 | Loss: 3.2326 | Recon: 3.1704 | \n",
      "Epoch 081 | Loss: 3.2150 | Recon: 3.1529 | \n",
      "Epoch 082 | Loss: 3.0865 | Recon: 3.0241 | \n",
      "Epoch 083 | Loss: 3.0656 | Recon: 3.0039 | \n",
      "Epoch 084 | Loss: 3.0215 | Recon: 2.9590 | \n",
      "Epoch 085 | Loss: 3.1035 | Recon: 3.0419 | \n",
      "Epoch 086 | Loss: 3.0568 | Recon: 2.9953 | \n",
      "Epoch 087 | Loss: 3.0127 | Recon: 2.9517 | \n",
      "Epoch 088 | Loss: 2.9829 | Recon: 2.9220 | \n",
      "Epoch 089 | Loss: 2.9821 | Recon: 2.9216 | \n",
      "Epoch 090 | Loss: 2.9940 | Recon: 2.9331 | \n",
      "Epoch 091 | Loss: 3.0465 | Recon: 2.9857 | \n",
      "Epoch 092 | Loss: 3.0517 | Recon: 2.9911 | \n",
      "Epoch 093 | Loss: 3.0430 | Recon: 2.9832 | \n",
      "Epoch 094 | Loss: 2.9776 | Recon: 2.9175 | \n",
      "Epoch 095 | Loss: 2.9290 | Recon: 2.8686 | \n",
      "Epoch 096 | Loss: 3.0178 | Recon: 2.9584 | \n",
      "Epoch 097 | Loss: 2.9402 | Recon: 2.8803 | \n",
      "Epoch 098 | Loss: 2.9852 | Recon: 2.9257 | \n",
      "Epoch 099 | Loss: 2.9974 | Recon: 2.9384 | \n",
      "Epoch 100 | Loss: 2.9193 | Recon: 2.8603 | \n",
      "Epoch 101 | Loss: 2.9464 | Recon: 2.8872 | \n",
      "Epoch 102 | Loss: 2.9530 | Recon: 2.8941 | \n",
      "Epoch 103 | Loss: 2.9503 | Recon: 2.8911 | \n",
      "Epoch 104 | Loss: 2.9519 | Recon: 2.8929 | \n",
      "Epoch 105 | Loss: 2.9072 | Recon: 2.8486 | \n",
      "Epoch 106 | Loss: 3.0292 | Recon: 2.9711 | \n",
      "Epoch 107 | Loss: 2.9187 | Recon: 2.8604 | \n",
      "Epoch 108 | Loss: 2.9124 | Recon: 2.8542 | \n",
      "Epoch 109 | Loss: 2.8921 | Recon: 2.8337 | \n",
      "Epoch 110 | Loss: 2.9014 | Recon: 2.8434 | \n",
      "Epoch 111 | Loss: 2.9442 | Recon: 2.8861 | \n",
      "Epoch 112 | Loss: 2.8499 | Recon: 2.7926 | \n",
      "Epoch 113 | Loss: 2.8930 | Recon: 2.8360 | \n",
      "Epoch 114 | Loss: 2.8981 | Recon: 2.8408 | \n",
      "Epoch 115 | Loss: 2.8601 | Recon: 2.8028 | \n",
      "Epoch 116 | Loss: 2.8608 | Recon: 2.8040 | \n",
      "Epoch 117 | Loss: 2.8156 | Recon: 2.7588 | \n",
      "Epoch 118 | Loss: 2.8465 | Recon: 2.7900 | \n",
      "Epoch 119 | Loss: 2.8332 | Recon: 2.7763 | \n",
      "Epoch 120 | Loss: 2.8126 | Recon: 2.7556 | \n",
      "Epoch 121 | Loss: 2.8433 | Recon: 2.7866 | \n",
      "Epoch 122 | Loss: 2.8142 | Recon: 2.7580 | \n",
      "Epoch 123 | Loss: 2.8207 | Recon: 2.7648 | \n",
      "Epoch 124 | Loss: 2.9033 | Recon: 2.8473 | \n",
      "Epoch 125 | Loss: 2.8137 | Recon: 2.7579 | \n",
      "Epoch 126 | Loss: 2.7862 | Recon: 2.7302 | \n",
      "Epoch 127 | Loss: 2.8160 | Recon: 2.7610 | \n",
      "Epoch 128 | Loss: 2.7964 | Recon: 2.7409 | \n",
      "Epoch 129 | Loss: 2.8020 | Recon: 2.7471 | \n",
      "Epoch 130 | Loss: 2.8494 | Recon: 2.7948 | \n",
      "Epoch 131 | Loss: 2.7824 | Recon: 2.7272 | \n",
      "Epoch 132 | Loss: 2.7402 | Recon: 2.6847 | \n",
      "Epoch 133 | Loss: 2.8165 | Recon: 2.7621 | \n",
      "Epoch 134 | Loss: 2.7649 | Recon: 2.7097 | \n",
      "Epoch 135 | Loss: 2.7934 | Recon: 2.7386 | \n",
      "Epoch 136 | Loss: 2.7295 | Recon: 2.6745 | \n",
      "Epoch 137 | Loss: 2.7016 | Recon: 2.6466 | \n",
      "Epoch 138 | Loss: 2.6929 | Recon: 2.6384 | \n",
      "Epoch 139 | Loss: 2.7242 | Recon: 2.6695 | \n",
      "Epoch 140 | Loss: 2.7398 | Recon: 2.6854 | \n",
      "Epoch 141 | Loss: 2.7209 | Recon: 2.6664 | \n",
      "Epoch 142 | Loss: 2.7189 | Recon: 2.6648 | \n",
      "Epoch 143 | Loss: 2.6761 | Recon: 2.6217 | \n",
      "Epoch 144 | Loss: 2.7427 | Recon: 2.6886 | \n",
      "Epoch 145 | Loss: 2.6880 | Recon: 2.6339 | \n",
      "Epoch 146 | Loss: 2.7133 | Recon: 2.6595 | \n",
      "Epoch 147 | Loss: 2.7086 | Recon: 2.6553 | \n",
      "Epoch 148 | Loss: 2.7092 | Recon: 2.6557 | \n",
      "Epoch 149 | Loss: 2.7124 | Recon: 2.6589 | \n",
      "Epoch 150 | Loss: 2.7024 | Recon: 2.6492 | \n",
      "Epoch 151 | Loss: 2.6732 | Recon: 2.6197 | \n",
      "Epoch 152 | Loss: 2.7172 | Recon: 2.6637 | \n",
      "Epoch 153 | Loss: 2.6890 | Recon: 2.6357 | \n",
      "Epoch 154 | Loss: 2.6702 | Recon: 2.6173 | \n",
      "Epoch 155 | Loss: 2.6550 | Recon: 2.6020 | \n",
      "Epoch 156 | Loss: 2.7024 | Recon: 2.6500 | \n",
      "Epoch 157 | Loss: 2.7089 | Recon: 2.6567 | \n",
      "Epoch 158 | Loss: 2.6523 | Recon: 2.5997 | \n",
      "Epoch 159 | Loss: 2.6641 | Recon: 2.6122 | \n",
      "Epoch 160 | Loss: 2.6041 | Recon: 2.5518 | \n",
      "Epoch 161 | Loss: 2.6311 | Recon: 2.5786 | \n",
      "Epoch 162 | Loss: 2.6526 | Recon: 2.6003 | \n",
      "Epoch 163 | Loss: 2.6371 | Recon: 2.5849 | \n",
      "Epoch 164 | Loss: 2.6104 | Recon: 2.5583 | \n",
      "Epoch 165 | Loss: 2.6484 | Recon: 2.5968 | \n",
      "Epoch 166 | Loss: 2.6292 | Recon: 2.5776 | \n",
      "Epoch 167 | Loss: 2.6471 | Recon: 2.5956 | \n",
      "Epoch 168 | Loss: 2.6235 | Recon: 2.5718 | \n",
      "Epoch 169 | Loss: 2.6263 | Recon: 2.5748 | \n",
      "Epoch 170 | Loss: 2.6702 | Recon: 2.6190 | \n",
      "Epoch 171 | Loss: 2.5845 | Recon: 2.5327 | \n",
      "Epoch 172 | Loss: 2.6371 | Recon: 2.5863 | \n",
      "Epoch 173 | Loss: 2.6034 | Recon: 2.5525 | \n",
      "Epoch 174 | Loss: 2.7092 | Recon: 2.6589 | \n",
      "Epoch 175 | Loss: 2.7011 | Recon: 2.6504 | \n",
      "Epoch 176 | Loss: 2.5797 | Recon: 2.5289 | \n",
      "Epoch 177 | Loss: 2.6351 | Recon: 2.5846 | \n",
      "Epoch 178 | Loss: 2.5790 | Recon: 2.5286 | \n",
      "Epoch 179 | Loss: 2.6157 | Recon: 2.5651 | \n",
      "Epoch 180 | Loss: 2.6472 | Recon: 2.5969 | \n",
      "Epoch 181 | Loss: 2.5611 | Recon: 2.5114 | \n",
      "Epoch 182 | Loss: 2.5374 | Recon: 2.4870 | \n",
      "Epoch 183 | Loss: 2.5693 | Recon: 2.5194 | \n",
      "Epoch 184 | Loss: 2.5798 | Recon: 2.5302 | \n",
      "Epoch 185 | Loss: 2.5529 | Recon: 2.5031 | \n",
      "Epoch 186 | Loss: 2.5322 | Recon: 2.4822 | \n",
      "Epoch 187 | Loss: 2.5837 | Recon: 2.5344 | \n",
      "Epoch 188 | Loss: 2.5702 | Recon: 2.5209 | \n",
      "Epoch 189 | Loss: 2.5612 | Recon: 2.5119 | \n",
      "Epoch 190 | Loss: 2.5400 | Recon: 2.4906 | \n",
      "Epoch 191 | Loss: 2.5337 | Recon: 2.4844 | \n",
      "Epoch 192 | Loss: 2.5693 | Recon: 2.5204 | \n",
      "Epoch 193 | Loss: 2.4895 | Recon: 2.4400 | \n",
      "Epoch 194 | Loss: 2.5268 | Recon: 2.4776 | \n",
      "Epoch 195 | Loss: 2.5433 | Recon: 2.4940 | \n",
      "Epoch 196 | Loss: 2.5333 | Recon: 2.4842 | \n",
      "Epoch 197 | Loss: 2.5279 | Recon: 2.4790 | \n",
      "Epoch 198 | Loss: 2.6080 | Recon: 2.5600 | \n",
      "Epoch 199 | Loss: 2.5268 | Recon: 2.4784 | \n",
      "Epoch 200 | Loss: 2.5041 | Recon: 2.4553 | \n",
      "Epoch 201 | Loss: 2.5229 | Recon: 2.4747 | \n",
      "Epoch 202 | Loss: 2.5076 | Recon: 2.4596 | \n",
      "Epoch 203 | Loss: 2.5201 | Recon: 2.4725 | \n",
      "Epoch 204 | Loss: 2.5180 | Recon: 2.4699 | \n",
      "Epoch 205 | Loss: 2.5152 | Recon: 2.4671 | \n",
      "Epoch 206 | Loss: 2.5510 | Recon: 2.5035 | \n",
      "Epoch 207 | Loss: 2.4591 | Recon: 2.4115 | \n",
      "Epoch 208 | Loss: 2.5489 | Recon: 2.5018 | \n",
      "Epoch 209 | Loss: 2.4962 | Recon: 2.4489 | \n",
      "Epoch 210 | Loss: 2.5016 | Recon: 2.4544 | \n",
      "Epoch 211 | Loss: 2.5043 | Recon: 2.4572 | \n",
      "Epoch 212 | Loss: 2.4962 | Recon: 2.4493 | \n",
      "Epoch 213 | Loss: 2.4461 | Recon: 2.3993 | \n",
      "Epoch 214 | Loss: 2.4595 | Recon: 2.4124 | \n",
      "Epoch 215 | Loss: 2.5261 | Recon: 2.4794 | \n",
      "Epoch 216 | Loss: 2.5416 | Recon: 2.4955 | \n",
      "Epoch 217 | Loss: 2.4485 | Recon: 2.4024 | \n",
      "Epoch 218 | Loss: 2.4629 | Recon: 2.4165 | \n",
      "Epoch 219 | Loss: 2.4154 | Recon: 2.3690 | \n",
      "Epoch 220 | Loss: 2.5150 | Recon: 2.4687 | \n",
      "Epoch 221 | Loss: 2.5518 | Recon: 2.5062 | \n",
      "Epoch 222 | Loss: 2.4419 | Recon: 2.3954 | \n",
      "Epoch 223 | Loss: 2.4623 | Recon: 2.4164 | \n",
      "Epoch 224 | Loss: 2.4518 | Recon: 2.4053 | \n",
      "Epoch 225 | Loss: 2.4510 | Recon: 2.4052 | \n",
      "Epoch 226 | Loss: 2.5039 | Recon: 2.4582 | \n",
      "Epoch 227 | Loss: 2.4409 | Recon: 2.3950 | \n",
      "Epoch 228 | Loss: 2.4280 | Recon: 2.3825 | \n",
      "Epoch 229 | Loss: 2.4637 | Recon: 2.4181 | \n",
      "Epoch 230 | Loss: 2.4932 | Recon: 2.4478 | \n",
      "Epoch 231 | Loss: 2.4737 | Recon: 2.4281 | \n",
      "Epoch 232 | Loss: 2.4327 | Recon: 2.3872 | \n",
      "Epoch 233 | Loss: 2.4357 | Recon: 2.3904 | \n",
      "Epoch 234 | Loss: 2.4876 | Recon: 2.4426 | \n",
      "Epoch 235 | Loss: 2.4490 | Recon: 2.4037 | \n",
      "Epoch 236 | Loss: 2.4377 | Recon: 2.3926 | \n",
      "Epoch 237 | Loss: 2.4108 | Recon: 2.3656 | \n",
      "Epoch 238 | Loss: 2.4206 | Recon: 2.3755 | \n",
      "Epoch 239 | Loss: 2.4663 | Recon: 2.4219 | \n",
      "Epoch 240 | Loss: 2.4293 | Recon: 2.3848 | \n",
      "Epoch 241 | Loss: 2.4409 | Recon: 2.3964 | \n",
      "Epoch 242 | Loss: 2.4650 | Recon: 2.4210 | \n",
      "Epoch 243 | Loss: 2.4673 | Recon: 2.4228 | \n",
      "Epoch 244 | Loss: 2.3931 | Recon: 2.3479 | \n",
      "Epoch 245 | Loss: 2.3552 | Recon: 2.3102 | \n",
      "Epoch 246 | Loss: 2.4169 | Recon: 2.3725 | \n",
      "Epoch 247 | Loss: 2.4149 | Recon: 2.3702 | \n",
      "Epoch 248 | Loss: 2.3926 | Recon: 2.3485 | \n",
      "Epoch 249 | Loss: 2.4372 | Recon: 2.3926 | \n",
      "Epoch 250 | Loss: 2.4338 | Recon: 2.3896 | \n",
      "Epoch 251 | Loss: 2.3699 | Recon: 2.3253 | \n",
      "Epoch 252 | Loss: 2.3909 | Recon: 2.3468 | \n",
      "Epoch 253 | Loss: 2.4383 | Recon: 2.3945 | \n",
      "Epoch 254 | Loss: 2.3957 | Recon: 2.3517 | \n",
      "Epoch 255 | Loss: 2.3926 | Recon: 2.3488 | \n",
      "Epoch 256 | Loss: 2.3976 | Recon: 2.3544 | \n",
      "Epoch 257 | Loss: 2.3997 | Recon: 2.3558 | \n",
      "Epoch 258 | Loss: 2.3931 | Recon: 2.3499 | \n",
      "Epoch 259 | Loss: 2.3627 | Recon: 2.3195 | \n",
      "Epoch 260 | Loss: 2.3910 | Recon: 2.3474 | \n",
      "Epoch 261 | Loss: 2.3903 | Recon: 2.3468 | \n",
      "Epoch 262 | Loss: 2.4169 | Recon: 2.3737 | \n",
      "Epoch 263 | Loss: 2.4127 | Recon: 2.3693 | \n",
      "Epoch 264 | Loss: 2.3757 | Recon: 2.3322 | \n",
      "Epoch 265 | Loss: 2.3630 | Recon: 2.3200 | \n",
      "Epoch 266 | Loss: 2.3802 | Recon: 2.3372 | \n",
      "Epoch 267 | Loss: 2.3806 | Recon: 2.3378 | \n",
      "Epoch 268 | Loss: 2.3775 | Recon: 2.3346 | \n",
      "Epoch 269 | Loss: 2.4025 | Recon: 2.3600 | \n",
      "Epoch 270 | Loss: 2.3691 | Recon: 2.3263 | \n",
      "Epoch 271 | Loss: 2.3720 | Recon: 2.3291 | \n",
      "Epoch 272 | Loss: 2.3457 | Recon: 2.3029 | \n",
      "Epoch 273 | Loss: 2.3702 | Recon: 2.3278 | \n",
      "Epoch 274 | Loss: 2.3808 | Recon: 2.3384 | \n",
      "Epoch 275 | Loss: 2.3887 | Recon: 2.3467 | \n",
      "Epoch 276 | Loss: 2.4146 | Recon: 2.3725 | \n",
      "Epoch 277 | Loss: 2.4158 | Recon: 2.3736 | \n",
      "Epoch 278 | Loss: 2.3738 | Recon: 2.3312 | \n",
      "Epoch 279 | Loss: 2.3949 | Recon: 2.3530 | \n",
      "Epoch 280 | Loss: 2.3730 | Recon: 2.3308 | \n",
      "Epoch 281 | Loss: 2.3113 | Recon: 2.2693 | \n",
      "Epoch 282 | Loss: 2.3687 | Recon: 2.3267 | \n",
      "Epoch 283 | Loss: 2.4003 | Recon: 2.3584 | \n",
      "Epoch 284 | Loss: 2.3864 | Recon: 2.3450 | \n",
      "Epoch 285 | Loss: 2.3954 | Recon: 2.3541 | \n",
      "Epoch 286 | Loss: 2.3518 | Recon: 2.3102 | \n",
      "Epoch 287 | Loss: 2.3723 | Recon: 2.3311 | \n",
      "Epoch 288 | Loss: 2.2870 | Recon: 2.2452 | \n",
      "Epoch 289 | Loss: 2.3743 | Recon: 2.3330 | \n",
      "Epoch 290 | Loss: 2.3290 | Recon: 2.2869 | \n",
      "Epoch 291 | Loss: 2.2702 | Recon: 2.2282 | \n",
      "Epoch 292 | Loss: 2.3366 | Recon: 2.2950 | \n",
      "Epoch 293 | Loss: 2.3395 | Recon: 2.2977 | \n",
      "Epoch 294 | Loss: 2.3225 | Recon: 2.2811 | \n",
      "Epoch 295 | Loss: 2.3269 | Recon: 2.2858 | \n",
      "Epoch 296 | Loss: 2.3371 | Recon: 2.2960 | \n",
      "Epoch 297 | Loss: 2.2784 | Recon: 2.2368 | \n",
      "Epoch 298 | Loss: 2.2927 | Recon: 2.2513 | \n",
      "Epoch 299 | Loss: 2.3302 | Recon: 2.2892 | \n",
      "Epoch 300 | Loss: 2.3257 | Recon: 2.2847 | \n",
      "Epoch 301 | Loss: 2.2558 | Recon: 2.2144 | \n",
      "Epoch 302 | Loss: 2.3217 | Recon: 2.2808 | \n",
      "Epoch 303 | Loss: 2.3161 | Recon: 2.2750 | \n",
      "Epoch 304 | Loss: 2.3182 | Recon: 2.2776 | \n",
      "Epoch 305 | Loss: 2.2593 | Recon: 2.2185 | \n",
      "Epoch 306 | Loss: 2.3204 | Recon: 2.2798 | \n",
      "Epoch 307 | Loss: 2.3232 | Recon: 2.2822 | \n",
      "Epoch 308 | Loss: 2.3783 | Recon: 2.3384 | \n",
      "Epoch 309 | Loss: 2.2697 | Recon: 2.2291 | \n",
      "Epoch 310 | Loss: 2.2787 | Recon: 2.2380 | \n",
      "Epoch 311 | Loss: 2.2851 | Recon: 2.2445 | \n",
      "Epoch 312 | Loss: 2.3128 | Recon: 2.2721 | \n",
      "Epoch 313 | Loss: 2.3259 | Recon: 2.2854 | \n",
      "Epoch 314 | Loss: 2.2878 | Recon: 2.2474 | \n",
      "Epoch 315 | Loss: 2.2401 | Recon: 2.1995 | \n",
      "Epoch 316 | Loss: 2.2903 | Recon: 2.2499 | \n",
      "Epoch 317 | Loss: 2.2481 | Recon: 2.2074 | \n",
      "Epoch 318 | Loss: 2.2903 | Recon: 2.2500 | \n",
      "Epoch 319 | Loss: 2.2860 | Recon: 2.2454 | \n",
      "Epoch 320 | Loss: 2.2632 | Recon: 2.2229 | \n",
      "Epoch 321 | Loss: 2.2432 | Recon: 2.2029 | \n",
      "Epoch 322 | Loss: 2.2501 | Recon: 2.2103 | \n",
      "Epoch 323 | Loss: 2.2597 | Recon: 2.2197 | \n",
      "Epoch 324 | Loss: 2.2467 | Recon: 2.2064 | \n",
      "Epoch 325 | Loss: 2.2268 | Recon: 2.1865 | \n",
      "Epoch 326 | Loss: 2.2267 | Recon: 2.1866 | \n",
      "Epoch 327 | Loss: 2.2374 | Recon: 2.1977 | \n",
      "Epoch 328 | Loss: 2.2419 | Recon: 2.2022 | \n",
      "Epoch 329 | Loss: 2.2269 | Recon: 2.1871 | \n",
      "Epoch 330 | Loss: 2.2724 | Recon: 2.2331 | \n",
      "Epoch 331 | Loss: 2.2285 | Recon: 2.1887 | \n",
      "Epoch 332 | Loss: 2.2226 | Recon: 2.1828 | \n",
      "Epoch 333 | Loss: 2.2209 | Recon: 2.1815 | \n",
      "Epoch 334 | Loss: 2.2014 | Recon: 2.1612 | \n",
      "Epoch 335 | Loss: 2.2016 | Recon: 2.1619 | \n",
      "Epoch 336 | Loss: 2.1986 | Recon: 2.1589 | \n",
      "Epoch 337 | Loss: 2.2348 | Recon: 2.1955 | \n",
      "Epoch 338 | Loss: 2.2458 | Recon: 2.2066 | \n",
      "Epoch 339 | Loss: 2.1701 | Recon: 2.1306 | \n",
      "Epoch 340 | Loss: 2.2061 | Recon: 2.1667 | \n",
      "Epoch 341 | Loss: 2.2517 | Recon: 2.2126 | \n",
      "Epoch 342 | Loss: 2.2210 | Recon: 2.1819 | \n",
      "Epoch 343 | Loss: 2.2177 | Recon: 2.1788 | \n",
      "Epoch 344 | Loss: 2.2046 | Recon: 2.1653 | \n",
      "Epoch 345 | Loss: 2.2527 | Recon: 2.2140 | \n",
      "Epoch 346 | Loss: 2.2041 | Recon: 2.1653 | \n",
      "Epoch 347 | Loss: 2.2059 | Recon: 2.1672 | \n",
      "Epoch 348 | Loss: 2.1895 | Recon: 2.1508 | \n",
      "Epoch 349 | Loss: 2.1848 | Recon: 2.1461 | \n",
      "Epoch 350 | Loss: 2.1746 | Recon: 2.1358 | \n",
      "Epoch 351 | Loss: 2.1590 | Recon: 2.1203 | \n",
      "Epoch 352 | Loss: 2.2449 | Recon: 2.2064 | \n",
      "Epoch 353 | Loss: 2.1759 | Recon: 2.1376 | \n",
      "Epoch 354 | Loss: 2.1587 | Recon: 2.1202 | \n",
      "Epoch 355 | Loss: 2.2179 | Recon: 2.1795 | \n",
      "Epoch 356 | Loss: 2.1537 | Recon: 2.1151 | \n",
      "Epoch 357 | Loss: 2.2429 | Recon: 2.2048 | \n",
      "Epoch 358 | Loss: 2.1948 | Recon: 2.1569 | \n",
      "Epoch 359 | Loss: 2.1772 | Recon: 2.1389 | \n",
      "Epoch 360 | Loss: 2.1803 | Recon: 2.1420 | \n",
      "Epoch 361 | Loss: 2.2104 | Recon: 2.1725 | \n",
      "Epoch 362 | Loss: 2.1909 | Recon: 2.1527 | \n",
      "Epoch 363 | Loss: 2.1431 | Recon: 2.1049 | \n",
      "Epoch 364 | Loss: 2.1868 | Recon: 2.1487 | \n",
      "Epoch 365 | Loss: 2.1217 | Recon: 2.0837 | \n",
      "Epoch 366 | Loss: 2.1494 | Recon: 2.1116 | \n",
      "Epoch 367 | Loss: 2.1407 | Recon: 2.1030 | \n",
      "Epoch 368 | Loss: 2.1358 | Recon: 2.0981 | \n",
      "Epoch 369 | Loss: 2.0955 | Recon: 2.0574 | \n",
      "Epoch 370 | Loss: 2.1682 | Recon: 2.1306 | \n",
      "Epoch 371 | Loss: 2.1942 | Recon: 2.1567 | \n",
      "Epoch 372 | Loss: 2.1418 | Recon: 2.1045 | \n",
      "Epoch 373 | Loss: 2.1514 | Recon: 2.1140 | \n",
      "Epoch 374 | Loss: 2.1358 | Recon: 2.0982 | \n",
      "Epoch 375 | Loss: 2.1007 | Recon: 2.0629 | \n",
      "Epoch 376 | Loss: 2.1344 | Recon: 2.0969 | \n",
      "Epoch 377 | Loss: 2.1313 | Recon: 2.0936 | \n",
      "Epoch 378 | Loss: 2.1261 | Recon: 2.0888 | \n",
      "Epoch 379 | Loss: 2.0777 | Recon: 2.0404 | \n",
      "Epoch 380 | Loss: 2.1133 | Recon: 2.0760 | \n",
      "Epoch 381 | Loss: 2.1014 | Recon: 2.0640 | \n",
      "Epoch 382 | Loss: 2.0934 | Recon: 2.0563 | \n",
      "Epoch 383 | Loss: 2.1163 | Recon: 2.0796 | \n",
      "Epoch 384 | Loss: 2.0625 | Recon: 2.0254 | \n",
      "Epoch 385 | Loss: 2.1335 | Recon: 2.0964 | \n",
      "Epoch 386 | Loss: 2.0474 | Recon: 2.0103 | \n",
      "Epoch 387 | Loss: 2.0803 | Recon: 2.0432 | \n",
      "Epoch 388 | Loss: 2.0768 | Recon: 2.0400 | \n",
      "Epoch 389 | Loss: 2.1077 | Recon: 2.0711 | \n",
      "Epoch 390 | Loss: 2.0403 | Recon: 2.0036 | \n",
      "Epoch 391 | Loss: 2.0612 | Recon: 2.0246 | \n",
      "Epoch 392 | Loss: 2.1521 | Recon: 2.1157 | \n",
      "Epoch 393 | Loss: 2.0808 | Recon: 2.0443 | \n",
      "Epoch 394 | Loss: 2.1023 | Recon: 2.0656 | \n",
      "Epoch 395 | Loss: 2.0659 | Recon: 2.0294 | \n",
      "Epoch 396 | Loss: 2.0574 | Recon: 2.0206 | \n",
      "Epoch 397 | Loss: 2.0579 | Recon: 2.0216 | \n",
      "Epoch 398 | Loss: 2.0763 | Recon: 2.0401 | \n",
      "Epoch 399 | Loss: 2.0885 | Recon: 2.0524 | \n",
      "Epoch 400 | Loss: 2.0492 | Recon: 2.0129 | \n",
      "Epoch 401 | Loss: 2.0195 | Recon: 1.9829 | \n",
      "Epoch 402 | Loss: 2.0210 | Recon: 1.9844 | \n",
      "Epoch 403 | Loss: 2.0264 | Recon: 1.9902 | \n",
      "Epoch 404 | Loss: 2.0569 | Recon: 2.0212 | \n",
      "Epoch 405 | Loss: 1.9860 | Recon: 1.9499 | \n",
      "Epoch 406 | Loss: 2.0335 | Recon: 1.9973 | \n",
      "Epoch 407 | Loss: 1.9705 | Recon: 1.9341 | \n",
      "Epoch 408 | Loss: 2.0067 | Recon: 1.9710 | \n",
      "Epoch 409 | Loss: 2.0323 | Recon: 1.9965 | \n",
      "Epoch 410 | Loss: 1.9963 | Recon: 1.9606 | \n",
      "Epoch 411 | Loss: 2.0228 | Recon: 1.9868 | \n",
      "Epoch 412 | Loss: 2.0034 | Recon: 1.9675 | \n",
      "Epoch 413 | Loss: 2.0384 | Recon: 2.0025 | \n",
      "Epoch 414 | Loss: 1.9986 | Recon: 1.9630 | \n",
      "Epoch 415 | Loss: 1.9995 | Recon: 1.9640 | \n",
      "Epoch 416 | Loss: 1.9648 | Recon: 1.9290 | \n",
      "Epoch 417 | Loss: 1.9884 | Recon: 1.9529 | \n",
      "Epoch 418 | Loss: 1.9983 | Recon: 1.9631 | \n",
      "Epoch 419 | Loss: 1.9290 | Recon: 1.8932 | \n",
      "Epoch 420 | Loss: 1.9714 | Recon: 1.9359 | \n",
      "Epoch 421 | Loss: 1.9533 | Recon: 1.9179 | \n",
      "Epoch 422 | Loss: 1.9210 | Recon: 1.8858 | \n",
      "Epoch 423 | Loss: 1.9986 | Recon: 1.9635 | \n",
      "Epoch 424 | Loss: 1.9726 | Recon: 1.9376 | \n",
      "Epoch 425 | Loss: 1.9026 | Recon: 1.8671 | \n",
      "Epoch 426 | Loss: 1.9017 | Recon: 1.8663 | \n",
      "Epoch 427 | Loss: 1.8983 | Recon: 1.8633 | \n",
      "Epoch 428 | Loss: 1.9348 | Recon: 1.8999 | \n",
      "Epoch 429 | Loss: 1.9024 | Recon: 1.8672 | \n",
      "Epoch 430 | Loss: 1.9330 | Recon: 1.8981 | \n",
      "Epoch 431 | Loss: 1.9386 | Recon: 1.9038 | \n",
      "Epoch 432 | Loss: 1.9444 | Recon: 1.9097 | \n",
      "Epoch 433 | Loss: 1.9156 | Recon: 1.8808 | \n",
      "Epoch 434 | Loss: 1.9366 | Recon: 1.9019 | \n",
      "Epoch 435 | Loss: 1.8781 | Recon: 1.8433 | \n",
      "Epoch 436 | Loss: 1.8914 | Recon: 1.8566 | \n",
      "Epoch 437 | Loss: 1.8965 | Recon: 1.8620 | \n",
      "Epoch 438 | Loss: 1.8857 | Recon: 1.8509 | \n",
      "Epoch 439 | Loss: 1.8772 | Recon: 1.8425 | \n",
      "Epoch 440 | Loss: 1.8857 | Recon: 1.8511 | \n",
      "Epoch 441 | Loss: 1.9197 | Recon: 1.8856 | \n",
      "Epoch 442 | Loss: 1.8984 | Recon: 1.8638 | \n",
      "Epoch 443 | Loss: 1.8490 | Recon: 1.8146 | \n",
      "Epoch 444 | Loss: 1.8463 | Recon: 1.8121 | \n",
      "Epoch 445 | Loss: 1.8397 | Recon: 1.8054 | \n",
      "Epoch 446 | Loss: 1.8420 | Recon: 1.8079 | \n",
      "Epoch 447 | Loss: 1.8079 | Recon: 1.7733 | \n",
      "Epoch 448 | Loss: 1.8562 | Recon: 1.8222 | \n",
      "Epoch 449 | Loss: 1.8207 | Recon: 1.7867 | \n",
      "Epoch 450 | Loss: 1.8395 | Recon: 1.8057 | \n",
      "Epoch 451 | Loss: 1.8613 | Recon: 1.8274 | \n",
      "Epoch 452 | Loss: 1.8928 | Recon: 1.8595 | \n",
      "Epoch 453 | Loss: 1.8473 | Recon: 1.8136 | \n",
      "Epoch 454 | Loss: 1.8403 | Recon: 1.8065 | \n",
      "Epoch 455 | Loss: 1.8141 | Recon: 1.7802 | \n",
      "Epoch 456 | Loss: 1.8329 | Recon: 1.7992 | \n",
      "Epoch 457 | Loss: 1.8072 | Recon: 1.7733 | \n",
      "Epoch 458 | Loss: 1.7844 | Recon: 1.7507 | \n",
      "Epoch 459 | Loss: 1.7983 | Recon: 1.7646 | \n",
      "Epoch 460 | Loss: 1.7676 | Recon: 1.7335 | \n",
      "Epoch 461 | Loss: 1.7609 | Recon: 1.7273 | \n",
      "Epoch 462 | Loss: 1.7667 | Recon: 1.7332 | \n",
      "Epoch 463 | Loss: 1.7450 | Recon: 1.7116 | \n",
      "Epoch 464 | Loss: 1.7674 | Recon: 1.7336 | \n",
      "Epoch 465 | Loss: 1.7541 | Recon: 1.7204 | \n",
      "Epoch 466 | Loss: 1.7703 | Recon: 1.7372 | \n",
      "Epoch 467 | Loss: 1.7499 | Recon: 1.7166 | \n",
      "Epoch 468 | Loss: 1.7441 | Recon: 1.7109 | \n",
      "Epoch 469 | Loss: 1.7015 | Recon: 1.6682 | \n",
      "Epoch 470 | Loss: 1.7053 | Recon: 1.6720 | \n",
      "Epoch 471 | Loss: 1.7267 | Recon: 1.6932 | \n",
      "Epoch 472 | Loss: 1.7238 | Recon: 1.6907 | \n",
      "Epoch 473 | Loss: 1.6756 | Recon: 1.6422 | \n",
      "Epoch 474 | Loss: 1.7272 | Recon: 1.6940 | \n",
      "Epoch 475 | Loss: 1.7138 | Recon: 1.6809 | \n",
      "Epoch 476 | Loss: 1.6806 | Recon: 1.6474 | \n",
      "Epoch 477 | Loss: 1.6641 | Recon: 1.6310 | \n",
      "Epoch 478 | Loss: 1.6297 | Recon: 1.5968 | \n",
      "Epoch 479 | Loss: 1.6711 | Recon: 1.6384 | \n",
      "Epoch 480 | Loss: 1.6827 | Recon: 1.6497 | \n",
      "Epoch 481 | Loss: 1.6971 | Recon: 1.6645 | \n",
      "Epoch 482 | Loss: 1.6695 | Recon: 1.6369 | \n",
      "Epoch 483 | Loss: 1.6578 | Recon: 1.6252 | \n",
      "Epoch 484 | Loss: 1.6662 | Recon: 1.6336 | \n",
      "Epoch 485 | Loss: 1.6405 | Recon: 1.6076 | \n",
      "Epoch 486 | Loss: 1.6680 | Recon: 1.6354 | \n",
      "Epoch 487 | Loss: 1.6359 | Recon: 1.6033 | \n",
      "Epoch 488 | Loss: 1.6270 | Recon: 1.5943 | \n",
      "Epoch 489 | Loss: 1.6720 | Recon: 1.6394 | \n",
      "Epoch 490 | Loss: 1.6024 | Recon: 1.5698 | \n",
      "Epoch 491 | Loss: 1.5779 | Recon: 1.5452 | \n",
      "Epoch 492 | Loss: 1.6038 | Recon: 1.5717 | \n",
      "Epoch 493 | Loss: 1.6103 | Recon: 1.5780 | \n",
      "Epoch 494 | Loss: 1.6102 | Recon: 1.5779 | \n",
      "Epoch 495 | Loss: 1.6153 | Recon: 1.5831 | \n",
      "Epoch 496 | Loss: 1.5740 | Recon: 1.5413 | \n",
      "Epoch 497 | Loss: 1.5802 | Recon: 1.5481 | \n",
      "Epoch 498 | Loss: 1.5540 | Recon: 1.5219 | \n",
      "Epoch 499 | Loss: 1.5325 | Recon: 1.5003 | \n",
      "Epoch 500 | Loss: 1.5150 | Recon: 1.4824 | \n",
      "Epoch 501 | Loss: 1.5541 | Recon: 1.5218 | \n",
      "Epoch 502 | Loss: 1.5442 | Recon: 1.5122 | \n",
      "Epoch 503 | Loss: 1.5037 | Recon: 1.4713 | \n",
      "Epoch 504 | Loss: 1.4833 | Recon: 1.4508 | \n",
      "Epoch 505 | Loss: 1.4812 | Recon: 1.4489 | \n",
      "Epoch 506 | Loss: 1.4866 | Recon: 1.4543 | \n",
      "Epoch 507 | Loss: 1.4835 | Recon: 1.4514 | \n",
      "Epoch 508 | Loss: 1.4973 | Recon: 1.4654 | \n",
      "Epoch 509 | Loss: 1.5097 | Recon: 1.4780 | \n",
      "Epoch 510 | Loss: 1.5254 | Recon: 1.4936 | \n",
      "Epoch 511 | Loss: 1.4679 | Recon: 1.4360 | \n",
      "Epoch 512 | Loss: 1.4422 | Recon: 1.4104 | \n",
      "Epoch 513 | Loss: 1.4289 | Recon: 1.3967 | \n",
      "Epoch 514 | Loss: 1.4365 | Recon: 1.4046 | \n",
      "Epoch 515 | Loss: 1.4487 | Recon: 1.4170 | \n",
      "Epoch 516 | Loss: 1.4346 | Recon: 1.4030 | \n",
      "Epoch 517 | Loss: 1.4048 | Recon: 1.3730 | \n",
      "Epoch 518 | Loss: 1.4044 | Recon: 1.3725 | \n",
      "Epoch 519 | Loss: 1.4131 | Recon: 1.3817 | \n",
      "Epoch 520 | Loss: 1.3643 | Recon: 1.3324 | \n",
      "Epoch 521 | Loss: 1.4009 | Recon: 1.3694 | \n",
      "Epoch 522 | Loss: 1.3495 | Recon: 1.3174 | \n",
      "Epoch 523 | Loss: 1.3767 | Recon: 1.3449 | \n",
      "Epoch 524 | Loss: 1.3735 | Recon: 1.3418 | \n",
      "Epoch 525 | Loss: 1.3562 | Recon: 1.3249 | \n",
      "Epoch 526 | Loss: 1.3979 | Recon: 1.3666 | \n",
      "Epoch 527 | Loss: 1.3469 | Recon: 1.3154 | \n",
      "Epoch 528 | Loss: 1.3227 | Recon: 1.2911 | \n",
      "Epoch 529 | Loss: 1.3237 | Recon: 1.2922 | \n",
      "Epoch 530 | Loss: 1.3038 | Recon: 1.2724 | \n",
      "Epoch 531 | Loss: 1.3073 | Recon: 1.2757 | \n",
      "Epoch 532 | Loss: 1.3034 | Recon: 1.2723 | \n",
      "Epoch 533 | Loss: 1.2756 | Recon: 1.2440 | \n",
      "Epoch 534 | Loss: 1.2740 | Recon: 1.2426 | \n",
      "Epoch 535 | Loss: 1.2961 | Recon: 1.2649 | \n",
      "Epoch 536 | Loss: 1.2826 | Recon: 1.2512 | \n",
      "Epoch 537 | Loss: 1.2601 | Recon: 1.2288 | \n",
      "Epoch 538 | Loss: 1.2805 | Recon: 1.2492 | \n",
      "Epoch 539 | Loss: 1.2424 | Recon: 1.2112 | \n",
      "Epoch 540 | Loss: 1.2595 | Recon: 1.2282 | \n",
      "Epoch 541 | Loss: 1.2316 | Recon: 1.2003 | \n",
      "Epoch 542 | Loss: 1.2280 | Recon: 1.1966 | \n",
      "Epoch 543 | Loss: 1.1878 | Recon: 1.1568 | \n",
      "Epoch 544 | Loss: 1.2148 | Recon: 1.1835 | \n",
      "Epoch 545 | Loss: 1.1907 | Recon: 1.1596 | \n",
      "Epoch 546 | Loss: 1.1916 | Recon: 1.1605 | \n",
      "Epoch 547 | Loss: 1.2036 | Recon: 1.1728 | \n",
      "Epoch 548 | Loss: 1.1455 | Recon: 1.1144 | \n",
      "Epoch 549 | Loss: 1.1546 | Recon: 1.1237 | \n",
      "Epoch 550 | Loss: 1.1337 | Recon: 1.1025 | \n",
      "Epoch 551 | Loss: 1.1036 | Recon: 1.0725 | \n",
      "Epoch 552 | Loss: 1.1063 | Recon: 1.0749 | \n",
      "Epoch 553 | Loss: 1.1239 | Recon: 1.0928 | \n",
      "Epoch 554 | Loss: 1.1260 | Recon: 1.0951 | \n",
      "Epoch 555 | Loss: 1.1215 | Recon: 1.0906 | \n",
      "Epoch 556 | Loss: 1.0932 | Recon: 1.0623 | \n",
      "Epoch 557 | Loss: 1.0741 | Recon: 1.0430 | \n",
      "Epoch 558 | Loss: 1.0691 | Recon: 1.0379 | \n",
      "Epoch 559 | Loss: 1.0590 | Recon: 1.0281 | \n",
      "Epoch 560 | Loss: 1.0431 | Recon: 1.0123 | \n",
      "Epoch 561 | Loss: 1.0472 | Recon: 1.0164 | \n",
      "Epoch 562 | Loss: 1.0323 | Recon: 1.0014 | \n",
      "Epoch 563 | Loss: 1.0256 | Recon: 0.9944 | \n",
      "Epoch 564 | Loss: 1.0392 | Recon: 1.0086 | \n",
      "Epoch 565 | Loss: 0.9891 | Recon: 0.9581 | \n",
      "Epoch 566 | Loss: 0.9970 | Recon: 0.9662 | \n",
      "Epoch 567 | Loss: 0.9829 | Recon: 0.9521 | \n",
      "Epoch 568 | Loss: 0.9925 | Recon: 0.9616 | \n",
      "Epoch 569 | Loss: 0.9741 | Recon: 0.9433 | \n",
      "Epoch 570 | Loss: 0.9950 | Recon: 0.9641 | \n",
      "Epoch 571 | Loss: 0.9415 | Recon: 0.9105 | \n",
      "Epoch 572 | Loss: 0.9442 | Recon: 0.9136 | \n",
      "Epoch 573 | Loss: 0.9695 | Recon: 0.9390 | \n",
      "Epoch 574 | Loss: 0.9461 | Recon: 0.9154 | \n",
      "Epoch 575 | Loss: 0.9109 | Recon: 0.8797 | \n",
      "Epoch 576 | Loss: 0.9367 | Recon: 0.9058 | \n",
      "Epoch 577 | Loss: 0.9132 | Recon: 0.8828 | \n",
      "Epoch 578 | Loss: 0.9042 | Recon: 0.8732 | \n",
      "Epoch 579 | Loss: 0.8901 | Recon: 0.8595 | \n",
      "Epoch 580 | Loss: 0.8935 | Recon: 0.8624 | \n",
      "Epoch 581 | Loss: 0.8730 | Recon: 0.8422 | \n",
      "Epoch 582 | Loss: 0.8663 | Recon: 0.8358 | \n",
      "Epoch 583 | Loss: 0.8886 | Recon: 0.8577 | \n",
      "Epoch 584 | Loss: 0.8516 | Recon: 0.8211 | \n",
      "Epoch 585 | Loss: 0.8650 | Recon: 0.8347 | \n",
      "Epoch 586 | Loss: 0.8590 | Recon: 0.8286 | \n",
      "Epoch 587 | Loss: 0.8353 | Recon: 0.8046 | \n",
      "Epoch 588 | Loss: 0.8303 | Recon: 0.7995 | \n",
      "Epoch 589 | Loss: 0.8231 | Recon: 0.7927 | \n",
      "Epoch 590 | Loss: 0.8298 | Recon: 0.7993 | \n",
      "Epoch 591 | Loss: 0.8293 | Recon: 0.7989 | \n",
      "Epoch 592 | Loss: 0.8088 | Recon: 0.7782 | \n",
      "Epoch 593 | Loss: 0.8057 | Recon: 0.7749 | \n",
      "Epoch 594 | Loss: 0.7903 | Recon: 0.7599 | \n",
      "Epoch 595 | Loss: 0.7764 | Recon: 0.7458 | \n",
      "Epoch 596 | Loss: 0.8003 | Recon: 0.7700 | \n",
      "Epoch 597 | Loss: 0.7809 | Recon: 0.7508 | \n",
      "Epoch 598 | Loss: 0.7678 | Recon: 0.7371 | \n",
      "Epoch 599 | Loss: 0.7613 | Recon: 0.7304 | \n",
      "Epoch 600 | Loss: 0.7406 | Recon: 0.7101 | \n",
      "Epoch 601 | Loss: 0.7304 | Recon: 0.6999 | \n",
      "Epoch 602 | Loss: 0.7495 | Recon: 0.7191 | \n",
      "Epoch 603 | Loss: 0.7415 | Recon: 0.7112 | \n",
      "Epoch 604 | Loss: 0.7270 | Recon: 0.6962 | \n",
      "Epoch 605 | Loss: 0.7183 | Recon: 0.6877 | \n",
      "Epoch 606 | Loss: 0.7211 | Recon: 0.6909 | \n",
      "Epoch 607 | Loss: 0.7007 | Recon: 0.6701 | \n",
      "Epoch 608 | Loss: 0.7112 | Recon: 0.6810 | \n",
      "Epoch 609 | Loss: 0.6922 | Recon: 0.6618 | \n",
      "Epoch 610 | Loss: 0.6985 | Recon: 0.6679 | \n",
      "Epoch 611 | Loss: 0.7032 | Recon: 0.6727 | \n",
      "Epoch 612 | Loss: 0.7003 | Recon: 0.6698 | \n",
      "Epoch 613 | Loss: 0.7000 | Recon: 0.6698 | \n",
      "Epoch 614 | Loss: 0.6620 | Recon: 0.6312 | \n",
      "Epoch 615 | Loss: 0.6758 | Recon: 0.6456 | \n",
      "Epoch 616 | Loss: 0.6626 | Recon: 0.6321 | \n",
      "Epoch 617 | Loss: 0.6648 | Recon: 0.6346 | \n",
      "Epoch 618 | Loss: 0.6420 | Recon: 0.6117 | \n",
      "Epoch 619 | Loss: 0.6507 | Recon: 0.6203 | \n",
      "Epoch 620 | Loss: 0.6648 | Recon: 0.6345 | \n",
      "Epoch 621 | Loss: 0.6540 | Recon: 0.6238 | \n",
      "Epoch 622 | Loss: 0.6317 | Recon: 0.6012 | \n",
      "Epoch 623 | Loss: 0.6331 | Recon: 0.6029 | \n",
      "Epoch 624 | Loss: 0.6221 | Recon: 0.5915 | \n",
      "Epoch 625 | Loss: 0.6273 | Recon: 0.5969 | \n",
      "Epoch 626 | Loss: 0.6137 | Recon: 0.5835 | \n",
      "Epoch 627 | Loss: 0.6145 | Recon: 0.5841 | \n",
      "Epoch 628 | Loss: 0.6022 | Recon: 0.5718 | \n",
      "Epoch 629 | Loss: 0.6084 | Recon: 0.5782 | \n",
      "Epoch 630 | Loss: 0.6025 | Recon: 0.5722 | \n",
      "Epoch 631 | Loss: 0.5926 | Recon: 0.5625 | \n",
      "Epoch 632 | Loss: 0.5925 | Recon: 0.5620 | \n",
      "Epoch 633 | Loss: 0.6001 | Recon: 0.5698 | \n",
      "Epoch 634 | Loss: 0.5870 | Recon: 0.5566 | \n",
      "Epoch 635 | Loss: 0.6047 | Recon: 0.5746 | \n",
      "Epoch 636 | Loss: 0.5823 | Recon: 0.5520 | \n",
      "Epoch 637 | Loss: 0.5890 | Recon: 0.5590 | \n",
      "Epoch 638 | Loss: 0.5950 | Recon: 0.5647 | \n",
      "Epoch 639 | Loss: 0.5751 | Recon: 0.5448 | \n",
      "Epoch 640 | Loss: 0.5729 | Recon: 0.5428 | \n",
      "Epoch 641 | Loss: 0.5810 | Recon: 0.5510 | \n",
      "Epoch 642 | Loss: 0.5758 | Recon: 0.5459 | \n",
      "Epoch 643 | Loss: 0.5585 | Recon: 0.5282 | \n",
      "Epoch 644 | Loss: 0.5595 | Recon: 0.5296 | \n",
      "Epoch 645 | Loss: 0.5582 | Recon: 0.5282 | \n",
      "Epoch 646 | Loss: 0.5699 | Recon: 0.5398 | \n",
      "Epoch 647 | Loss: 0.5499 | Recon: 0.5199 | \n",
      "Epoch 648 | Loss: 0.5483 | Recon: 0.5182 | \n",
      "Epoch 649 | Loss: 0.5511 | Recon: 0.5215 | \n",
      "Epoch 650 | Loss: 0.5429 | Recon: 0.5128 | \n",
      "Epoch 651 | Loss: 0.5439 | Recon: 0.5140 | \n",
      "Epoch 652 | Loss: 0.5534 | Recon: 0.5235 | \n",
      "Epoch 653 | Loss: 0.5567 | Recon: 0.5269 | \n",
      "Epoch 654 | Loss: 0.5390 | Recon: 0.5090 | \n",
      "Epoch 655 | Loss: 0.5379 | Recon: 0.5084 | \n",
      "Epoch 656 | Loss: 0.5450 | Recon: 0.5154 | \n",
      "Epoch 657 | Loss: 0.5330 | Recon: 0.5031 | \n",
      "Epoch 658 | Loss: 0.5239 | Recon: 0.4941 | \n",
      "Epoch 659 | Loss: 0.5312 | Recon: 0.5015 | \n",
      "Epoch 660 | Loss: 0.5335 | Recon: 0.5039 | \n",
      "Epoch 661 | Loss: 0.5535 | Recon: 0.5239 | \n",
      "Epoch 662 | Loss: 0.5217 | Recon: 0.4923 | \n",
      "Epoch 663 | Loss: 0.5274 | Recon: 0.4977 | \n",
      "Epoch 664 | Loss: 0.5274 | Recon: 0.4974 | \n",
      "Epoch 665 | Loss: 0.5178 | Recon: 0.4884 | \n",
      "Epoch 666 | Loss: 0.5200 | Recon: 0.4906 | \n",
      "Epoch 667 | Loss: 0.5249 | Recon: 0.4954 | \n",
      "Epoch 668 | Loss: 0.5174 | Recon: 0.4876 | \n",
      "Epoch 669 | Loss: 0.5168 | Recon: 0.4870 | \n",
      "Epoch 670 | Loss: 0.5124 | Recon: 0.4829 | \n",
      "Epoch 671 | Loss: 0.5059 | Recon: 0.4767 | \n",
      "Epoch 672 | Loss: 0.5181 | Recon: 0.4889 | \n",
      "Epoch 673 | Loss: 0.5214 | Recon: 0.4922 | \n",
      "Epoch 674 | Loss: 0.5218 | Recon: 0.4925 | \n",
      "Epoch 675 | Loss: 0.5204 | Recon: 0.4912 | \n",
      "Epoch 676 | Loss: 0.5083 | Recon: 0.4788 | \n",
      "Epoch 677 | Loss: 0.5044 | Recon: 0.4748 | \n",
      "Epoch 678 | Loss: 0.5109 | Recon: 0.4813 | \n",
      "Epoch 679 | Loss: 0.5164 | Recon: 0.4869 | \n",
      "Epoch 680 | Loss: 0.5145 | Recon: 0.4850 | \n",
      "Epoch 681 | Loss: 0.5039 | Recon: 0.4749 | \n",
      "Epoch 682 | Loss: 0.5081 | Recon: 0.4788 | \n",
      "Epoch 683 | Loss: 0.4961 | Recon: 0.4666 | \n",
      "Epoch 684 | Loss: 0.4952 | Recon: 0.4664 | \n",
      "Epoch 685 | Loss: 0.4985 | Recon: 0.4691 | \n",
      "Epoch 686 | Loss: 0.5077 | Recon: 0.4786 | \n",
      "Epoch 687 | Loss: 0.5018 | Recon: 0.4728 | \n",
      "Epoch 688 | Loss: 0.5018 | Recon: 0.4731 | \n",
      "Epoch 689 | Loss: 0.4912 | Recon: 0.4619 | \n",
      "Epoch 690 | Loss: 0.4997 | Recon: 0.4704 | \n",
      "Epoch 691 | Loss: 0.5066 | Recon: 0.4777 | \n",
      "Epoch 692 | Loss: 0.4909 | Recon: 0.4619 | \n",
      "Epoch 693 | Loss: 0.4988 | Recon: 0.4698 | \n",
      "Epoch 694 | Loss: 0.5024 | Recon: 0.4736 | \n",
      "Epoch 695 | Loss: 0.4907 | Recon: 0.4618 | \n",
      "Epoch 696 | Loss: 0.4888 | Recon: 0.4598 | \n",
      "Epoch 697 | Loss: 0.4985 | Recon: 0.4695 | \n",
      "Epoch 698 | Loss: 0.5015 | Recon: 0.4728 | \n",
      "Epoch 699 | Loss: 0.4993 | Recon: 0.4704 | \n",
      "Epoch 700 | Loss: 0.4925 | Recon: 0.4636 | \n",
      "Epoch 701 | Loss: 0.4919 | Recon: 0.4634 | \n",
      "Epoch 702 | Loss: 0.4912 | Recon: 0.4625 | \n",
      "Epoch 703 | Loss: 0.4970 | Recon: 0.4685 | \n",
      "Epoch 704 | Loss: 0.4900 | Recon: 0.4614 | \n",
      "Epoch 705 | Loss: 0.4972 | Recon: 0.4686 | \n",
      "Epoch 706 | Loss: 0.4898 | Recon: 0.4612 | \n",
      "Epoch 707 | Loss: 0.4851 | Recon: 0.4567 | \n",
      "Epoch 708 | Loss: 0.5058 | Recon: 0.4776 | \n",
      "Epoch 709 | Loss: 0.4767 | Recon: 0.4482 | \n",
      "Epoch 710 | Loss: 0.5029 | Recon: 0.4746 | \n",
      "Epoch 711 | Loss: 0.4904 | Recon: 0.4618 | \n",
      "Epoch 712 | Loss: 0.4992 | Recon: 0.4707 | \n",
      "Epoch 713 | Loss: 0.4900 | Recon: 0.4618 | \n",
      "Epoch 714 | Loss: 0.4852 | Recon: 0.4569 | \n",
      "Epoch 715 | Loss: 0.4819 | Recon: 0.4541 | \n",
      "Epoch 716 | Loss: 0.4863 | Recon: 0.4581 | \n",
      "Epoch 717 | Loss: 0.4915 | Recon: 0.4630 | \n",
      "Epoch 718 | Loss: 0.4863 | Recon: 0.4583 | \n",
      "Epoch 719 | Loss: 0.4832 | Recon: 0.4551 | \n",
      "Epoch 720 | Loss: 0.4783 | Recon: 0.4504 | \n",
      "Epoch 721 | Loss: 0.4824 | Recon: 0.4546 | \n",
      "Epoch 722 | Loss: 0.4862 | Recon: 0.4582 | \n",
      "Epoch 723 | Loss: 0.4973 | Recon: 0.4690 | \n",
      "Epoch 724 | Loss: 0.4761 | Recon: 0.4481 | \n",
      "Epoch 725 | Loss: 0.4832 | Recon: 0.4554 | \n",
      "Epoch 726 | Loss: 0.4888 | Recon: 0.4607 | \n",
      "Epoch 727 | Loss: 0.4820 | Recon: 0.4538 | \n",
      "Epoch 728 | Loss: 0.4925 | Recon: 0.4646 | \n",
      "Epoch 729 | Loss: 0.4818 | Recon: 0.4543 | \n",
      "Epoch 730 | Loss: 0.4794 | Recon: 0.4519 | \n",
      "Epoch 731 | Loss: 0.4796 | Recon: 0.4516 | \n",
      "Epoch 732 | Loss: 0.4731 | Recon: 0.4454 | \n",
      "Epoch 733 | Loss: 0.4755 | Recon: 0.4480 | \n",
      "Epoch 734 | Loss: 0.4861 | Recon: 0.4587 | \n",
      "Epoch 735 | Loss: 0.4731 | Recon: 0.4453 | \n",
      "Epoch 736 | Loss: 0.4683 | Recon: 0.4409 | \n",
      "Epoch 737 | Loss: 0.4782 | Recon: 0.4509 | \n",
      "Epoch 738 | Loss: 0.4794 | Recon: 0.4519 | \n",
      "Epoch 739 | Loss: 0.4883 | Recon: 0.4610 | \n",
      "Epoch 740 | Loss: 0.4803 | Recon: 0.4526 | \n",
      "Epoch 741 | Loss: 0.4878 | Recon: 0.4603 | \n",
      "Epoch 742 | Loss: 0.4683 | Recon: 0.4413 | \n",
      "Epoch 743 | Loss: 0.4839 | Recon: 0.4565 | \n",
      "Epoch 744 | Loss: 0.4785 | Recon: 0.4509 | \n",
      "Epoch 745 | Loss: 0.4759 | Recon: 0.4488 | \n",
      "Epoch 746 | Loss: 0.4712 | Recon: 0.4440 | \n",
      "Epoch 747 | Loss: 0.4770 | Recon: 0.4498 | \n",
      "Epoch 748 | Loss: 0.4773 | Recon: 0.4502 | \n",
      "Epoch 749 | Loss: 0.4858 | Recon: 0.4585 | \n",
      "Epoch 750 | Loss: 0.4772 | Recon: 0.4502 | \n",
      "Epoch 751 | Loss: 0.4750 | Recon: 0.4482 | \n",
      "Epoch 752 | Loss: 0.4661 | Recon: 0.4390 | \n",
      "Epoch 753 | Loss: 0.4702 | Recon: 0.4429 | \n",
      "Epoch 754 | Loss: 0.4547 | Recon: 0.4278 | \n",
      "Epoch 755 | Loss: 0.4744 | Recon: 0.4476 | \n",
      "Epoch 756 | Loss: 0.4710 | Recon: 0.4442 | \n",
      "Epoch 757 | Loss: 0.4857 | Recon: 0.4588 | \n",
      "Epoch 758 | Loss: 0.4668 | Recon: 0.4398 | \n",
      "Epoch 759 | Loss: 0.4856 | Recon: 0.4588 | \n",
      "Epoch 760 | Loss: 0.4713 | Recon: 0.4446 | \n",
      "Epoch 761 | Loss: 0.4789 | Recon: 0.4518 | \n",
      "Epoch 762 | Loss: 0.4886 | Recon: 0.4619 | \n",
      "Epoch 763 | Loss: 0.4782 | Recon: 0.4514 | \n",
      "Epoch 764 | Loss: 0.4701 | Recon: 0.4435 | \n",
      "Epoch 765 | Loss: 0.4690 | Recon: 0.4422 | \n",
      "Epoch 766 | Loss: 0.4675 | Recon: 0.4409 | \n",
      "Epoch 767 | Loss: 0.4689 | Recon: 0.4420 | \n",
      "Epoch 768 | Loss: 0.4659 | Recon: 0.4393 | \n",
      "Epoch 769 | Loss: 0.4738 | Recon: 0.4473 | \n",
      "Epoch 770 | Loss: 0.4670 | Recon: 0.4403 | \n",
      "Epoch 771 | Loss: 0.4620 | Recon: 0.4355 | \n",
      "Epoch 772 | Loss: 0.4742 | Recon: 0.4481 | \n",
      "Epoch 773 | Loss: 0.4747 | Recon: 0.4479 | \n",
      "Epoch 774 | Loss: 0.4655 | Recon: 0.4390 | \n",
      "Epoch 775 | Loss: 0.4674 | Recon: 0.4412 | \n",
      "Epoch 776 | Loss: 0.4675 | Recon: 0.4409 | \n",
      "Epoch 777 | Loss: 0.4680 | Recon: 0.4417 | \n",
      "Epoch 778 | Loss: 0.4626 | Recon: 0.4365 | \n",
      "Epoch 779 | Loss: 0.4766 | Recon: 0.4505 | \n",
      "Epoch 780 | Loss: 0.4667 | Recon: 0.4404 | \n",
      "Epoch 781 | Loss: 0.4689 | Recon: 0.4425 | \n",
      "Epoch 782 | Loss: 0.4815 | Recon: 0.4556 | \n",
      "Epoch 783 | Loss: 0.4697 | Recon: 0.4434 | \n",
      "Epoch 784 | Loss: 0.4759 | Recon: 0.4500 | \n",
      "Epoch 785 | Loss: 0.4614 | Recon: 0.4352 | \n",
      "Epoch 786 | Loss: 0.4731 | Recon: 0.4471 | \n",
      "Epoch 787 | Loss: 0.4628 | Recon: 0.4368 | \n",
      "Epoch 788 | Loss: 0.4615 | Recon: 0.4355 | \n",
      "Epoch 789 | Loss: 0.4576 | Recon: 0.4313 | \n",
      "Epoch 790 | Loss: 0.4643 | Recon: 0.4385 | \n",
      "Epoch 791 | Loss: 0.4623 | Recon: 0.4362 | \n",
      "Epoch 792 | Loss: 0.4705 | Recon: 0.4445 | \n",
      "Epoch 793 | Loss: 0.4644 | Recon: 0.4387 | \n",
      "Epoch 794 | Loss: 0.4618 | Recon: 0.4360 | \n",
      "Epoch 795 | Loss: 0.4715 | Recon: 0.4460 | \n",
      "Epoch 796 | Loss: 0.4760 | Recon: 0.4499 | \n",
      "Epoch 797 | Loss: 0.4602 | Recon: 0.4346 | \n",
      "Epoch 798 | Loss: 0.4717 | Recon: 0.4460 | \n",
      "Epoch 799 | Loss: 0.4787 | Recon: 0.4532 | \n",
      "Epoch 800 | Loss: 0.4689 | Recon: 0.4431 | \n",
      "Epoch 801 | Loss: 0.4817 | Recon: 0.4558 | \n",
      "Epoch 802 | Loss: 0.4693 | Recon: 0.4438 | \n",
      "Epoch 803 | Loss: 0.4863 | Recon: 0.4606 | \n",
      "Epoch 804 | Loss: 0.4719 | Recon: 0.4464 | \n",
      "Epoch 805 | Loss: 0.4725 | Recon: 0.4470 | \n",
      "Epoch 806 | Loss: 0.4644 | Recon: 0.4388 | \n",
      "Epoch 807 | Loss: 0.4648 | Recon: 0.4394 | \n",
      "Epoch 808 | Loss: 0.4762 | Recon: 0.4508 | \n",
      "Epoch 809 | Loss: 0.4745 | Recon: 0.4490 | \n",
      "Epoch 810 | Loss: 0.4805 | Recon: 0.4551 | \n",
      "Epoch 811 | Loss: 0.4660 | Recon: 0.4404 | \n",
      "Epoch 812 | Loss: 0.4741 | Recon: 0.4487 | \n",
      "Epoch 813 | Loss: 0.4626 | Recon: 0.4374 | \n",
      "Epoch 814 | Loss: 0.4691 | Recon: 0.4438 | \n",
      "Epoch 815 | Loss: 0.4581 | Recon: 0.4328 | \n",
      "Epoch 816 | Loss: 0.4645 | Recon: 0.4391 | \n",
      "Epoch 817 | Loss: 0.4646 | Recon: 0.4394 | \n",
      "Epoch 818 | Loss: 0.4625 | Recon: 0.4372 | \n",
      "Epoch 819 | Loss: 0.4711 | Recon: 0.4461 | \n",
      "Epoch 820 | Loss: 0.4720 | Recon: 0.4468 | \n",
      "Epoch 821 | Loss: 0.4703 | Recon: 0.4450 | \n",
      "Epoch 822 | Loss: 0.4701 | Recon: 0.4449 | \n",
      "Epoch 823 | Loss: 0.4494 | Recon: 0.4244 | \n",
      "Epoch 824 | Loss: 0.4601 | Recon: 0.4351 | \n",
      "Epoch 825 | Loss: 0.4651 | Recon: 0.4399 | \n",
      "Epoch 826 | Loss: 0.4643 | Recon: 0.4393 | \n",
      "Epoch 827 | Loss: 0.4736 | Recon: 0.4485 | \n",
      "Epoch 828 | Loss: 0.4613 | Recon: 0.4365 | \n",
      "Epoch 829 | Loss: 0.4639 | Recon: 0.4391 | \n",
      "Epoch 830 | Loss: 0.4592 | Recon: 0.4346 | \n",
      "Epoch 831 | Loss: 0.4511 | Recon: 0.4262 | \n",
      "Epoch 832 | Loss: 0.4594 | Recon: 0.4346 | \n",
      "Epoch 833 | Loss: 0.4750 | Recon: 0.4503 | \n",
      "Epoch 834 | Loss: 0.4659 | Recon: 0.4412 | \n",
      "Epoch 835 | Loss: 0.4541 | Recon: 0.4294 | \n",
      "Epoch 836 | Loss: 0.4528 | Recon: 0.4277 | \n",
      "Epoch 837 | Loss: 0.4640 | Recon: 0.4393 | \n",
      "Epoch 838 | Loss: 0.4603 | Recon: 0.4357 | \n",
      "Epoch 839 | Loss: 0.4635 | Recon: 0.4388 | \n",
      "Epoch 840 | Loss: 0.4574 | Recon: 0.4325 | \n",
      "Epoch 841 | Loss: 0.4552 | Recon: 0.4305 | \n",
      "Epoch 842 | Loss: 0.4637 | Recon: 0.4390 | \n",
      "Epoch 843 | Loss: 0.4679 | Recon: 0.4435 | \n",
      "Epoch 844 | Loss: 0.4687 | Recon: 0.4440 | \n",
      "Epoch 845 | Loss: 0.4760 | Recon: 0.4515 | \n",
      "Epoch 846 | Loss: 0.4542 | Recon: 0.4296 | \n",
      "Epoch 847 | Loss: 0.4621 | Recon: 0.4380 | \n",
      "Epoch 848 | Loss: 0.4668 | Recon: 0.4421 | \n",
      "Epoch 849 | Loss: 0.4606 | Recon: 0.4363 | \n",
      "Epoch 850 | Loss: 0.4674 | Recon: 0.4431 | \n",
      "Epoch 851 | Loss: 0.4625 | Recon: 0.4381 | \n",
      "Epoch 852 | Loss: 0.4588 | Recon: 0.4346 | \n",
      "Epoch 853 | Loss: 0.4637 | Recon: 0.4393 | \n",
      "Epoch 854 | Loss: 0.4694 | Recon: 0.4449 | \n",
      "Epoch 855 | Loss: 0.4564 | Recon: 0.4322 | \n",
      "Epoch 856 | Loss: 0.4737 | Recon: 0.4492 | \n",
      "Epoch 857 | Loss: 0.4570 | Recon: 0.4326 | \n",
      "Epoch 858 | Loss: 0.4495 | Recon: 0.4253 | \n",
      "Epoch 859 | Loss: 0.4687 | Recon: 0.4445 | \n",
      "Epoch 860 | Loss: 0.4582 | Recon: 0.4339 | \n",
      "Epoch 861 | Loss: 0.4680 | Recon: 0.4436 | \n",
      "Epoch 862 | Loss: 0.4606 | Recon: 0.4367 | \n",
      "Epoch 863 | Loss: 0.4755 | Recon: 0.4513 | \n",
      "Epoch 864 | Loss: 0.4614 | Recon: 0.4373 | \n",
      "Epoch 865 | Loss: 0.4583 | Recon: 0.4342 | \n",
      "Epoch 866 | Loss: 0.4653 | Recon: 0.4411 | \n",
      "Epoch 867 | Loss: 0.4672 | Recon: 0.4432 | \n",
      "Epoch 868 | Loss: 0.4649 | Recon: 0.4410 | \n",
      "Epoch 869 | Loss: 0.4513 | Recon: 0.4274 | \n",
      "Epoch 870 | Loss: 0.4652 | Recon: 0.4411 | \n",
      "Epoch 871 | Loss: 0.4593 | Recon: 0.4357 | \n",
      "Epoch 872 | Loss: 0.4450 | Recon: 0.4212 | \n",
      "Epoch 873 | Loss: 0.4540 | Recon: 0.4302 | \n",
      "Epoch 874 | Loss: 0.4555 | Recon: 0.4317 | \n",
      "Epoch 875 | Loss: 0.4616 | Recon: 0.4377 | \n",
      "Epoch 876 | Loss: 0.4602 | Recon: 0.4364 | \n",
      "Epoch 877 | Loss: 0.4544 | Recon: 0.4305 | \n",
      "Epoch 878 | Loss: 0.4546 | Recon: 0.4306 | \n",
      "Epoch 879 | Loss: 0.4689 | Recon: 0.4453 | \n",
      "Epoch 880 | Loss: 0.4505 | Recon: 0.4268 | \n",
      "Epoch 881 | Loss: 0.4645 | Recon: 0.4408 | \n",
      "Epoch 882 | Loss: 0.4497 | Recon: 0.4261 | \n",
      "Epoch 883 | Loss: 0.4574 | Recon: 0.4337 | \n",
      "Epoch 884 | Loss: 0.4654 | Recon: 0.4418 | \n",
      "Epoch 885 | Loss: 0.4573 | Recon: 0.4339 | \n",
      "Epoch 886 | Loss: 0.4731 | Recon: 0.4492 | \n",
      "Epoch 887 | Loss: 0.4467 | Recon: 0.4232 | \n",
      "Epoch 888 | Loss: 0.4747 | Recon: 0.4509 | \n",
      "Epoch 889 | Loss: 0.4544 | Recon: 0.4307 | \n",
      "Epoch 890 | Loss: 0.4636 | Recon: 0.4402 | \n",
      "Epoch 891 | Loss: 0.4545 | Recon: 0.4311 | \n",
      "Epoch 892 | Loss: 0.4516 | Recon: 0.4283 | \n",
      "Epoch 893 | Loss: 0.4553 | Recon: 0.4320 | \n",
      "Epoch 894 | Loss: 0.4608 | Recon: 0.4374 | \n",
      "Epoch 895 | Loss: 0.4581 | Recon: 0.4350 | \n",
      "Epoch 896 | Loss: 0.4550 | Recon: 0.4316 | \n",
      "Epoch 897 | Loss: 0.4503 | Recon: 0.4271 | \n",
      "Epoch 898 | Loss: 0.4595 | Recon: 0.4362 | \n",
      "Epoch 899 | Loss: 0.4672 | Recon: 0.4438 | \n",
      "Epoch 900 | Loss: 0.4582 | Recon: 0.4348 | \n",
      "Epoch 901 | Loss: 0.4522 | Recon: 0.4290 | \n",
      "Epoch 902 | Loss: 0.4562 | Recon: 0.4330 | \n",
      "Epoch 903 | Loss: 0.4590 | Recon: 0.4358 | \n",
      "Epoch 904 | Loss: 0.4613 | Recon: 0.4380 | \n",
      "Epoch 905 | Loss: 0.4681 | Recon: 0.4451 | \n",
      "Epoch 906 | Loss: 0.4676 | Recon: 0.4446 | \n",
      "Epoch 907 | Loss: 0.4607 | Recon: 0.4374 | \n",
      "Epoch 908 | Loss: 0.4470 | Recon: 0.4240 | \n",
      "Epoch 909 | Loss: 0.4534 | Recon: 0.4302 | \n",
      "Epoch 910 | Loss: 0.4487 | Recon: 0.4257 | \n",
      "Epoch 911 | Loss: 0.4683 | Recon: 0.4454 | \n",
      "Epoch 912 | Loss: 0.4625 | Recon: 0.4396 | \n",
      "Epoch 913 | Loss: 0.4586 | Recon: 0.4357 | \n",
      "Epoch 914 | Loss: 0.4480 | Recon: 0.4252 | \n",
      "Epoch 915 | Loss: 0.4540 | Recon: 0.4310 | \n",
      "Epoch 916 | Loss: 0.4536 | Recon: 0.4308 | \n",
      "Epoch 917 | Loss: 0.4581 | Recon: 0.4352 | \n",
      "Epoch 918 | Loss: 0.4468 | Recon: 0.4240 | \n",
      "Epoch 919 | Loss: 0.4652 | Recon: 0.4423 | \n",
      "Epoch 920 | Loss: 0.4529 | Recon: 0.4301 | \n",
      "Epoch 921 | Loss: 0.4574 | Recon: 0.4346 | \n",
      "Epoch 922 | Loss: 0.4468 | Recon: 0.4242 | \n",
      "Epoch 923 | Loss: 0.4591 | Recon: 0.4363 | \n",
      "Epoch 924 | Loss: 0.4547 | Recon: 0.4319 | \n",
      "Epoch 925 | Loss: 0.4540 | Recon: 0.4310 | \n",
      "Epoch 926 | Loss: 0.4631 | Recon: 0.4404 | \n",
      "Epoch 927 | Loss: 0.4583 | Recon: 0.4352 | \n",
      "Epoch 928 | Loss: 0.4539 | Recon: 0.4311 | \n",
      "Epoch 929 | Loss: 0.4663 | Recon: 0.4435 | \n",
      "Epoch 930 | Loss: 0.4509 | Recon: 0.4283 | \n",
      "Epoch 931 | Loss: 0.4586 | Recon: 0.4361 | \n",
      "Epoch 932 | Loss: 0.4446 | Recon: 0.4219 | \n",
      "Epoch 933 | Loss: 0.4577 | Recon: 0.4353 | \n",
      "Epoch 934 | Loss: 0.4628 | Recon: 0.4401 | \n",
      "Epoch 935 | Loss: 0.4474 | Recon: 0.4247 | \n",
      "Epoch 936 | Loss: 0.4547 | Recon: 0.4324 | \n",
      "Epoch 937 | Loss: 0.4528 | Recon: 0.4303 | \n",
      "Epoch 938 | Loss: 0.4560 | Recon: 0.4335 | \n",
      "Epoch 939 | Loss: 0.4609 | Recon: 0.4382 | \n",
      "Epoch 940 | Loss: 0.4540 | Recon: 0.4315 | \n",
      "Epoch 941 | Loss: 0.4555 | Recon: 0.4329 | \n",
      "Epoch 942 | Loss: 0.4467 | Recon: 0.4242 | \n",
      "Epoch 943 | Loss: 0.4467 | Recon: 0.4246 | \n",
      "Epoch 944 | Loss: 0.4529 | Recon: 0.4306 | \n",
      "Epoch 945 | Loss: 0.4546 | Recon: 0.4325 | \n",
      "Epoch 946 | Loss: 0.4531 | Recon: 0.4308 | \n",
      "Epoch 947 | Loss: 0.4470 | Recon: 0.4247 | \n",
      "Epoch 948 | Loss: 0.4426 | Recon: 0.4202 | \n",
      "Epoch 949 | Loss: 0.4467 | Recon: 0.4241 | \n",
      "Epoch 950 | Loss: 0.4406 | Recon: 0.4182 | \n",
      "Epoch 951 | Loss: 0.4532 | Recon: 0.4311 | \n",
      "Epoch 952 | Loss: 0.4613 | Recon: 0.4392 | \n",
      "Epoch 953 | Loss: 0.4591 | Recon: 0.4368 | \n",
      "Epoch 954 | Loss: 0.4529 | Recon: 0.4307 | \n",
      "Epoch 955 | Loss: 0.4415 | Recon: 0.4194 | \n",
      "Epoch 956 | Loss: 0.4466 | Recon: 0.4244 | \n",
      "Epoch 957 | Loss: 0.4551 | Recon: 0.4332 | \n",
      "Epoch 958 | Loss: 0.4623 | Recon: 0.4399 | \n",
      "Epoch 959 | Loss: 0.4562 | Recon: 0.4341 | \n",
      "Epoch 960 | Loss: 0.4625 | Recon: 0.4405 | \n",
      "Epoch 961 | Loss: 0.4438 | Recon: 0.4217 | \n",
      "Epoch 962 | Loss: 0.4747 | Recon: 0.4524 | \n",
      "Epoch 963 | Loss: 0.4550 | Recon: 0.4328 | \n",
      "Epoch 964 | Loss: 0.4435 | Recon: 0.4215 | \n",
      "Epoch 965 | Loss: 0.4465 | Recon: 0.4245 | \n",
      "Epoch 966 | Loss: 0.4411 | Recon: 0.4193 | \n",
      "Epoch 967 | Loss: 0.4497 | Recon: 0.4279 | \n",
      "Epoch 968 | Loss: 0.4461 | Recon: 0.4243 | \n",
      "Epoch 969 | Loss: 0.4442 | Recon: 0.4223 | \n",
      "Epoch 970 | Loss: 0.4466 | Recon: 0.4248 | \n",
      "Epoch 971 | Loss: 0.4579 | Recon: 0.4362 | \n",
      "Epoch 972 | Loss: 0.4474 | Recon: 0.4254 | \n",
      "Epoch 973 | Loss: 0.4490 | Recon: 0.4271 | \n",
      "Epoch 974 | Loss: 0.4481 | Recon: 0.4263 | \n",
      "Epoch 975 | Loss: 0.4591 | Recon: 0.4375 | \n",
      "Epoch 976 | Loss: 0.4668 | Recon: 0.4451 | \n",
      "Epoch 977 | Loss: 0.4562 | Recon: 0.4343 | \n",
      "Epoch 978 | Loss: 0.4529 | Recon: 0.4310 | \n",
      "Epoch 979 | Loss: 0.4557 | Recon: 0.4341 | \n",
      "Epoch 980 | Loss: 0.4469 | Recon: 0.4252 | \n",
      "Epoch 981 | Loss: 0.4487 | Recon: 0.4272 | \n",
      "Epoch 982 | Loss: 0.4535 | Recon: 0.4320 | \n",
      "Epoch 983 | Loss: 0.4537 | Recon: 0.4318 | \n",
      "Epoch 984 | Loss: 0.4549 | Recon: 0.4332 | \n",
      "Epoch 985 | Loss: 0.4543 | Recon: 0.4325 | \n",
      "Epoch 986 | Loss: 0.4487 | Recon: 0.4273 | \n",
      "Epoch 987 | Loss: 0.4530 | Recon: 0.4313 | \n",
      "Epoch 988 | Loss: 0.4596 | Recon: 0.4381 | \n",
      "Epoch 989 | Loss: 0.4578 | Recon: 0.4362 | \n",
      "Epoch 990 | Loss: 0.4625 | Recon: 0.4412 | \n",
      "Epoch 991 | Loss: 0.4575 | Recon: 0.4358 | \n",
      "Epoch 992 | Loss: 0.4624 | Recon: 0.4409 | \n",
      "Epoch 993 | Loss: 0.4548 | Recon: 0.4334 | \n",
      "Epoch 994 | Loss: 0.4456 | Recon: 0.4243 | \n",
      "Epoch 995 | Loss: 0.4462 | Recon: 0.4248 | \n",
      "Epoch 996 | Loss: 0.4564 | Recon: 0.4349 | \n",
      "Epoch 997 | Loss: 0.4395 | Recon: 0.4182 | \n",
      "Epoch 998 | Loss: 0.4504 | Recon: 0.4291 | \n",
      "Epoch 999 | Loss: 0.4399 | Recon: 0.4186 | \n",
      "Epoch 1000 | Loss: 0.4495 | Recon: 0.4283 | \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vae = MaskedVAE(input_dim=4, hidden_dim=4, latent_dim=4)\n",
    "vae = train_masked_vae(vae, dataloader, device,epochs=1000,lr=1e-3)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code ran for 7.49 seconds\n"
     ]
    }
   ],
   "source": [
    "elapsed = end_time - start_time\n",
    "print(f\"Code ran for {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = extract_latent_features(vae, X_scaled,mask_tensor,device)\n",
    "latent_np = latent.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53984445, -0.2840212 , -0.15451774, -1.094321  ],\n",
       "       [ 0.5386686 , -0.2877748 , -0.14990786, -1.097413  ],\n",
       "       [ 0.54151386, -0.2757576 , -0.16088781, -1.0872672 ],\n",
       "       [ 0.5409251 , -0.27831912, -0.15856546, -1.0892897 ],\n",
       "       [ 0.5410723 , -0.2789303 , -0.15924942, -1.0899963 ],\n",
       "       [ 0.5346752 , -0.30830473, -0.13509709, -1.116067  ],\n",
       "       [ 0.5422205 , -0.27360135, -0.16360286, -1.0853381 ],\n",
       "       [ 0.5396452 , -0.28474724, -0.15371192, -1.0948472 ],\n",
       "       [ 0.5416951 , -0.274265  , -0.16150352, -1.0858532 ],\n",
       "       [ 0.53944844, -0.2845295 , -0.15291235, -1.094655  ],\n",
       "       [ 0.53836536, -0.29117543, -0.14885837, -1.1004049 ],\n",
       "       [ 0.54067385, -0.2803824 , -0.1576378 , -1.0910487 ],\n",
       "       [ 0.5400282 , -0.2815848 , -0.15513968, -1.0921887 ],\n",
       "       [ 0.5447421 , -0.26057804, -0.17329553, -1.0744629 ],\n",
       "       [ 0.53570384, -0.302943  , -0.1391848 , -1.1119442 ],\n",
       "       [ 0.529367  , -0.33185405, -0.11593822, -1.1395398 ],\n",
       "       [ 0.53763574, -0.29499823, -0.14631656, -1.1043234 ],\n",
       "       [ 0.53919387, -0.28711116, -0.1520071 , -1.0968978 ],\n",
       "       [ 0.53242654, -0.31758273, -0.1265032 , -1.1241494 ],\n",
       "       [ 0.5403608 , -0.28297424, -0.15652192, -1.0933621 ],\n",
       "       [ 0.5360299 , -0.3008185 , -0.13979384, -1.1084294 ],\n",
       "       [ 0.5391914 , -0.288055  , -0.15200084, -1.0976162 ],\n",
       "       [ 0.54546666, -0.2591883 , -0.17620063, -1.0734217 ],\n",
       "       [ 0.5356866 , -0.30277878, -0.13841528, -1.109895  ],\n",
       "       [ 0.5395053 , -0.28588855, -0.1530881 , -1.0955374 ],\n",
       "       [ 0.5371805 , -0.2945457 , -0.1441535 , -1.1030529 ],\n",
       "       [ 0.53795457, -0.29276252, -0.14717412, -1.101497  ],\n",
       "       [ 0.5387459 , -0.28895676, -0.15027991, -1.0984647 ],\n",
       "       [ 0.5386166 , -0.2891121 , -0.14978608, -1.0986458 ],\n",
       "       [ 0.5403453 , -0.28126377, -0.1563381 , -1.0917559 ],\n",
       "       [ 0.53911746, -0.28635472, -0.15160638, -1.0960808 ],\n",
       "       [ 0.5355078 , -0.30332762, -0.13780579, -1.1105905 ],\n",
       "       [ 0.5425092 , -0.2739221 , -0.16485333, -1.0858238 ],\n",
       "       [ 0.5380274 , -0.29359663, -0.14810762, -1.1038954 ],\n",
       "       [ 0.53879786, -0.2876194 , -0.15040177, -1.0972319 ],\n",
       "       [ 0.53977615, -0.28322262, -0.15424064, -1.0937133 ],\n",
       "       [ 0.5373977 , -0.2945863 , -0.14514932, -1.1034145 ],\n",
       "       [ 0.54243195, -0.27274024, -0.16448128, -1.084772  ],\n",
       "       [ 0.5426035 , -0.27043885, -0.16503057, -1.0826797 ],\n",
       "       [ 0.53893614, -0.2878474 , -0.1509907 , -1.0974946 ],\n",
       "       [ 0.5402925 , -0.2821756 , -0.1562449 , -1.0927542 ],\n",
       "       [ 0.5376122 , -0.29056436, -0.14572543, -1.0996454 ],\n",
       "       [ 0.5436411 , -0.26645726, -0.16905147, -1.079325  ],\n",
       "       [ 0.53717226, -0.29695165, -0.1441634 , -1.1049734 ],\n",
       "       [ 0.53815216, -0.2934057 , -0.1479451 , -1.1019238 ],\n",
       "       [ 0.5387271 , -0.28776467, -0.15011844, -1.0973424 ],\n",
       "       [ 0.5406218 , -0.28171962, -0.157516  , -1.0922815 ],\n",
       "       [ 0.5418334 , -0.27449292, -0.16209248, -1.0861161 ],\n",
       "       [ 0.5390744 , -0.28807533, -0.15157962, -1.0977575 ],\n",
       "       [ 0.5395159 , -0.28490263, -0.153218  , -1.0950284 ],\n",
       "       [ 0.42876244, -1.2717457 ,  1.4711587 , -2.0283532 ],\n",
       "       [ 0.43613258, -1.1000842 ,  1.135258  , -1.8621476 ],\n",
       "       [ 0.4275415 , -1.3196962 ,  1.5724726 , -2.0744655 ],\n",
       "       [ 0.45396093, -0.81770223,  0.6390504 , -1.5866082 ],\n",
       "       [ 0.4345514 , -1.19571   ,  1.3466575 , -1.95379   ],\n",
       "       [ 0.44826066, -0.8927738 ,  0.76104146, -1.6602179 ],\n",
       "       [ 0.4347804 , -1.1225566 ,  1.1748528 , -1.8840494 ],\n",
       "       [ 0.48330772, -0.49697948,  0.06572109, -1.2805216 ],\n",
       "       [ 0.43609476, -1.1502566 ,  1.2542017 , -1.909934  ],\n",
       "       [ 0.45666075, -0.6961424 ,  0.37668812, -1.4698536 ],\n",
       "       [ 0.46436435, -0.59736806,  0.21859282, -1.372942  ],\n",
       "       [ 0.44435903, -0.93516487,  0.82407635, -1.7020417 ],\n",
       "       [ 0.45195264, -0.8800492 ,  0.7682902 , -1.6467246 ],\n",
       "       [ 0.44049805, -1.0562164 ,  1.0731723 , -1.8187444 ],\n",
       "       [ 0.45453143, -0.70355934,  0.3745817 , -1.477714  ],\n",
       "       [ 0.43435913, -1.1504753 ,  1.238132  , -1.9107921 ],\n",
       "       [ 0.44652054, -0.9000098 ,  0.76091915, -1.6677583 ],\n",
       "       [ 0.4536271 , -0.77644175,  0.53843373, -1.5474463 ],\n",
       "       [ 0.43935305, -1.171086  ,  1.3348374 , -1.9285488 ],\n",
       "       [ 0.45600015, -0.74303436,  0.48228037, -1.5147502 ],\n",
       "       [ 0.43652174, -1.0999033 ,  1.1372418 , -1.8618276 ],\n",
       "       [ 0.4459403 , -0.92317146,  0.81179637, -1.690032  ],\n",
       "       [ 0.43562776, -1.226966  ,  1.4305861 , -1.9831469 ],\n",
       "       [ 0.44345468, -1.0080471 ,  0.9879282 , -1.7717738 ],\n",
       "       [ 0.44034365, -1.0444416 ,  1.0448232 , -1.8075931 ],\n",
       "       [ 0.43570673, -1.1350205 ,  1.2144847 , -1.8955727 ],\n",
       "       [ 0.43102756, -1.2827895 ,  1.519037  , -2.038021  ],\n",
       "       [ 0.4267187 , -1.3534306 ,  1.6438758 , -2.1068933 ],\n",
       "       [ 0.4414578 , -1.025525  ,  1.0098076 , -1.7891631 ],\n",
       "       [ 0.45834434, -0.66213244,  0.31405705, -1.4368438 ],\n",
       "       [ 0.45790946, -0.7111037 ,  0.4251778 , -1.4836333 ],\n",
       "       [ 0.45991656, -0.66417396,  0.3336712 , -1.4381986 ],\n",
       "       [ 0.45185924, -0.8043987 ,  0.5876278 , -1.5747278 ],\n",
       "       [ 0.43677408, -1.1803112 ,  1.3303068 , -1.9382937 ],\n",
       "       [ 0.44908464, -0.84362185,  0.6519049 , -1.6131086 ],\n",
       "       [ 0.4396842 , -0.9922841 ,  0.91356224, -1.7581751 ],\n",
       "       [ 0.43122882, -1.2303567 ,  1.3965485 , -1.9880214 ],\n",
       "       [ 0.44145796, -1.1091572 ,  1.2089272 , -1.8687961 ],\n",
       "       [ 0.4516579 , -0.77319884,  0.5109964 , -1.5450943 ],\n",
       "       [ 0.45382968, -0.7922242 ,  0.5773315 , -1.562398  ],\n",
       "       [ 0.45296323, -0.81493396,  0.62223965, -1.5843436 ],\n",
       "       [ 0.44099402, -1.0270016 ,  1.008858  , -1.7907422 ],\n",
       "       [ 0.45136327, -0.83361334,  0.6519421 , -1.60273   ],\n",
       "       [ 0.47514543, -0.5255832 ,  0.0977788 , -1.3061295 ],\n",
       "       [ 0.45129314, -0.8278916 ,  0.63702935, -1.5973067 ],\n",
       "       [ 0.45125973, -0.78741455,  0.5409066 , -1.5587785 ],\n",
       "       [ 0.44987983, -0.83060765,  0.6298178 , -1.6004214 ],\n",
       "       [ 0.44290787, -0.9880535 ,  0.9358085 , -1.752943  ],\n",
       "       [ 0.48572665, -0.4898423 ,  0.05585361, -1.2743642 ],\n",
       "       [ 0.45050704, -0.826871  ,  0.62722236, -1.5966296 ],\n",
       "       [ 0.41447026, -1.6108274 ,  2.1322315 , -2.356552  ],\n",
       "       [ 0.43500173, -1.2152855 ,  1.395448  , -1.9722574 ],\n",
       "       [ 0.41075408, -1.7363044 ,  2.3952043 , -2.4774182 ],\n",
       "       [ 0.42709768, -1.382702  ,  1.7154872 , -2.1346195 ],\n",
       "       [ 0.4175626 , -1.5811187 ,  2.0927587 , -2.3271093 ],\n",
       "       [ 0.40041256, -1.9926046 ,  2.9019244 , -2.7253222 ],\n",
       "       [ 0.4529322 , -0.82725495,  0.64976984, -1.5960853 ],\n",
       "       [ 0.41034567, -1.7799724 ,  2.4947429 , -2.519148  ],\n",
       "       [ 0.42110866, -1.5793854 ,  2.123863  , -2.3241336 ],\n",
       "       [ 0.40217304, -1.8428323 ,  2.563672  , -2.5820577 ],\n",
       "       [ 0.42425352, -1.3794026 ,  1.6807528 , -2.1325438 ],\n",
       "       [ 0.42618608, -1.4174012 ,  1.7894008 , -2.168001  ],\n",
       "       [ 0.41684666, -1.5858197 ,  2.0978642 , -2.331855  ],\n",
       "       [ 0.43553114, -1.2265477 ,  1.4272563 , -1.9827833 ],\n",
       "       [ 0.42770854, -1.3548167 ,  1.6548474 , -2.1078413 ],\n",
       "       [ 0.4200759 , -1.4755222 ,  1.8673105 , -2.2256265 ],\n",
       "       [ 0.4250294 , -1.4098755 ,  1.7601871 , -2.161267  ],\n",
       "       [ 0.39659837, -1.9658161 ,  2.8010626 , -2.701241  ],\n",
       "       [ 0.39481726, -2.18209   ,  3.2963371 , -2.9078345 ],\n",
       "       [ 0.43910927, -1.1970767 ,  1.3930974 , -1.9533846 ],\n",
       "       [ 0.41141915, -1.6823953 ,  2.2736657 , -2.4258394 ],\n",
       "       [ 0.4371779 , -1.1436611 ,  1.2467163 , -1.9032462 ],\n",
       "       [ 0.40014565, -2.0322986 ,  2.9935536 , -2.763217  ],\n",
       "       [ 0.43115997, -1.2928503 ,  1.5430224 , -2.04755   ],\n",
       "       [ 0.4168087 , -1.5523599 ,  2.0176883 , -2.3000088 ],\n",
       "       [ 0.41311568, -1.664134  ,  2.2472925 , -2.4078164 ],\n",
       "       [ 0.43293804, -1.2354414 ,  1.4242008 , -1.9922225 ],\n",
       "       [ 0.43352723, -1.1982449 ,  1.3414297 , -1.9565847 ],\n",
       "       [ 0.42154467, -1.5149972 ,  1.9750092 , -2.2626626 ],\n",
       "       [ 0.4172611 , -1.5957526 ,  2.1259983 , -2.341157  ],\n",
       "       [ 0.40880695, -1.8184079 ,  2.5712514 , -2.5563216 ],\n",
       "       [ 0.39861   , -1.9118694 ,  2.6936092 , -2.6491241 ],\n",
       "       [ 0.42009917, -1.5454514 ,  2.033061  , -2.2922008 ],\n",
       "       [ 0.4343077 , -1.2217004 ,  1.4049172 , -1.9786255 ],\n",
       "       [ 0.4356407 , -1.2427146 ,  1.4668438 , -1.9981337 ],\n",
       "       [ 0.39904737, -1.9993283 ,  2.905261  , -2.732237  ],\n",
       "       [ 0.41809645, -1.5017314 ,  1.9095008 , -2.2513204 ],\n",
       "       [ 0.42624587, -1.3689424 ,  1.6748207 , -2.121837  ],\n",
       "       [ 0.43537092, -1.1535754 ,  1.2534678 , -1.9133627 ],\n",
       "       [ 0.41606057, -1.584799  ,  2.0880573 , -2.3311777 ],\n",
       "       [ 0.413165  , -1.6527246 ,  2.220108  , -2.3969355 ],\n",
       "       [ 0.41485432, -1.59628   ,  2.1037962 , -2.342562  ],\n",
       "       [ 0.43500173, -1.2152855 ,  1.395448  , -1.9722574 ],\n",
       "       [ 0.41157806, -1.6871529 ,  2.2860684 , -2.4303088 ],\n",
       "       [ 0.4110267 , -1.6741763 ,  2.2498956 , -2.4181602 ],\n",
       "       [ 0.4169225 , -1.5691066 ,  2.059096  , -2.3159144 ],\n",
       "       [ 0.4292841 , -1.3652581 ,  1.6962478 , -2.1171951 ],\n",
       "       [ 0.42382318, -1.4213563 ,  1.7759264 , -2.1726513 ],\n",
       "       [ 0.42194718, -1.4101319 ,  1.730032  , -2.1626635 ],\n",
       "       [ 0.4349683 , -1.1748083 ,  1.299325  , -1.9337289 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Stage 2: Distributional Evidential Clustering (Option 3)\n",
    "# ------------------------------------------------------------\n",
    "# Input  : U  -> latent features from Stage 1 (masked AE/VAE)\n",
    "# Output : fuzzy memberships, hard labels, cluster parameters\n",
    "#\n",
    "# Key design choices (intentional):\n",
    "# - Clusters are Gaussian distributions in latent space\n",
    "# - Memberships are evidential (fuzzy), not probabilistic\n",
    "# - Distance is negative log-likelihood (not Euclidean)\n",
    "# - Explicit clusterâ€“cluster divergence regularization\n",
    "# - Diagonal covariance for stability and efficiency\n",
    "# - No EM, no mixture weights, no DST\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Negative log-likelihood under a diagonal Gaussian\n",
    "# This replaces point-wise Euclidean distance\n",
    "# ------------------------------------------------------------\n",
    "def neg_log_likelihood(u, mu, var, eps=1e-6):\n",
    "    \"\"\"\n",
    "    u   : latent feature vector of a sample (d,)\n",
    "    mu  : cluster mean (d,)\n",
    "    var : diagonal variance of cluster (d,)\n",
    "    \"\"\"\n",
    "    d = u.shape[0]\n",
    "    return 0.5 * (\n",
    "        np.sum(np.log(var + eps)) +\n",
    "        np.sum((u - mu) ** 2 / (var + eps)) +\n",
    "        d * np.log(2 * np.pi)\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# KL divergence between two diagonal Gaussian clusters\n",
    "# Used to explicitly penalize cluster similarity\n",
    "# ------------------------------------------------------------\n",
    "def kl_divergence(mu1, var1, mu2, var2, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes KL( N(mu1,var1) || N(mu2,var2) )\n",
    "    \"\"\"\n",
    "    return 0.5 * np.sum(\n",
    "        np.log((var2 + eps) / (var1 + eps)) +\n",
    "        (var1 + (mu1 - mu2) ** 2) / (var2 + eps) - 1.0\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Initialization\n",
    "# - Means initialized from random latent points\n",
    "# - Variances initialized to ones\n",
    "# - Memberships initialized uniformly (maximum uncertainty)\n",
    "# ------------------------------------------------------------\n",
    "def initialize_clusters(U, n_clusters, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n, d = U.shape\n",
    "\n",
    "    idx = np.random.choice(n, n_clusters, replace=False)\n",
    "    mu = U[idx].copy()\n",
    "    var = np.ones((n_clusters, d))           # diagonal covariance\n",
    "    M = np.ones((n, n_clusters)) / n_clusters\n",
    "\n",
    "    return mu, var, M\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute divergence penalty for each cluster\n",
    "# Î”_j = sum_{kâ‰ j} KL(C_j || C_k)\n",
    "# ------------------------------------------------------------\n",
    "def compute_cluster_penalties(mu, var):\n",
    "    c = mu.shape[0]\n",
    "    Delta = np.zeros(c)\n",
    "\n",
    "    for j in range(c):\n",
    "        for k in range(c):\n",
    "            if j != k:\n",
    "                Delta[j] += kl_divergence(\n",
    "                    mu[j], var[j],\n",
    "                    mu[k], var[k]\n",
    "                )\n",
    "    return Delta\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Membership update (Option 3)\n",
    "# Implements Eq. (4) in the paper\n",
    "# Uses direct exponentiation (no logsumexp)\n",
    "# ------------------------------------------------------------\n",
    "def update_memberships(U, mu, var, beta, gamma, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Memberships are updated using likelihood + divergence cost.\n",
    "    This is NOT a posterior probability (unlike GMM).\n",
    "    \"\"\"\n",
    "    n, _ = U.shape\n",
    "    c = mu.shape[0]\n",
    "\n",
    "    Delta = compute_cluster_penalties(mu, var)\n",
    "    M = np.zeros((n, c))\n",
    "\n",
    "    for i in range(n):\n",
    "        numerators = np.zeros(c)\n",
    "\n",
    "        for j in range(c):\n",
    "            # Combined cost:\n",
    "            # data-to-cluster fit + cluster-separation penalty\n",
    "            cost_ij = (\n",
    "                neg_log_likelihood(U[i], mu[j], var[j])\n",
    "                - gamma * Delta[j]\n",
    "            )\n",
    "\n",
    "            numerators[j] = np.exp(-cost_ij / (beta - 1.0))\n",
    "\n",
    "        # Normalize memberships to sum to 1\n",
    "        M[i, :] = numerators / (np.sum(numerators) + eps)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cluster parameter update\n",
    "# Weighted maximum likelihood using evidential memberships\n",
    "# Implements Eq. (5) and Eq. (6)\n",
    "# ------------------------------------------------------------\n",
    "def update_clusters(U, M, beta, eps=1e-8):\n",
    "    n, d = U.shape\n",
    "    c = M.shape[1]\n",
    "\n",
    "    mu_new = np.zeros((c, d))\n",
    "    var_new = np.zeros((c, d))\n",
    "    M_beta = M ** beta\n",
    "\n",
    "    for j in range(c):\n",
    "        w = M_beta[:, j][:, None]\n",
    "        denom = np.sum(w) + eps\n",
    "\n",
    "        # Mean update\n",
    "        mu_new[j] = np.sum(w * U, axis=0) / denom\n",
    "\n",
    "        # Diagonal covariance update\n",
    "        var_new[j] = np.sum(\n",
    "            w * (U - mu_new[j]) ** 2, axis=0\n",
    "        ) / denom\n",
    "\n",
    "    return mu_new, var_new\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main optimization loop (Algorithm 1 in paper)\n",
    "# ------------------------------------------------------------\n",
    "def distributional_evidential_clustering(\n",
    "    U,\n",
    "    n_clusters,\n",
    "    beta=2.0,\n",
    "    gamma=0.05,\n",
    "    max_iter=3,\n",
    "    tol=1e-4,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs distributional evidential clustering on latent features.\n",
    "    \"\"\"\n",
    "    mu, var, M = initialize_clusters(U, n_clusters, seed)\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        mu_old = mu.copy()\n",
    "\n",
    "        # Step 1: update memberships\n",
    "        M = update_memberships(U, mu, var, beta, gamma)\n",
    "\n",
    "        # Step 2: update cluster distributions\n",
    "        mu, var = update_clusters(U, M, beta)\n",
    "\n",
    "        # Convergence check on cluster means\n",
    "        shift = np.max(np.linalg.norm(mu - mu_old, axis=1))\n",
    "        if shift < tol:\n",
    "            print(f\"Converged at iteration {it}\")\n",
    "            break\n",
    "\n",
    "    labels = np.argmax(M, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"memberships\": M,\n",
    "        \"labels\": labels,\n",
    "        \"mu\": mu,\n",
    "        \"var\": var\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = distributional_evidential_clustering(\n",
    "    latent_np,\n",
    "    n_clusters=3,\n",
    "    beta=2,\n",
    "    gamma=0.0001,\n",
    "    max_iter=500,\n",
    "    tol=1e-8,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memberships': array([[1.71362881e-028, 1.00000000e+000, 2.44759750e-031],\n",
       "        [5.78310311e-028, 1.00000000e+000, 5.88376296e-031],\n",
       "        [7.94534094e-029, 1.00000000e+000, 2.07416779e-031],\n",
       "        [8.98815770e-029, 1.00000000e+000, 1.92212987e-031],\n",
       "        [8.21116176e-029, 1.00000000e+000, 1.75972558e-031],\n",
       "        [1.05988275e-023, 1.00000000e+000, 2.71173685e-027],\n",
       "        [7.73578453e-029, 1.00000000e+000, 2.49946502e-031],\n",
       "        [2.03998500e-028, 1.00000000e+000, 2.74404966e-031],\n",
       "        [8.35735791e-029, 1.00000000e+000, 2.38872912e-031],\n",
       "        [2.37670122e-028, 1.00000000e+000, 3.11090455e-031],\n",
       "        [1.07731087e-027, 1.00000000e+000, 9.21856091e-031],\n",
       "        [9.73675704e-029, 1.00000000e+000, 1.84594172e-031],\n",
       "        [1.44870545e-028, 1.00000000e+000, 2.33488449e-031],\n",
       "        [7.98759007e-028, 1.00000000e+000, 6.83281770e-030],\n",
       "        [3.92027047e-025, 1.00000000e+000, 1.40345323e-028],\n",
       "        [4.92955914e-013, 9.99999996e-001, 2.62142124e-017],\n",
       "        [4.32266013e-027, 1.00000000e+000, 2.82970298e-030],\n",
       "        [3.35203188e-028, 1.00000000e+000, 3.82560645e-031],\n",
       "        [4.14300246e-020, 1.00000000e+000, 5.54701609e-024],\n",
       "        [1.17576588e-028, 1.00000000e+000, 1.91570929e-031],\n",
       "        [1.20157286e-025, 1.00000000e+000, 5.00019467e-029],\n",
       "        [3.56969535e-028, 1.00000000e+000, 3.94476889e-031],\n",
       "        [1.56730382e-027, 1.00000000e+000, 1.63243070e-029],\n",
       "        [3.25740847e-025, 1.00000000e+000, 1.20370252e-028],\n",
       "        [2.38067288e-028, 1.00000000e+000, 3.01323536e-031],\n",
       "        [6.97266571e-027, 1.00000000e+000, 4.35174332e-030],\n",
       "        [2.04046418e-027, 1.00000000e+000, 1.54569079e-030],\n",
       "        [5.85490330e-028, 1.00000000e+000, 5.78872181e-031],\n",
       "        [6.77433013e-028, 1.00000000e+000, 6.50764036e-031],\n",
       "        [1.17300872e-028, 1.00000000e+000, 2.03068880e-031],\n",
       "        [3.42641133e-028, 1.00000000e+000, 3.97306630e-031],\n",
       "        [5.15892055e-025, 1.00000000e+000, 1.81202791e-028],\n",
       "        [7.17584743e-029, 1.00000000e+000, 2.41269816e-031],\n",
       "        [2.30946928e-027, 1.00000000e+000, 1.67634249e-030],\n",
       "        [5.01872093e-028, 1.00000000e+000, 5.25553874e-031],\n",
       "        [1.77172766e-028, 1.00000000e+000, 2.56868435e-031],\n",
       "        [5.32869377e-027, 1.00000000e+000, 3.42766689e-030],\n",
       "        [8.06074816e-029, 1.00000000e+000, 2.79057428e-031],\n",
       "        [1.02059349e-028, 1.00000000e+000, 3.98204139e-031],\n",
       "        [4.46339036e-028, 1.00000000e+000, 4.74551904e-031],\n",
       "        [1.21680268e-028, 1.00000000e+000, 2.01229735e-031],\n",
       "        [2.44015047e-027, 1.00000000e+000, 1.88109046e-030],\n",
       "        [1.85448183e-028, 1.00000000e+000, 1.02485332e-030],\n",
       "        [1.01893019e-026, 1.00000000e+000, 5.85893883e-030],\n",
       "        [1.74815694e-027, 1.00000000e+000, 1.34245100e-030],\n",
       "        [5.43764728e-028, 1.00000000e+000, 5.59665843e-031],\n",
       "        [9.95623899e-029, 1.00000000e+000, 1.77955678e-031],\n",
       "        [7.86385377e-029, 1.00000000e+000, 2.28517436e-031],\n",
       "        [3.99239766e-028, 1.00000000e+000, 4.31008648e-031],\n",
       "        [2.27885016e-028, 1.00000000e+000, 2.97662099e-031],\n",
       "        [4.42631375e-014, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.27946268e-005, 0.00000000e+000, 9.99987205e-001],\n",
       "        [7.59293232e-017, 0.00000000e+000, 1.00000000e+000],\n",
       "        [9.99904810e-001, 0.00000000e+000, 9.51897178e-005],\n",
       "        [6.59081823e-010, 0.00000000e+000, 9.99999999e-001],\n",
       "        [9.94219909e-001, 0.00000000e+000, 5.78009120e-003],\n",
       "        [1.32834391e-006, 0.00000000e+000, 9.99998672e-001],\n",
       "        [9.99999738e-001, 1.93533315e-269, 2.62368739e-007],\n",
       "        [9.88076284e-008, 0.00000000e+000, 9.99999901e-001],\n",
       "        [9.99996804e-001, 0.00000000e+000, 3.19570594e-006],\n",
       "        [9.99999338e-001, 0.00000000e+000, 6.62001953e-007],\n",
       "        [9.04445227e-001, 0.00000000e+000, 9.55547734e-002],\n",
       "        [9.98118177e-001, 0.00000000e+000, 1.88182304e-003],\n",
       "        [1.02938513e-003, 0.00000000e+000, 9.98970615e-001],\n",
       "        [9.99994884e-001, 0.00000000e+000, 5.11611157e-006],\n",
       "        [7.88757155e-008, 0.00000000e+000, 9.99999921e-001],\n",
       "        [9.89261651e-001, 0.00000000e+000, 1.07383490e-002],\n",
       "        [9.99973825e-001, 0.00000000e+000, 2.61752180e-005],\n",
       "        [1.52377168e-008, 0.00000000e+000, 9.99999985e-001],\n",
       "        [9.99992168e-001, 0.00000000e+000, 7.83212312e-006],\n",
       "        [1.39666193e-005, 0.00000000e+000, 9.99986033e-001],\n",
       "        [9.59450314e-001, 0.00000000e+000, 4.05496858e-002],\n",
       "        [2.02110685e-011, 0.00000000e+000, 1.00000000e+000],\n",
       "        [6.18505290e-002, 0.00000000e+000, 9.38149471e-001],\n",
       "        [2.61411159e-003, 0.00000000e+000, 9.97385888e-001],\n",
       "        [4.35536784e-007, 0.00000000e+000, 9.99999564e-001],\n",
       "        [1.35202101e-014, 0.00000000e+000, 1.00000000e+000],\n",
       "        [6.59076914e-019, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.34823199e-002, 0.00000000e+000, 9.86517680e-001],\n",
       "        [9.99998212e-001, 0.00000000e+000, 1.78834296e-006],\n",
       "        [9.99996811e-001, 0.00000000e+000, 3.18855074e-006],\n",
       "        [9.99998620e-001, 0.00000000e+000, 1.37958952e-006],\n",
       "        [9.99918319e-001, 0.00000000e+000, 8.16808445e-005],\n",
       "        [4.58252956e-009, 0.00000000e+000, 9.99999995e-001],\n",
       "        [9.99464801e-001, 0.00000000e+000, 5.35199433e-004],\n",
       "        [1.08607433e-001, 0.00000000e+000, 8.91392567e-001],\n",
       "        [8.86295354e-012, 0.00000000e+000, 1.00000000e+000],\n",
       "        [9.89781970e-006, 0.00000000e+000, 9.99990102e-001],\n",
       "        [9.99966937e-001, 0.00000000e+000, 3.30633704e-005],\n",
       "        [9.99959519e-001, 0.00000000e+000, 4.04814668e-005],\n",
       "        [9.99901072e-001, 0.00000000e+000, 9.89279512e-005],\n",
       "        [1.13428102e-002, 0.00000000e+000, 9.88657190e-001],\n",
       "        [9.99744594e-001, 0.00000000e+000, 2.55405731e-004],\n",
       "        [9.99999646e-001, 0.00000000e+000, 3.54390006e-007],\n",
       "        [9.99794247e-001, 0.00000000e+000, 2.05752812e-004],\n",
       "        [9.99947346e-001, 0.00000000e+000, 5.26541073e-005],\n",
       "        [9.99715745e-001, 0.00000000e+000, 2.84255127e-004],\n",
       "        [2.04407786e-001, 0.00000000e+000, 7.95592214e-001],\n",
       "        [9.99999747e-001, 3.08877794e-247, 2.52710893e-007],\n",
       "        [9.99776282e-001, 0.00000000e+000, 2.23717640e-004],\n",
       "        [5.40472055e-038, 0.00000000e+000, 1.00000000e+000],\n",
       "        [7.74414611e-011, 0.00000000e+000, 1.00000000e+000],\n",
       "        [2.56770360e-049, 0.00000000e+000, 1.00000000e+000],\n",
       "        [9.88426977e-021, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.75177674e-035, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.43745926e-076, 0.00000000e+000, 1.00000000e+000],\n",
       "        [9.99843876e-001, 0.00000000e+000, 1.56123735e-004],\n",
       "        [1.45217559e-053, 0.00000000e+000, 1.00000000e+000],\n",
       "        [2.66770798e-035, 0.00000000e+000, 1.00000000e+000],\n",
       "        [5.46723958e-060, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.27044374e-020, 0.00000000e+000, 1.00000000e+000],\n",
       "        [4.62267213e-023, 0.00000000e+000, 1.00000000e+000],\n",
       "        [6.78283031e-036, 0.00000000e+000, 1.00000000e+000],\n",
       "        [2.16953650e-011, 0.00000000e+000, 1.00000000e+000],\n",
       "        [6.13432909e-019, 0.00000000e+000, 1.00000000e+000],\n",
       "        [2.68142500e-027, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.38401034e-022, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.91256453e-073, 0.00000000e+000, 1.00000000e+000],\n",
       "        [2.50604407e-100, 0.00000000e+000, 9.99999998e-001],\n",
       "        [8.57276685e-010, 0.00000000e+000, 9.99999999e-001],\n",
       "        [2.62630297e-044, 0.00000000e+000, 1.00000000e+000],\n",
       "        [2.26294164e-007, 0.00000000e+000, 9.99999774e-001],\n",
       "        [2.62038703e-081, 0.00000000e+000, 1.00000000e+000],\n",
       "        [3.79348282e-015, 0.00000000e+000, 1.00000000e+000],\n",
       "        [3.39177054e-033, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.18670837e-042, 0.00000000e+000, 1.00000000e+000],\n",
       "        [5.87009641e-012, 0.00000000e+000, 1.00000000e+000],\n",
       "        [4.58600977e-010, 0.00000000e+000, 1.00000000e+000],\n",
       "        [3.45686932e-030, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.02481775e-036, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.88563202e-057, 0.00000000e+000, 1.00000000e+000],\n",
       "        [2.08637415e-067, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.41690518e-032, 0.00000000e+000, 1.00000000e+000],\n",
       "        [3.37836869e-011, 0.00000000e+000, 1.00000000e+000],\n",
       "        [3.12167902e-012, 0.00000000e+000, 1.00000000e+000],\n",
       "        [2.31278035e-077, 0.00000000e+000, 1.00000000e+000],\n",
       "        [2.85236462e-029, 0.00000000e+000, 1.00000000e+000],\n",
       "        [7.00530148e-020, 0.00000000e+000, 1.00000000e+000],\n",
       "        [6.67495079e-008, 0.00000000e+000, 9.99999933e-001],\n",
       "        [7.85410764e-036, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.23523736e-041, 0.00000000e+000, 1.00000000e+000],\n",
       "        [8.31654876e-037, 0.00000000e+000, 1.00000000e+000],\n",
       "        [7.74414611e-011, 0.00000000e+000, 1.00000000e+000],\n",
       "        [9.95803459e-045, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.43842355e-043, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.54863766e-034, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.46087161e-019, 0.00000000e+000, 1.00000000e+000],\n",
       "        [2.13012086e-023, 0.00000000e+000, 1.00000000e+000],\n",
       "        [1.06951454e-022, 0.00000000e+000, 1.00000000e+000],\n",
       "        [6.97995265e-009, 0.00000000e+000, 9.99999993e-001]]),\n",
       " 'labels': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 0, 2,\n",
       "        0, 0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 2,\n",
       "        0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'mu': array([[ 0.45591395, -0.76217875,  0.51679051, -1.53400765],\n",
       "        [ 0.53917354, -0.28689941, -0.15197824, -1.0968906 ],\n",
       "        [ 0.42483883, -1.4095607 ,  1.757926  , -2.16103966]]),\n",
       " 'var': array([[9.80637786e-05, 1.43148104e-02, 4.61776676e-02, 1.32744978e-02],\n",
       "        [8.00860372e-06, 1.66410600e-04, 1.14615771e-04, 1.30778161e-04],\n",
       "        [1.63143202e-04, 8.11413428e-02, 3.05188419e-01, 7.61451558e-02]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = result[\"labels\"]\n",
    "memberships = result[\"memberships\"]\n",
    "df[\"pred\"]=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Re': 0.6733333333333333, 'Ri': 0.0, 'EP': 0.7225294541957201, 'ERI': 0.8367785234899329, 'RI': 0.8367785234899329, 'ARI': 0.64225125183629}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "ari = compute_all_metrics(df[\"target\"], df[\"pred\"])\n",
    "print(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdOn": 1769068019794,
  "creator": "2018509",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env offus)",
   "language": "python",
   "name": "py-dku-venv-offus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "modifiedBy": "2018509",
  "tags": [],
  "versionNumber": 1
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
